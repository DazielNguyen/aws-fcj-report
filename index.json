[{"uri":"https://dazielnguyen.github.io/aws-fcj-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders (Track 2: Migration \u0026amp; Modernization)” Event Objectives Complete large-scale migration and modernization with AWS Modernize applications using generative AI–powered tools Group discussion: Application modernization—Accelerate business transformation Transform VMware with AI-driven cloud-modernization technologies AWS security at scale: From development to production Speakers Nguyen Van Hai - Director of Software Engineering, Techcombank Nguyen The Vinh - Co-Founder \u0026amp; CTO, Ninety Eight Nguyen Minh Nganh - AI Specialist, OCB Nguyen Manh Tuyen - Head of Data Application, LPBank Securities Key Highlights 1. Learn large-scale migration and modernization strategies with AWS through real-world case studies from Techcombank The Modernization Journey of Techcombank\u0026rsquo;s\nAssess: Inventory the environment and identify gaps. Mobilize: Establish CCoE, define guardrails, and build cloud fluency. Migrate \u0026amp; Modernize: Prioritize high-impact workloads. Reinvent: AI, automation, data products, new business models. Generative AI in Modernization.\nCode Transformation: Java 8 -\u0026gt; 21, .NET -\u0026gt; .NET 8 Dependency Mapping: Automatied analysis of system relationships. Environment Assessment: Amazon: thousands of services modernized with AI Solutions and strategies Techcombank applied when using AWS services. Modernization with AWS native technologies:\nAmazon EKS Amazon Aurora MySQL Amazon MSK Amazon ElastiCache for Redis OSS. Overview of the Modernization Strategy Blueprint.\nAlign: Executive sponsorship and business drivers | Assess: Understand people, process, and technology | Mobilize: CoE, governance, training | Modernize: Replatform, refactor, rebuild | Reinvent: Data, AI, and modern apps for innovation 2. Gain knowledge about modernizing applications using Generative Al-powered tools, with practical insights from VPBank Modernization is the process of progressively transforming applications to achieve the availability, scalability, business agility, and cost optimization benefits of running on the cloud. Top 4 Use cases – App Modernization with GenAI.\nUse case 1: Streamline VMware Migration with AWS Transform for VMware\nAccelerate infrastructure migration and modernization with intelligent discovery and automated execution\nSlash VMware migration timelines with AWS Transform\u0026rsquo;s intelligent automation.\nConvert complex network configurations in hours instead of weeks using Al-powered discovery, dependency mapping, and automated wave planning.\nScale your migration practice with automated security group creation, intelligent EC2 instance selection, and flexible deployment options including hub-and-spoke or isolated VPC configurations.\nGain up to 90% improverent in execution times while reducing manual effort by 80%.\nUse case 2: GenAI Development with AWS Serverless and Container Solutions\nBuild Enterprise-Ready GenAI Applications Across AWS Serverless and Containers Platforms\nAWS offers two powerful paths for GenAI application development and deployment:\nServerless with AWS Bedrock: Rapidly develop and deploy GenAI applications using AWS Lambda, ECS with Fargate, Step Functions, and EventBridge. Ideal for chatbots, document generation, and intelligent content processing. Leverage the latest AWS Bedrock updates and reference architectures.\nContainer-Based with Amazon EKS: Build, train, and run GenAl apps on Kubernetes, benefiting from its powerful orchestration capabilities. Utilize open-source tools and cloud-native services for scalable GenAl workloads. Flexible deployment across cloud and on-premises environments with constant innovation from the OSS community.\nChoose one approach or combine both to best fit your specific GenAI application requirements and accelerate your AI journey.\nUse case 3: Revolutionize .NET Modernization with AWS Transform for .NET\nTransform legacy Windows applications to cloud-native with AI-powered automation.\nModernize Windows-based applications up to 4× faster with AWS Transform for .NET. Leverage AI automation to analyze dependencies, refactor code, and optimize for Linux deployment while cutting licensing costs by up to 40%. Transform hundreds of applications in parallel with automated testing and validation câpbilities—from legacy MVC applications to WCF services. Advanced features include automatic UI modernization, private package handling, and intelligent wave planning, delivering comprehensive modernization with exceptional speed. Use case 4: Elevate Platform Engineering with GenAI \u0026amp; IDP\nHarness the power of intelligent assistants like AWS Transform Developer with Internal Development Platforms.\nScaling modernization at an enterprise level requires time and investment to develop Internal Development Platforms (IDP). Gartner predicts that by 2026, 80% of software engineering organizations will establish platform teams as internal providers of reusable services, components, and tools for application delivery.\nHarness the power of intelligent assistants like AWS Transform Developer with IDPs to:\nCreate workflows and automate repetitive tasks.\nLearn IDP best practices leading organizations, such as Adobe, Expedia, JPMC, and Goldman Sachs.\nUnderstand AWS container blueprints and reference architectures to deliver accelerated velocity and scaling for an enterprise scale modernization initiative.\nCommon modernization drivers\nReduce costs\nReduce/eliminate Windows \u0026amp; SQL Server licensing costs Create architectures that match cost to actual load Leverage ARM64 architectures for better price-performance Increase pace of innovation\nDecompose monoliths into smaller services / microservices Take advantage of new technologies and C# language features Automate manual processes Improve ability to scale\nScale individual components / services Granular scaling with containers / serverless Attract and retain talent\n3. Gain insights from top industry experts through panel discussions on application modernization .NET Framework vs cross-platform .NET\n.NET Framework:\nWindows-only OS Version 1.0 released in 2002 Final version is 4.8*, released in 2019 Monolithic installation—A large number of libraries are installed at once. EC2, Elastic Beanstalk, ECS, and EKS. .NET (formerly .NET Core)\nCross-platform (Windows, Linux, macOS) Version 1.0 released in 2016 Current GA version is 8.0, released in 2023 Supports side-by-side installations Most libraries are distributed individually EC2, Elastic Beanstalk, ECS, EKS, Lambda Fargate AWS Transform: Orchestrated intelligence\nUnified web experience -\u0026gt; End-to-end automation -\u0026gt; Dedicated agency -\u0026gt; Goal-driven -\u0026gt; Human-in-the-loop -\u0026gt; Simplified collaboration AWS Transform for .NET\nCustomer benefits\nReduce operating costs by up to 40% Eliminate operating system license commercial costs Access a larger developer pool Cloud scale and performance. Technical benefits\nVulnerability remediation support Cross-platform support: Windows, macOS, Linux (x86-64, arm64) Compatibility with x86-64 and arm64 LightweightContainer Lambda serverless architecture Complete language upgrades in minutes via Amazon Q\nAccelerate application modernization Automatic Language Upgrade (Java, .NET) Reduce Technical Debt Save Costs and Improve Operational Efficiency Enhance Competitive Advantage Kiro application: Specification-driven development solution\nKiro helps developers and engineering teams ship high-quality software with AI agents. Kiro turns your prompts into clear requirements, system designs, and discrete tasks. Iterate with Kiro on your specifications and architecture. Kiro agents implement the specification while keeping you in control. Agent hook\nDelegate tasks to AI agents triggered by events such as ‘file save’ Agents auto-execute in the background based on your predefined prompts Agent hooks help you scale work by generating documentation, unit tests, or code performance optimizations Advanced context management\nConnect to documents, databases, APIs, and more with native MCP integrations Configure how you want Kiro agents to interact with each project via directive files Drop in an image of your UI design or a photo from your architecture discussion and Kiro can use it to guide implementation Power, flexibility, and security\nCompatible with VS Code\nKiro supports VS Code Open VSX plugins, themes, and settings in a streamlined AI-ready environment Advanced Claude models\nChoose between Claude Sonnet 3.7 or Sonnet 4, with more options coming soon Enterprise-grade security\nKiro is built and operated by AWS Use cases\nBuild new applications\nQuickly go from prototype to production code and deployment, with best practices baked in, including structured design, runbooks, or test coverage scope Build on existing applications\nWith smart specification and context management, Kiro makes it easy to integrate and extend existing applications while maintaining consistency Refactor and modernize\nKiro understands your codebase and can precisely guide refactoring across codebases exceeding one million LOC 4. Learn about AI-driven cloud modernization tailored for VMware environments The future state of your VMware workloads\nRELOCATE: Amazon EVS | REHOST: Amazon EC2 | REPLATFORM TO CONTAINERS: Amazon ECS or Amazon EKS | REPLATFORM TO MANAGED SERVICES: Amazon RDS, Amazon FSx, Amazon WorkSpaces, and more | REFACTOR: Modern Application =\u0026gt; Rapid adoption, foundational cloud benefits and quick ROI....................----\u0026gt;....................All native cloud benefits and high ROI AWS Transform for VMware\nModernize VMware workloads onto Amazon EC2 with purpose-built AI agents Automate and simplify transformation tasks Reduce costs and licensing fees with Amazon EC2 Enhance security, scalability, and resilience Drive innovation with over 200 AWS native services Mapping VMware native technologies to AWS\nAn agentic AI-based approach to VMware modernization\n1. Connect to your VMware environment | 2. Analyze workloads, dependencies, and readiness | 3. Transform VMware network configurations into AWS native constructs | 4. Generate intelligent wave plans based on application dependencies | 5. Validate with your team, then execute =\u0026gt; Step-by-step transformation with human-in-the-loop validation Why AWS Transform for migrating from VMware?\nLower costs\nEliminate VMware licensing fees Optimize infrastructure costs with AI-driven instance right-sizing Faster migration\nAccelerate network transformation up to 80× Minimize disruption, preserve application integrity, and speed up the transition Improved security\nStrengthen security with a cloud-native foundation Migrate safely with a human-in-the-loop validation process Innovation at scale\nReduce technical debt and build modern, scalable applications Seamlessly integrate with over 200 AWS native services like data lakes, advanced analytics, and AI/ML 5. Connect and learn directly from AWS Solutions Architects and industry experts In this part, experts presented the challenges faced in the early stages of full-system modernization from on-premises to AWS.\nThey proposed specific plans and strategies for each area, executing the most critical parts first. They also adhered to current regulations and laws in management and did not collect user information. Once on AWS, the most important factor is the ability to scale rapidly, which leads to substantial gains when moving to the AWS environment. Applying AI is proving highly effective in their businesses—for example, anh Vinh has applied AI to detect potentially fraudulent transactions and defend against hackers in Blockchain. 6. Understand AWS security best practices from development to production environments What I Learned 5-step framework: Align → Assess → Mobilize → Modernize → Reinvent. GenAI-assisted modernization: code transformation (Java 8→21, .NET→8), dependency mapping, environment assessment. Prioritize high-impact workloads, human-in-the-loop, measure ROI. Design Thinking Problem→Pilot→Scale; prioritize value-first. Strangler Fig refactor by parts; event-driven mindset. Platform thinking/IDP, security-by-design, early governance. Technical Architecture Microservices, containers (EKS/ECS/Fargate), serverless (Lambda/Step Functions/EventBridge). Data: Aurora MySQL, MSK (Kafka), ElastiCache (Redis). VMware→AWS: rehost EC2 → replatform containers/managed → refactor app. Multi-arch (x86_64 + ARM64), end-to-end observability. Modernization Strategies Assess/Mobilize/MM/Reinvent (Techcombank blueprint). AWS Transform: for VMware \u0026amp; .NET (automated migration/testing/UI modernization). Cost-first: drop Windows/SQL licenses, right-size, ARM64. Scale \u0026amp; Innovate: decompose monolith, CI/CD automation, adopt GenAI. Application to Work Build a migration backlog by ROI; select a small pilot. Standardize container baseline (EKS) + Bedrock pattern for GenAI. Use Amazon Q/Transform to upgrade languages \u0026amp; refactor quickly. Design an internal IDP: service templates, golden paths, policy guardrails. Experience at the event “GenAI-powered Migration \u0026amp; Modernization provided a comprehensive view of transforming applications \u0026amp; databases at enterprise scale. Highlights: demos of automated VMware/.NET migration, serverless–container reference architectures, quantitative ROI lessons and battle-tested governance models, along with case studies that significantly shortened migration time and reduced costs.”\nLearning from highly specialized speakers Techcombank: operate with CCoE, measure business outcomes, 5-step roadmap. Ninety Eight: AI anti-fraud, strong security posture, real-time. OCB/LPBankS: data products, automation, safe cloud scale. Hands-on technical experience Clear view of dependency mapping, wave planning, auto SG/EC2 sizing, hub-and-spoke VPC. Auto code upgrade, cross-platform .NET, automatic UI modernization. Modern tool adoption AWS Transform (VMware/.NET), Amazon Q (auto language upgrade). Bedrock, Lambda, ECS/Fargate, EKS, Step Functions, EventBridge. Aurora, MSK, ElastiCache, EC2; IDP tooling; Kiro (spec→tasks/agents, MCP). Networking and exchange Finalize best practices from SAs \u0026amp; major banks; governance/security checklists. Connect for mentorship, pattern reuse, ROI comparison \u0026amp; benchmarking. The event created opportunities to interact directly with experts, peers, and business teams, helping to strengthen the ubiquitous language between business and tech. Lessons learned Applying DDD and event-driven patterns reduces coupling and increases scalability and resilience.\nA modernization strategy needs a phased approach and ROI measurement; avoid rushing a full system overhaul.\nAI tools like Amazon Q Developer can boost productivity when integrated into the current development workflow.\nModernize with strategy: measure ROI, prioritize by value.\nAutomation + GenAI shortens timelines and reduces technical debt.\nPlatform/IDP is a scale lever; security-by-default is indispensable.\nHuman-in-the-loop ensures safety for broad automation.\nSome photos from the event Over 400 passionate technology developers in Thành phố Hồ Chí Minh gathered at the AWS office (36th Floor) to watch the live plenary session from Hà Nội, sharing the excitement and knowledge about AWS Cloud Day Vietnam 2025\nOverall, the event not only provided technical knowledge but also changed my mindset about application design, system modernization, and more effective cross-team collaboration.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Redirecting Internet-bound traffic through a transparent forward proxy by Vijay Menon | on 08 SEP 2025 | in Amazon VPC, AWS Transit Gateway, AWS Transit Gateway network manager, Networking \u0026amp; Content Delivery | Permalink | Share\nCentralized egress is the principle of using a single, common inspection point for all network traffic going out to the Internet. This approach benefits security by limiting exposure to malicious resources accessible from outside, such as malware command and control (C\u0026amp;C) infrastructure. This inspection operation is typically performed by firewalls like AWS Network Firewall, and customers often want to insert a forward proxy in the path with or without a firewall. Proxies can operate in two modes: explicit proxy mode, where every client needing Internet access is configured to use an explicit proxy and sends supported outbound traffic through explicit proxy configuration. Such an implementation is described in How to set up an outbound VPC proxy with domain whitelisting and content filtering. However, there are systems that cannot be configured with explicit proxy, and customers often look for ways to transparently redirect traffic from such systems to the Internet through a proxy.\nIn this post, I explain an architecture that can be used to deploy a proxy on the egress path to the Internet and transparently redirect traffic to it using Web Cache Communication Protocol (WCCP). I will present a conceptual overview of implementing transparent proxy with WCCP in an AWS Transit Gateway-based architecture. Specific implementation details may vary depending on your organization\u0026rsquo;s requirements and chosen technology.\nReal-world use cases Here are three real-world use cases.\nSecure internet access: Organizations can use this architecture to ensure all outbound Internet traffic goes through security controls without requiring explicit proxy configuration on each client device.\nData loss prevention: Transparent traffic inspection allows businesses to detect and block sensitive data from leaving the network, even when endpoints don\u0026rsquo;t support explicit proxy configuration.\nRegulatory compliance: Industries with strict compliance requirements can perform continuous inspection and logging of all network traffic to meet regulatory standards.\nPrerequisites Before starting, you should be familiar with the following AWS networking concepts and services:\nAmazon Virtual Private Clouds (Amazon VPCs) Route tables AWS Transit Gateway Transit Gateway Connect attachments AWS Network Firewall NAT Gateway Third-party virtual appliances (WCCP-compliant routers and proxy servers) with WCCP functionality. Architecture overview The architecture focuses on an Egress VPC containing WCCP-compliant virtual routers and proxy servers running on Amazon Elastic Compute Cloud (Amazon EC2). These virtual routers redirect traffic to the proxy using WCCP. This setup allows transparent inspection and filtering of traffic without requiring client-side configuration changes. Figure 1 shows the diagram, followed by architectural details.\nFigure 1: Overall architecture diagram\nKey components Transit Gateway: Transit Gateway serves as the central hub for network connectivity, enabling simplified management of connections between VPCs and on-premises networks. In our architecture, Transit Gateway routes traffic between multiple Spoke VPCs, as well as between Spoke VPCs and the Internet through the Egress VPC for inspection.\nTransit Gateway Connect attachments: Connect attachments provide the ability to integrate third-party virtual appliances with Transit Gateway. In our design, the attachments:\nConnect Transit Gateway with WCCP-compliant routers through Generic routing encapsulation (GRE) running on Connect attachment Enable traffic redirection without modifying route tables in each VPC Provide flexibility for scaling and high availability You can refer to Simplify SD-WAN connectivity with AWS Transit Gateway Connect as a foundation for implementing Transit Gateway with Connect attachments. You can then modify the implementation to suit the requirements outlined in this post.\nEgress VPC: The Egress VPC is a specialized environment:\nContains WCCP-compliant routers and proxy servers running on Amazon EC2 in private subnets Contains firewalls in separate private subnets Enforces centralized security policies using proxies and firewalls Uses NAT Gateway in public subnets for Internet connectivity through Internet Gateway (IGW) WCCP-compliant routers running on Amazon EC2: These are core components of the transparent proxy implementation:\nLocated inline in the traffic path from Spoke VPCs to the Internet Intercept traffic based on configured policies Redirect specific traffic types to proxy servers Return processed traffic to the original path Proxy servers running on Amazon EC2: These are Amazon EC2-based virtual appliances operating as forward proxies (e.g., Squid) that interact with and receive redirected traffic from the previously mentioned WCCP routing devices. Using WCCP, they apply configured policies to all egress traffic and forward to the next security function in the path. In this example, we use AWS Network Firewall. However, these proxies can be daisy-chained with other security/inspection systems, such as Data Loss Protection (DLP) systems, in the path before traffic is routed to the Internet.\nConnectivity As shown in Figure 1, Spoke VPCs connect to Transit Gateway via VPC attachments. These attachments are associated with the Spoke VPC Route Table (routing will be explained in a later section). The Egress VPC connects to the same Transit Gateway via VPC attachment, and a Connect attachment is provisioned between Transit Gateway and Egress VPC through this VPC attachment. This Connect attachment is used to establish GRE tunnels between Transit Gateway and virtual appliances in the Egress VPC, acting as WCCP routers. This Egress VPC has Internet connectivity through an attached IGW.\nRouting (refer to Figure 1) Spoke VPCs: Each Spoke VPC needing Internet access has subnets attached to route tables containing a default route pointing to the Transit Gateway attached to that VPC. This configuration routes all Internet-bound traffic to the Transit Gateway, where the Spoke VPC\u0026rsquo;s VPC attachment is associated with the Transit Gateway Spoke Route Table.\nTransit Gateway Configuration:\nThe Transit Gateway Spoke Route Table contains propagated routes for all Spoke VPCs and a default route propagated from the Connect attachment. This default route is created by the WCCP virtual appliance in the Egress VPC and advertised to Transit Gateway through BGP peering over the GRE tunnel.\nOn Transit Gateway, the Egress VPC attachment is associated with the Egress Route Table. This table contains routes propagated to the Egress VPC CIDR from the Egress VPC attachment, as well as routes propagated to all Spoke VPC CIDRs from their respective attachments.\nThe Connect attachment is associated with its own route table. This route table contains propagated Spoke VPC routes as well as the default route advertised by the WCCP router over the BGP session through the GRE tunnel.\nEgress VPC Configuration: The Egress VPC requires additional routing configuration to enable transparent proxy functionality.\nWCCP router devices in private subnets establish GRE tunnels with Transit Gateway and create BGP peering sessions.\nThrough these BGP sessions, WCCP routers advertise the default route to Transit Gateway and receive all Spoke VPC prefixes.\nWCCP routers redirect Internet-bound traffic to forward proxies in the same subnet using WCCP (detailed discussion will be provided later in this post).\nSubnet-specific Routing: private subnets (WCCP Router and Proxy servers).\nRoute tables have entries for Transit Gateway CIDR connect attachment peers pointing back to Transit Gateway.\nDefault route points to the firewall in a separate private subnet.\nPrivate subnets (firewalls):\nDefault route points to NAT Gateway in the public subnet of the corresponding AWS Availability Zone (AZ).\nRoutes to WCCP router ENI in the same AZ for return traffic not matching WCCP redirect rules.\nPublic Subnets (NAT Gateways):\nDefault route points to IGW.\nRoutes to firewall endpoint in the same AZ for return traffic not matching WCCP redirect rules.\nWCCP redirection All Internet-bound traffic to WCCP routers is evaluated against configured WCCP redirect rules and forwarded to proxies in the same subnet. For redundancy, these WCCP routers configure WCCP redirection to connect with proxies in other AZs. WCCP redirection can be configured to handle all supported traffic or selected traffic types. Configuration mechanisms vary by WCCP virtual appliance vendor. Traffic to the proxy is evaluated and processed according to configuration, then forwarded to the firewall on the Internet egress path. As mentioned, if needed, proxies can be chained with other functions like DLP.\nPacket walk-through The following figure outlines the packet journey.\nFigure 2: Architecture diagram with packet flow\nForward path: This section explains how packets travel from resources in Spoke VPCs to destinations on the Internet through proxies and firewalls in the Egress VPC.\n(A) Traffic from sources in Spoke VPCs destined for the Internet follows the default route in the subnet route table and enters Transit Gateway.\n(B) Once in Transit Gateway, it follows the route in the Spoke Route Table on Transit Gateway, which forwards traffic to the Transit Gateway Connect attachment. Multiple GRE tunnels run over the Connect attachment, all advertising default routes to Transit Gateway, so Transit Gateway uses Equal Cost Multi-Pathing (ECMP) to send traffic to WCCP virtual appliances in the Egress VPC.\n(C) The WCCP-virtual appliance intercepts traffic and determines if inspection is needed. Traffic requiring inspection is redirected to proxy servers, where security policies, content filtering, or other controls are applied before forwarding to Network Firewall.\n(CD) Traffic not matching WCCP redirect rules exits the WCCP virtual appliance and goes directly to Network Firewall according to the subnet route table.\n(D) Proxied traffic is Source NATed to the proxy\u0026rsquo;s IP address and leaves the proxy for Network Firewall according to the subnet route table. This table has a default route pointing to the Network Firewall endpoint in the same AZ of the Egress VPC.\n(E) Network Firewall inspects the traffic and, if allowed, sends it to NAT Gateway in the same AZ of the Egress VPC according to the default route configured in its subnet route table.\n(F) NAT Gateway performs Source NAT on the traffic and sends it to the destination on the Internet.\nReverse path: This section explains how return packets travel from the Internet to resources in Spoke VPCs through proxies and firewalls in the Egress VPC.\n(G) Return traffic from the Internet enters the same NAT Gateway due to Source NAT.\n(H) According to the subnet route table, NAT Gateway forwards traffic to Network Firewall in the same AZ.\n(I) Network Firewall sends traffic to the proxy server in the same AZ if this flow in the forward direction matched WCCP redirect rules and was Source NATed by the proxy server.\n(J) The proxy server sends return traffic of the proxied flow to the WCCP router using WCCP configuration on the proxy server.\n(IJ) Return traffic of flows not matching redirect rules is sent by Network Firewall to the WCCP router according to the subnet route table.\n(K) Multiple GRE tunnels exist from WCCP routers to Transit Gateway via Connect attachment. Spoke VPC CIDRs are advertised to WCCP routers through these tunnels. WCCP routers use ECMP to send traffic to Transit Gateway through GRE tunnels.\n(L) Transit Gateway uses the Connect attachment route table to forward traffic to the corresponding Spoke VPC attachment.\n(M) In the Spoke VPC, traffic follows local routes to reach the destination from the Transit Gateway ENI.\nConsiderations Consider the following four points when implementing this solution.\nWCCP virtual appliance sizing Ensure proper sizing of WCCP virtual appliances to handle expected traffic volumes. You can refer to Amazon EC2 instance network bandwidth documentation to select EC2 instances supporting your traffic requirements. Plan for redundancy by deploying across multiple AZs. Component-level redundancy (for WCCP routers or Proxy servers) can be implemented using automatic instance recovery. WCCP configuration Define service groups to classify traffic for different inspection policies as needed. Different service groups can be used for primary and secondary proxy servers. Configure redirection according to your requirements. You can use multiple attributes like port, protocol, source/destination addresses. Implement authentication between WCCP routers and proxy servers (outside the scope of this post). Proxy selection Choose proxy solutions supporting WCCP integration. Consider performance requirements based on expected traffic volumes. Refer to Amazon EC2 instance network bandwidth documentation to select EC2 instances supporting your traffic requirements. Monitoring and logging Implement comprehensive logging for both WCCP routers and proxy servers. Below are some examples of logs to collect. Since WCCP and proxy implementation depends on vendor/solution, these logs may be called by different names. Please refer to vendor documentation for details: On WCCP router:\nWCCP service status, packet redirection, and router-to-proxy communication logs Interface traffic statistics ACL match counts for WCCP-related rules NetFlow/sFlow data for traffic analysis\nOn proxy server: Access logs and Error logs (client IP, URL, status code, bytes transferred, timing, connection failures, timeouts) WCCP service logs (service registration, group membership) Cache performance logs (hit/miss ratio) Connection handling logs (TCP connections, handshakes) Authentication logs (if applicable) SSL/TLS inspection logs (for HTTPS traffic) System resource usage (CPU, memory, disk I/O) Set up alerts for traffic anomalies or proxy health issues, and create dashboards to visualize traffic patterns and security events. You can export the previously mentioned logs to Amazon CloudWatch to set up alerts and create dashboards. Conclusion\nImplementing transparent proxies using WCCP with AWS Transit Gateway provides a powerful solution for organizations looking to enhance their security posture without disrupting user experience. Centralizing security controls in an egress VPC and using WCCP-compatible routers allows organizations to achieve comprehensive traffic inspection while maintaining network performance and scalability.\nThis architecture provides flexibility to adapt to changing security requirements and increasing network demands, making it an excellent choice for enterprises looking to strengthen their cloud security infrastructure. For more information and architectural options for centralized egress, visit the AWS Whitepaper, Centralized egress to internet.\nVijay Menon Vijay Menon is a Principal Solutions Architect working in Singapore, with a background in large-scale networking and telecommunications infrastructure. He enjoys learning new technologies and helping customers solve complex technical problems by providing solutions using AWS products and services. When not supporting customers, he enjoys long-distance running and spending time with family and friends. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Building resilient multi-Region Serverless applications on AWS by Vamsi Vikash Ankam | on 08 SEP 2025 | in Advanced (300), Resilience, Serverless, Technical How-to | Permalink | Share\nMission-critical applications demand high availability and resilience against potential disruptions. In online gaming, millions of players connect simultaneously, making availability challenges particularly apparent. When gaming platforms fail, players lose progress, tournaments are disrupted, and brand reputation suffers. Traditional environments often over-provision compute resources to address these challenges, leading to complex setups along with high infrastructure and operational costs. Modern Amazon Web Services (AWS) serverless infrastructure offers a more efficient approach. This post presents architectural best practices for building resilient serverless applications, illustrated through implementing a multi-Region serverless authorization mechanism.\nOverview Recognizing the importance of availability only after experiencing disaster is too late. Applications can fail for many reasons such as infrastructure failures, source code errors, configuration errors, unexpected traffic surges, or service disruptions at the regional level. Critical business services such as authentication systems, payment processors, and real-time gaming features require high availability. To minimize impact on user experience and business revenue, establish bounded recovery times for critical services during incidents.\nAWS serverless architectures inherently provide high availability through multi-Availability Zone (AZ) deployment and integrated scalability. These services minimize infrastructure management while operating on a pay-for-value pricing model at the Regional level. The AWS serverless pay-for-value model enables cost-effective multi-Region deployment, making it an ideal choice for building resilient architectures.\nFigure 1: Diagram showing failure causes, their impact and frequency of occurrence\nThe previous diagram maps failures—from common operational errors to rare catastrophic events. It guides organizations to prioritize multi-Region recovery strategies based on occurrence probability and potential business impact.\nRegional decisions To determine the appropriate multi-Region approach, carefully evaluate the following factors:\nAssess whether your Recovery Time Objective (RTO) and Recovery Point Objective (RPO) requirements can be met within a single Region, or if a multi-Region architecture is necessary to achieve your recovery objectives. Whether the business benefits of multi-Region redundancy outweigh the operational costs of data replication, synchronization, and increased deployment costs and complexity. Evaluate whether data sovereignty laws, compliance requirements, or geographic restrictions prevent data replication between specific AWS Regions. Ensure that Regions selected in a \u0026ldquo;multi-Region\u0026rdquo; solution have compatible service capabilities, quota limits, and pricing appropriate to your needs. After evaluating these requirements, if the organization determines the need for multi-Region workloads, they must choose between two architectural patterns: Active-Passive or Active-Active deployment. Each pattern offers distinct benefits and trade-offs regarding resilience, cost, and operational complexity.\nMulti-Region deployment patterns The following sections outline different multi-Region deployment patterns: Active-Passive, and Active-Active.\nActive-Passive In this pattern, one AWS Region serves as the \u0026ldquo;Active\u0026rdquo; Region, handling all production traffic, while other Region(s) remain in \u0026ldquo;Passive\u0026rdquo; state, as shown in the following figure. Passive Region(s) replicate data and configuration from the Active Region without serving requests, and are ready to handle when disruptions occur in the \u0026ldquo;Active\u0026rdquo; Region. Depending on application criticality, passive Regions deploy different levels of infrastructure readiness: fully deployed infrastructure (Hot Standby), partial deployment (Warm Standby), or minimal core infrastructure (Pilot Light).\nTraditional Active-Passive architectures require significant investment in idle infrastructure: load balancers, auto-scaling groups, running compute resources, and monitoring systems. Organizations can use AWS serverless applications with pay-for-value pricing model to primarily pay for data replication costs, not idle compute resources. AWS manages the underlying infrastructure, eliminating most operational costs.\nService quotas, API limits, and concurrency settings must correspond across AWS Regions to provide seamless failover capability. AWS Lambda provides provisioned concurrency to keep functions warm and responsive, particularly useful for secondary Regions during failover. It helps reduce cold starts by maintaining warm execution environments, so the system can handle traffic surges with fewer cold starts. Note that provisioned concurrency incurs compute costs regardless of usage level. Consider implementing auto-scaling for provisioned concurrency based on traffic patterns to optimize costs during idle periods.\nThis pattern suits organizations seeking cost-effective disaster recovery (DR) solutions, because AWS serverless costs only apply when resources are actively used in the secondary Region. Managed services like Amazon DynamoDB Global Tables and Amazon Aurora Global Database handle data replication, simplifying deployment. The serverless authorization mechanism presented in the following section illustrates this pattern in practice.\nFigure 2: Active-Passive pattern with dotted lines indicating standby Regions, while Active-Active pattern serves traffic simultaneously\nActive-Active In this pattern, multiple Regions serve traffic simultaneously, distributing load and providing rapid failover capability. Active-Active architecture is expensive and designed for the highest availability level. However, they don\u0026rsquo;t by default provide DR for all potential failure types. This approach suits applications requiring geographic routing, or demanding the highest availability.\nActive-Active deployment requires rigorous engineering to handle data synchronization and resolve conflicts. Each Region must be sized to carry the full application load if another Region experiences service degradation. Active users are distributed across AWS Regions, so disruption in one Region will redirect all traffic to remaining Regions, requiring them to handle the combined load. To improve application resilience, implement retry mechanisms, circuit breaker, and fallback strategies. Plan for static stability by pre-provisioning capacity and applying client-side caching. Services like Amazon Route 53 with latency-based routing and Amazon DynamoDB Global Tables with strong consistency provide foundation but require thorough testing under various failure scenarios. This post does not cover Active-Active deployment.\nMulti-Region serverless authorizer To illustrate the Active-Passive scenario, we build a sample application showing how to build a multi-Region serverless authorizer using Amazon API Gateway, Lambda functions and Amazon Route53. Modern gaming and entertainment platforms host critical services like player matchmaking, live streaming, and real-time sports analytics. These services depend on robust authorization systems—when authorization fails, players cannot join matches, viewers lose stream access, and live events become unavailable. This post presents how to build a fault-tolerant, multi-Region serverless authorizer while maintaining lower costs compared to traditional approaches.\nServerless multi-Region architecture typically includes Routing, Compute, and Data layers. When implementing multi-Region deployments, data replication between AWS Regions is essential, regardless of compute services used. The compute layer must prioritize idempotency to ensure safe event processing across AWS Regions.\nUse Powertools for Lambda to handle idempotency efficiently, or implement custom solutions using unique event IDs with DynamoDB as an idempotent store. While the post focuses on authorizer service deployment, this pattern can be applied to build multi-Region microservices for various critical functions, such as game session management, content distribution orchestration, user preference management, and profile services.\nDemo overview To illustrate multi-Region serverless authorizer operation, we can examine the workflow:\nFrontend application authenticates with Identity Provider to obtain authentication token.\nAuthentication token is sent to a resilient multi-Region DNS endpoint, hosted on Route 53 in this demo.\nRoute 53 routes requests with token to API Gateway in primary Region.\nRoute 53 monitors application health using a Lambda function simulated in this demo. In production environments, implement deep health checks to monitor the entire service stack.\nWhen authorization succeeds, application receives response. If Route 53 detects degradation in primary Region, it triggers Amazon CloudWatch alarm, which application owners can use to evaluate and approve traffic redirection to secondary Region.\nNew traffic will be routed to API Gateway in secondary Region after manual failover approval.\nRoute 53 health checks continue monitoring primary Region health and restore traffic routing when this Region recovers.\nFigure 3: Multi-Region serverless authorizer workflow with Route53 failover between primary and secondary Regions\nThe previous figure shows architecture illustrating both failover and fallback capabilities through CloudWatch alarms and manual approval process. This approach aligns with best practices for critical applications, where automatic failover is not recommended despite being technically feasible. Teams can use this to assess technical readiness, business impact, and make informed decisions about timing and potential revenue impact.\nThe demo implements multi-Region serverless authorizer serving as reference architecture. Real-world implementations need careful evaluation of failover strategies based on business criticality and operational requirements.\nTesting multi-Region scenarios\nThe demo application hosts frontend on Amazon Elastic Container Service (Amazon ECS). Route 53 health check configuration in this GitHub defines key failover parameters:\nFailureThreshold: Specifies the number of consecutive failed health checks before Route 53 marks endpoint as unhealthy. RequestInterval: Standard: 30-second cycle ($0.50 per health check/month) Fast: 10-second cycle ($1.00 per health check/month) Route53HealthCheck: Type: AWS::Route53::HealthCheck Properties: HealthCheckConfig: FailureThreshold: 2 FullyQualifiedDomainName: !Ref DomainName Port: 443 RequestInterval: 10 ResourcePath: /failure Type: HTTPS HealthCheckTags: - Key: Environment Value: Production - Key: Name Value: multi-authorizer-health-check Fast cycle enables faster failure detection. However, it increases health check costs through more logging operations, request processing, and backend compute resources. Temporary issues like network glitches, transient errors, or third-party latency may self-resolve within minutes. Implementing effective retry handling would introduce unnecessary complexity and potential data inconsistencies. Choose appropriate cycle based on business SLAs and cost considerations.\nTo test failover scenarios, the architecture uses a mock Lambda function as health check endpoint. We trigger CloudWatch alarm by simulating 500 response status code from this function, which will drive the manual failover decision process, as illustrated in the following figure.\nFigure 4: Console screenshot showing multi-authorizer-health-check with \"Unhealthy\" status\nDNS caching occurs at multiple layers (browser, operating system, ISP, and VPN). To observe immediate failover behavior, clear DNS resolver cache at each layer.\nFor more comprehensive resilience testing, consider applying chaos engineering practices. You can use chaos-lambda-extension to introduce latency or modify function responses in a controlled manner. AWS Fault Injection Service (AWS FIS), a fully managed service, enables executing fault injection experiments to improve resilience, performance and observability. Combining these tools helps validate multi-Region architecture under various controlled failure conditions.\nObservability in multi-Region deployment Implementing multi-Region architecture is only the first step. Cross-Region observability requires monitoring Region A resources from Region B and vice versa. CloudWatch enables this through cross-account and cross-Region monitoring, providing unified logs and metrics in a single dashboard. Implement deep operational health checks to verify critical application functionality across AWS Regions.\nAlthough AWS serverless services are distributed, identifying exact failures requires combining multiple data points. CloudWatch composite alarms help aggregate these signals, facilitating informed decisions. Consider implementing custom monitoring solutions to trace end-to-end requests across AWS Regions. This comprehensive view helps manage multi-Region complexity and respond quickly to potential issues.\nConclusion Building resilient multi-Region applications requires careful consideration of architectural patterns, costs, and operational complexity. AWS Serverless services, with their pay-for-value model, significantly reduce challenges when implementing multi-Region architecture. The authorizer pattern presented in this post shows how organizations can achieve high availability without bearing idle infrastructure costs like traditional approaches. Teams can follow these architectural patterns and best practices to build robust, cost-effective solutions that maintain service availability during disruptions.\nTo learn about resilience concepts, visit AWS Developer Center. Full source code of the demo used in this post is available in our GitHub repository. To expand serverless knowledge, visit Serverless Land.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"AWS Weekly Roundup: AWS Transform, Amazon Neptune, and more (September 8, 2025) by Esra Kayabali | on September 8, 2025 | in Amazon Bedrock, Amazon Elastic Container Service, Amazon Neptune, Announcements, AWS Transform, AWS User Notifications, Launch, News | Permalink | Comments | Share\nSummer has come to an end in Utrecht, where I live in the Netherlands. In two weeks, I will attend AWS Community Day 2025, hosted at Kinepolis Jaarbeurs Utrecht on September 24. This one-day event will bring together over 500 cloud professionals from across the Netherlands, with 25 breakout sessions spanning 5 technical tracks. The day will kick off with virtual keynotes at 9:00 AM, followed by parallel sessions focusing on practical implementations of serverless architectures and container optimization strategies, delivering value for all experience levels.\nLast year\u0026rsquo;s AWS Community Day Netherlands 2024 brought together a diverse community of practitioners, speakers and AWS enthusiasts, creating a community-led conference with high knowledge-sharing value. If you plan to attend, feel free to find me to discuss AWS services or share your cloud deployment experiences!\nLast week\u0026rsquo;s launches Let\u0026rsquo;s review last week\u0026rsquo;s new announcements.\nAWS Transform assessments now include detached storage – AWS Transform has expanded its assessment capabilities to analyze on-premises detached storage infrastructure, helping customers identify migration total cost of ownership (TCO). Assessments now evaluate Storage Area Network (SAN), Network Attached Storage (NAS), file servers, object storage, and virtual environments, while recommending appropriate target AWS services like Amazon S3, Amazon EBS, and Amazon FSx. The tool provides comprehensive TCO comparison between current environments and AWS, with performance and cost optimization recommendations. Since storage can represent up to 45% of total migration opportunities, this upgrade helps customers visualize migration options on AWS. AWS Transform assessment is now available in US East (N. Virginia) and Europe (Frankfurt).\nAmazon Bedrock now supports global cross-region inference for Anthropic Claude Sonnet 4 – The Anthropic Claude Sonnet 4 model in Amazon Bedrock now supports Global cross-Region inference, allowing inference requests to be routed to any supported commercial AWS Region for processing. This upgrade optimizes available resources and increases model throughput by distributing traffic across multiple Regions. Previously, you could select cross-Region inference profiles tied to each region (US, EU, APAC). The new Global cross-Region inference profile provides additional flexibility for generative AI without geographic constraints, helping handle unplanned traffic bursts and increase throughput. Detailed implementation guidance is available in Amazon Bedrock documentation.\nAmazon Neptune Database now supports Public Endpoints, simplifying development access – Amazon Neptune now supports Public Endpoints, allowing direct connections to Neptune databases from outside VPC without complex network configuration. This feature enables developers to securely access graph databases from personal development machines without VPN or bastion hosts, while still ensuring security through IAM authentication, VPC security groups, and encryption in transit. You can enable Public Endpoints for Neptune clusters running engine version 1.4.6 or later through AWS Management Console, AWS CLI, or AWS SDK. This feature has no additional charges beyond standard Neptune pricing, and is available in all AWS Regions where Neptune Database is offered. Implementation details are in Amazon Neptune documentation.\nECS Exec now supported on AWS Management Console – Amazon ECS now supports ECS Exec directly in AWS Management Console, enabling secure, interactive shell access to running containers without opening inbound ports or managing SSH keys. Previously only available via API/CLI/SDK, this feature makes troubleshooting convenient by accessing containers directly in the console. You can enable ECS Exec when creating/updating services and standalone tasks, then connect to containers by selecting \u0026ldquo;Connect\u0026rdquo; on the task details page, opening an interactive session via CloudShell. The console also displays corresponding AWS CLI commands for use on local machines. The feature is available in all AWS commercial Regions and documented in the ECS developer guide.\nGeneral availability of organizational notification configurations for AWS User Notifications – AWS User Notifications now supports Organizational Notification Configurations, enabling AWS Organizations users to configure and observe notifications centrally across the organization. Management accounts or delegated administrators can configure notifications for specific organizational units or for all accounts in the organization. The service supports configuring notifications for any supported Amazon EventBridge event, such as console sign-ins without MFA, with notifications appearing in administrators\u0026rsquo; Console Notifications Center and AWS Console Mobile Application. User Notifications supports up to 5 delegated administrators and is available in all AWS Regions where the service is offered. Implementation details are in AWS User Notifications user guide.\nFor a full list of AWS announcements, follow the What\u0026rsquo;s New at AWS page.\nUpcoming AWS events Mark your calendar and register for upcoming AWS events.\nAWS Summits – Free online and in-person events that bring the cloud computing community together to connect, collaborate and learn about AWS. Register in a city near you: Zurich (September 11), Los Angeles (September 17), and Bogotá (October 9).\nAWS re:Invent 2025 – See you in Las Vegas from December 1–5, where cloud pioneers from around the world gather to get AWS innovation updates, learn from peers, expert-led in-depth discussions and expand connections. Don\u0026rsquo;t forget to explore the event catalog.\nAWS Community Days – A series of community-led conferences featuring technical discussions, workshops, and hands-on labs led by expert AWS users and industry leaders sharing: Baltic (September 10), Aotearoa (September 18), South Africa (September 20), Bolivia (September 20), Portugal (September 27).\nBrowse all in-person and virtual events organized by AWS here.\nThat\u0026rsquo;s all for this week. See you next Monday in the Weekly Roundup!\n— Esra\nThis post is part of the Weekly Roundup series. Come back every week for a quick overview of interesting news and announcements from AWS!\nTAGS: Week in Review\nEsra Kayabali Esra Kayabali is a Senior Solutions Architect at AWS, specializing in analytics including data warehousing, data lakes, big data analytics, batch and real-time data streaming, and data integration. She has over ten years of experience in software development and solution architecture. She is passionate about learning collaboratively, sharing knowledge and leading the community on their cloud technology journey. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.3-knowledge-base/5.3.1-create-kb/","title":"Initialize Knowledge Base","tags":[],"description":"","content":"Target We will use the Amazon Bedrock Wizard to set up the entire RAG architecture. This process will connect the S3 data source, the Embedding model, and automatically initialize the Vector storage (OpenSearch Serverless).\nImplementation Steps Log in to the AWS Management Console and access the Amazon Bedrock service. In the left-hand menu, select Knowledge bases. Click the Create knowledge base button in the top right corner of the screen. Step 1: Configure Knowledge Base\nOn the first configuration screen:\nKnowledge base name: Enter knowledge-base-demo Knowledge Base description - optional: Enter Knowledge Base from AWS Overview (This section requires you to describe the data you have previously uploaded to S3). IAM permissions: Select the option Create and use a new service role. Service role name: Keep the default value suggested by AWS (starting with AmazonBedrockExecutionRoleForKnowledgeBase_...). Click Next. Step 2: Configure Data Source\nConnect to the S3 Bucket containing the documents:\nData source name: Enterknowledge-base-demo S3 URI:\nClick the Browse S3 button. In the pop-up window, select the bucket rag-workshop-demo you created in the previous section. Click Choose. Keep Default configurations. Click Next.\nStep 3: Storage \u0026amp; Processing\nThis is the most critical step to define the AI model and vector storage location:\nEmbeddings model:\nClick Select model. Select model: Titan Embeddings G1 - Text v2. Vector Store:\nVector store creation method: Choose Quick create a new vector store - Recommended Vector store type - new: Choose Amazon OpenSearch Serverless Note: This option allows AWS to automatically create an Amazon OpenSearch Serverless cluster to store data, saving you from manual infrastructure management. Click Next. Step 4: Review and Create Knowledge Base\nReview all configuration information on the Review page. Ensure the S3 URI and Model items are correct. Scroll to the bottom of the page and click the Create knowledge base button. Step 5: Wait for Initialization\nAfter clicking Create, the system will begin the background infrastructure initialization process for the Vector Store.\nWait time: Approximately 2 - 5 minutes. Note: Please do not close the browser during this time. Success: When the screen displays a green notification \u0026ldquo;Knowledge base created successfully\u0026rdquo;, you have completed this step and are ready for the next section. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Van Anh Duy\nPhone Number: 0387 883 041\nEmail: duynguyenvananh@gmail.com\nUniversity: FPT University Campus Ho Chi Minh\nMajor: Artificial Intelligent\nClass: SE181823\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Overview In this workshop, we will focus on building an intelligent AI assistant capable of \u0026ldquo;reading and understanding\u0026rdquo; and answering questions based on proprietary enterprise data (RAG technique).\nThe main objective is to establish a fully automated and serverless data processing workflow, consisting of the following steps:\nIngestion: Loading source documents into the system. Indexing: Converting text into vectors and storing them for retrieval. Retrieval \u0026amp; Generation: Configuring the AI model to search for relevant information and generate answers to user questions. 💡 Highlight: This solution allows you to eliminate the need to manage any server infrastructure, optimizing costs and operational time.\nContents RAG Explanation Service Introduction "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.2-prerequiste/5.2.1-model-access/","title":"Verify Model Access","tags":[],"description":"","content":"Overview According to AWS\u0026rsquo;s new policy, Foundation Models are often automatically enabled. However, for third-party partner models like Anthropic (Claude), first-time users in a new Region must declare usage information (Use Case) to be able to invoke the model.\nEnsure your AWS account has permission to access and use the Anthropic Claude 3 Sonnet model. This is a mandatory step to avoid AccessDenied errors when the Chatbot operates later. If this is the first time you are using this model in a new Region, you need to declare the intended use (Use Case).\nAccess Check We will perform a quick test (Test Run) to ensure your account is ready.\nIn the search bar, access the Amazon Bedrock.\nStep 1. Access Chat Playground\nIn the left menu of the Bedrock Console, find Playgrounds. Click Chat. Step 2. Select Test Model\nClick Select model (above the chat box). Category: Select Anthropic. Model: Select Claude 3 Sonnet (or Claude 3.5 Sonnet). Throughput: Select On-demand. Click Apply. Step 3. Send Activation Message\nIn the chat box: Enter Hello.\nClick Run.\nObserve result:\nIf AI answers: Success (Proceed immediately to section 5.2.2). If a red error or \u0026ldquo;Submit use case details\u0026rdquo; popup appears: Information declaration required (Continue to step 4 below).\nStep 4. Submit Use Case (Only perform if error occurred in step 3)\nClick Submit use case details (in the error message). Fill in the form: Company Name: Enter Personal Learning. Company website URL: Enter https://daihoc.fpt.edu.vn/ Industry: Select Education. Select Intended Use Describe\u0026hellip; Enter Research \u0026amp; Development. Click Submit. Wait 1 minute, return to the chat box, Click Run on the Hello message again to confirm success. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect with teammates and get acquainted with the Champions in the First Cloud Journey. Learn about the AWS services provided to customers. Complete the Lab as well as the knowledge in Module 1 of FJC 2025. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Read carefully the notes, rules, and regulations at AWS\n- Connect and interact with team members\n- Form groups and plan study schedules as well as project timing needed for the internship at the company.\n- Create a Google Sheet to manage study activities and track the progress of team members. 08/09/2025 08/09/2025 Module 01 3 - Learn about the basic concepts:\n+ Understand what cloud computing is.\n+ What are the benefits of using cloud computing?\n+ What makes AWS different.\n+ How to start a cloud journey? 09/09/2025 09/09/2025 Module 01 4 - Learn about AWS global infrastructure.\n- Learn about AWS Services Management tools.\n- How to optimize costs on AWS.\n- Practice Lab 01:\n+ Create an AWS account.\n+ Set up MFA for the account and understand what MFA is.\n+ Create Admin Group and Admin User.\n+ Experiment with AWS Support. 10/09/2025 10/09/2025 Module 01 5 - Practice Lab 07:\n+ Create Budget\n+ Create Cost Budget\n+ Create Usage Budget\n+ Create RI Budget\n+ Create Saving Plan Budget\n+ How to delete resources.\n- Learn about the services of each Budget\n- And identify which Budgets are suitable for which users. 11/09/2025 11/09/2025 Module 01 6 - Practice Lab 09:\n+ Learn about AWS Support packages\n+ Access AWS Support\n+ Initiate support requests. 12/09/2025 12/09/2025 Module 01 Week 1 Achievements: Understand what AWS is and the concepts and services AWS provides:\nCloud computing The differences of AWS How to start a cloud journey AWS global infrastructure AWS Services management tools How to optimize costs on AWS Successfully created and configured an AWS Free Tier account.\nGet familiar with AWS Management Console and know how to find, access, and use services from the web interface.\nInstalled and configured AWS CLI on the computer including:\nAccess Key Secret Key Default Region MFA for the account Admin Group Admin User AWS Support Know how to set up Budgets:\nCost Budget Usage Budget RI Budget Saving Plan Budget Clean resources Clearly understand the support packages and how to access AWS Support:\nBasic Package Developer Package Business Package Enterprise Package "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.1-workshop-overview/5.1.1-whatisrag/","title":"What is Retrieval-Augmented Generation (RAG)","tags":[],"description":"","content":"Brief Definition RAG (short for Retrieval-Augmented Generation) is a technique or software architecture in the field of Artificial Intelligence (AI), designed to optimize the output of a Large Language Model (LLM).\nIn essence, RAG is a combination of two mechanisms:\nInformation Retrieval Mechanism: Searching for data from a highly reliable External Knowledge Base. Text Generation Mechanism: Using the LLM\u0026rsquo;s language understanding and synthesis capabilities to generate natural responses. The goal of RAG is to provide the LLM with accurate, up-to-date, and specific context, helping the model overcome the limitations of static training data.\nWhy is RAG needed? Traditional LLM models often face 3 major problems that RAG can solve:\nInformation Updates (Freshness): The LLM does not need Re-training or Fine-tuning yet can still answer the latest information, simply by updating the search database. Data Ownership (Proprietary Data): Allows AI to answer questions regarding private enterprise data (internal documents, code base, customer information) that the original model does not know. Authenticity (Grounding): Minimizes \u0026ldquo;Hallucination\u0026rdquo; (AI fabricating information) by forcing the AI to cite or rely on actual text passages found. Operational Architecture The process of handling a question in RAG proceeds as follows:\nStep Name Action Description 1 Retrieval (Truy xuất) The system searches for text segments most relevant to the question in the data repository (usually using a Vector Database). 2 Augmentation (Tăng cường) Combine the user\u0026rsquo;s question + The data just found into a complete \u0026ldquo;prompt\u0026rdquo;. 3 Generation (Tạo sinh) Send that prompt to the AI (LLM) for it to synthesize and write out the final answer for the user. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Get familiar with AWS platforms and learn about the FCJ 2025 program\nWeek 2: Networking on AWS (AWS VPC, VPC Peering \u0026amp; Transit Gateway, VPN \u0026amp; Direct Connect, Elastic Load Balancing)\nWeek 3: Compute VM Services on AWS (Amazon EC2, Amazon Lightsail, Amazon EFS/FSx, AWS Application Migration Service (MGN))\nWeek 4: Storage Services on AWS (S3, Snow Family, Amazon Storage Gateway, Disaster Recovery on AWS, AWS Backup)\nWeek 5: Security Services on AWS - \u0026ldquo;Security is job zero\u0026rdquo; (Shared Responsibility Model, AWS Identity and Access Management, Amazon Cognito, AWS Organizations, AWS Identity Center (SSO), AWS Key Management Service - KMS, AWS Security Hub)\nWeek 6: Database Services on AWS (Database Concepts, Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon ElastiCache)\nWeek 7: Learning and supplementing knowledge about AWS Cloud, based on Skill Builder courses, completing Cloud Practitioner Certificate training\nWeek 8: Learning and supplementing knowledge about NLP and FastAPI to apply to the final project\nWeek 9: Project deployment Sprint 01 - Research on Speech to Text and OCR models\nWeek 10: Project deployment Sprint 02 - Improvement and quality enhancement of models\nWeek 11: Project deployment Sprint 03 - Getting familiar with AWS support services and applying them to the project\nWeek 12: Project deployment Sprint 04 - Testing all models and error handling\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Learning Notes from \u0026ldquo;AI-Driven Development Lifecycle: Reinventing Software Engineering\u0026rdquo; Event Objectives Understand how AI can automate and optimize each stage of the Software Development Lifecycle (SDLC). Grasp the philosophy of AI augmenting humans rather than replacing them in the application development process. Directly observe how Amazon Q and other AI tools support developers from ideation, coding, to infrastructure deployment (Infrastructure as Code – IaC). Recognize the \u0026ldquo;AI-first development\u0026rdquo; trend – where AI becomes a natural part of future software development processes. Speaker List Mr.Toan Huynh - PMP, Senior Solutions Architect, AWS Ms.My Nguyen - Senior Solutions Architect, AWS Highlighted Content Challenges in AI-Assisted Programming The opening section presented limitations and challenges when integrating AI into programming:\nAI cannot yet handle projects with complex logic requiring deep understanding of business context. Developers struggle to control details in generated code if they don\u0026rsquo;t clearly describe objectives and scope. Code quality heavily depends on prompts and context provided by users. This is precisely why AI-DLC was created: to establish a structured process enabling more effective collaboration between AI and humans.\nAI in Development – How AI is Changing Software This section analyzed how AI is transforming the software industry:\nAI assists in code generation, technical documentation, API design, and automated testing. Developers transition from \u0026ldquo;code writers\u0026rdquo; to \u0026ldquo;AI orchestrators\u0026rdquo; — coordinators who guide and evaluate outputs. Tools like Amazon Q, GitHub Copilot, ChatGPT for Developers become central tools in modern dev team workflows. Introduction to AI-DLC AI-Driven Development Lifecycle (AI-DLC) is a software development approach with AI collaboration, where each step is designed to provide AI with specific context and objectives to generate more accurate results.\nInception\nBuild Context on Existing Codes – AI is \u0026ldquo;fed\u0026rdquo; with current source code to understand project structure. Elaborate Intent with User Stories – Developers describe requirements through user stories, clarifying objectives. Plan with Units of Work – Break down work into small units for AI to execute and generate code incrementally. Construction\nDomain Model (Component Model) – Build domain models or logical architecture diagrams. Generate Code \u0026amp; Test – AI generates code and automated tests based on planned information. Add Architectural Components – Add architectural components like APIs, data layers, logging, security. Deploy with IaC \u0026amp; Tests – Automatically deploy systems with Infrastructure as Code and integration tests. Each step provides additional \u0026ldquo;rich context\u0026rdquo; for the next step, helping AI understand the system more deeply and generate increasingly accurate results.\nCORE CONCEPTS – Three Core Principles Context Awareness – AI needs clear context about code, requirements, and domain to operate effectively. Collaborative Generation – Humans and AI collaborate: AI generates code, humans guide and review. Continuous Refinement – Iterative process to refine outputs and improve quality. Mob Elaboration Mob Elaboration is a method for expanding requirements (intent elaboration) through team collaboration:\nMultiple members jointly describe requirements, ask questions, and add information for AI. Helps AI understand more deeply about business, objectives, and complex project logic. This approach helps reduce misunderstanding risks, especially in large or multi-domain teams. 5-Stage Sequential Process of AI-DLC AI-DLC is executed through 5 stages:\nInception – Understand requirements, analyze systems. Construction – Create domain models and initial structure. Generation – Automatic code generation. Testing – Automate unit and integration testing. Deployment – Deploy applications with IaC and CI/CD pipelines. Each iteration helps AI learn more and improve output quality.\nDemo 1 – Hands-on Experience with AI DLC using Amazon Q The demo illustrated how to apply AI-DLC in practice through a small project:\nStarting from a simple idea → converting to user story describing business requirements. AI assists in dividing work (Units of Work) and detailed planning for each module. Participants can control AI through questions, checkboxes, and logical conditions, helping AI understand the scope of work. AI continues to generate code, write tests, create project structure, and automatically deploy trials. The demo clearly showed how AI and humans collaborate harmoniously: AI handles repetitive tasks, humans guide and make strategic decisions. Introduction to Kiro Kiro\u0026rsquo;s Philosophy\nThe next part of the workshop introduced Kiro, an intelligent development environment designed around the \u0026ldquo;AI-native development\u0026rdquo; philosophy – where AI is a core component, not just a support tool.\nKiro\u0026rsquo;s philosophy focuses on three main elements:\nDeep integration with development processes – AI not only assists in writing code but also participates in planning, managing context, and analyzing change impacts. Comprehensive project context understanding – Kiro maintains continuous state awareness of system structure, allowing AI to interact with the entire project rather than individual files. Intelligent control \u0026amp; collaboration – Developers can guide AI through contextual commands, ensuring each change has a clear purpose and is consistent with the system. Project Structure in Kiro\nUnlike traditional text editors like VSCode or JetBrains, Kiro is not just a code writing environment — it\u0026rsquo;s an AI workspace with structural awareness.\nProject structure in Kiro includes:\nContext Layer – Stores context, domain models, and relationships between modules. Task Layer – Manages Units of Work tracked and gradually completed by AI. AI Agent Layer – Each task (code, test, refactor, deploy) has a dedicated agent, creating a multi-agent – collaborative – parallel development model. Human-in-the-Loop Control – Developers can intervene at every step: confirm, modify, or reject AI outputs. This makes Kiro not just a code generation tool but a collaborative development ecosystem between humans and AI.\nDemo 2: Kiro – Applying AI-DLC In the demonstration, the speaker illustrated how Kiro operates AI-DLC seamlessly:\nUser inputs a basic business requirement, e.g., \u0026ldquo;build an event management system\u0026rdquo;. Kiro automatically analyzes intent, creates domain models, and breaks down into user stories. AI in Kiro generates corresponding modules, components, and test cases. Developers can interact through checkbox-based task control to confirm each part of the work. Finally, Kiro deploys the complete system with IaC and automated testing. The demo showed that AI-DLC is not just theory, but can be implemented practically within the Kiro environment — where AI, humans, and development processes merge into a unified system.\nEvent Experience Participating in the \u0026ldquo;AI DLC x Kiro: Reinventing Developer Experience with AI\u0026rdquo; workshop was an extremely valuable experience, helping me better understand how AI is deeply integrated into software development environments and how Kiro\u0026rsquo;s design philosophy brings a new approach to developers.\nLearning from Expert Speakers Speakers shared about AI DLC – a platform supporting AI-based software development, automating many SDLC processes. Additionally, the introduction to Kiro Editor provided deep insights into building a text editor in an AI-native direction rather than just \u0026ldquo;adding AI plugins\u0026rdquo; to old environments. I was particularly impressed with Kiro\u0026rsquo;s philosophy: minimalist, high-performance, focused on user experience and module-based scalability. Practical Technical Experience In learning:\nApply AI-DLC structure for personal projects Practice \u0026ldquo;Context Awareness\u0026rdquo; principle with AI assistants Build habit of writing clear requirements as user stories For future career:\nUnderstand modular, extensible, maintainable system design like Kiro Master Amazon Q and other AI tools effectively Recognize importance of providing quality context for AI Mindset shift:\nApproach problems with \u0026ldquo;AI-augmented\u0026rdquo; thinking Consider building custom tools with deep AI integration Always ask: \u0026ldquo;How can AI assist better at this step?\u0026rdquo; Applying Modern Tools Experiencing AI DLC on Kiro helped me better understand the capability of automating development processes, especially in steps like code generation, documentation, and debugging. I recognized the potential of building personal learning and working tools with intelligent suggestions, helping shorten development time and improve product quality. Kiro\u0026rsquo;s modular design concepts also suggested directions for designing flexible, scalable, and maintainable systems. Networking and Exchange The workshop created opportunities for me to connect with developers, AI researchers, and product designers, thereby learning more about the AI-augmented development trend. Through discussions, I learned much about how AI can play the role of creative collaborator, helping developers focus more on logic and system thinking rather than repetitive operations. Lessons Learned Participating in the \u0026ldquo;AI DLC x Kiro\u0026rdquo; workshop was a turning point in how I perceive AI\u0026rsquo;s role in software development.\nThe most important thing I learned wasn\u0026rsquo;t specific tools, but the necessary mindset shift:\nAI is not a tool for faster coding AI is a partner for better thinking and system design Structured processes (like AI-DLC) are more important than raw AI power The workshop also showed me the future of development tools - where AI-first architecture like Kiro will become standard, and developers need to prepare for this paradigm shift.\nInsights from AWS Solution Architects and hands-on experience with Kiro equipped me with a solid foundation to apply AI in my learning journey and future career in software engineering.\nSome Photos from the Event Group photo check-in after the event\nThis is the group check-in moment after the workshop ended. This event provided many valuable insights about how AI is reshaping development workflow.\nProfessional event space\nThe workshop was professionally organized with complete demo stations and networking opportunities. This was one of the important events that helped me deeply understand AI-driven development.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand AWS VPC: Grasp the basic concepts of VPC (Virtual Private Cloud) as an isolated logical network environment, including key components like Subnets (Public and Private), Route Tables, and ENI. Traffic Control and Security: Learn to configure security layers (Security Groups and NACLs) and control network traffic flow to/from the Internet (Internet Gateway and NAT Gateway). Complex Network Connectivity: Differentiate and know how to use methods for connecting VPCs (VPC Peering) and the central connection model (Transit Gateway). Build a Hybrid Cloud Environment: Learn about solutions for connecting on-premises networks with AWS, including VPN (Site-to-Site) and private connections (AWS Direct Connect). Application Load Balancing: Understand the function of Elastic Load Balancing (ELB) and differentiate between various load balancer types (ALB, NLB, CLB, GLB) to ensure high availability and scalability for applications. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS Virtual Private Cloud (VPC)\n+ What is VPC?\n+ How does the VPC structure work?\n- Learn about VPC-Subnets and Subnet architecture?\n- Learn about VPC-Route Table?\n- Learn about VPC-ENI and VPC-ENI architecture?\n- Learn about VPC-Endpoint and VPC-Endpoint architecture?\n- Learn about VPC-Internet Gateway and VPC-Internet Gateway architecture?\n- Learn about VPC-NAT Gateway and VPC-NAT Gateway architecture?\n- Learn about VPC-Security Group and VPC-Security Group architecture?\n- Learn about VPC-NACL and VPC-NACL architecture?\n- Learn about VPC-Flow Logs 15/09/2025 15/09/2025 Module 02 3 - Learn about networking services on AWS?\n- Learn about VPC Peering and its architecture?\n- Learn about Transit Gateway and its architecture?\n- Understand the concepts of VPN \u0026amp; Direct Connect services?\n- What is Site-to-Site VPN? How to set it up?\n- Learn about Client-to-Site VPN?\n- What is AWS Direct Connect? 16/09/2025 16/09/2025 Module 02 4 - Learn about the concepts and overview of Elastic Load Balancing? And the current types of ELB?\n- Learn about ELB - Application Load Balancer and its architecture?\n- Learn about ELB - Network Load Balancer and understand the concept?\n- Learn about ELB - Classic Load Balancer and understand the concept?\n- Learn about ELB - Gateway Load Balancer and its architecture? 17/09/2025 17/09/2025 Module 02 5 - Lab 03 - VPC Initialization\n1. Configure VPC Firewall\n2. Practice Creating a VPC\n3. Configure Site to Site VPN\n- Lab 58 - System Manage - Session Manage\n1. Create Connection to EC2 Server\n2. Manage Session Logs 3. Use Port Forwarding\n- Lab 19 - Set Up VPC Peering\n1. Update Network ACL\n2. Create Peering Connection 3. Configure Route Tables\n4. Enable Cross-Peer DNS\u0026quot; 18/09/2025 18/09/2025 Module 02 6 - Lab 20 - Transit Gateway Setup\n1. Infrastructure Setup\n2. Create Transit Gateway -\u0026gt; Connect Multiple VPCs Together\n3. Transit Gateway Attachments\n4. Create Route Table for TGW\n5. Add Gateway to Route Tables \u0026amp; Check Results\n- Lab 10 - Hybrid DNS\n1. Hybrid DNS Setup\n2. Create Outbound Endpoint\n3. Create Route 53 Resolver Rule\n4. Create Inbound Endpoint.\n- Additional research on AWS Advanced Networking - Specialty Study Guid 19/09/2025 19/09/2025 Module 02 Research Link Week 2 Achievements: Explain what VPC is, its role in AWS, and its core components (Subnet, Route Table, ENI). Clearly differentiate between a Public Subnet (with an Internet Gateway) and a Private Subnet (using a NAT Gateway for Internet access). Compare and contrast the two main firewall mechanisms: Security Group (stateful, applies to ENI) and NACL (stateless, applies to Subnet). Present how to privately connect from a VPC to AWS services (like S3) without going over the Internet using a VPC Endpoint. Evaluate the pros and cons of two VPC connection solutions: VPC Peering (1:1 connection, no transitive support) and Transit Gateway (hub-and-spoke model, simplifies management). Describe methods for establishing a hybrid cloud connection, including Site-to-Site VPN (over the Internet) and AWS Direct Connect (private physical connection). Classify and select the appropriate Elastic Load Balancer type for specific scenarios: Application Load Balancer (ALB): For HTTP/HTTPS traffic (Layer 7), supports path-based routing. Network Load Balancer (NLB): For TCP/TLS traffic (Layer 4), requires ultra-high performance and static IP. Gateway Load Balancer (GLB): Used for integrating virtual appliances. Identify the necessary labs to reinforce knowledge of VPC, Peering, Transit Gateway, and related services. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.3-knowledge-base/5.3.2-sync-data/","title":"Check Vector Store and Data Synchronization","tags":[],"description":"","content":"Target Before the AI can answer, data must be ingested into the vector storage (Vector Store). We will perform a \u0026ldquo;Before and After\u0026rdquo; check to clearly see how Bedrock automatically encodes and stores data into OpenSearch.\nImplementation Steps Step 1: Check Vector Store (Empty State)\nWe will directly access Amazon OpenSearch Serverless to confirm that no data exists yet.\nIn the AWS Console search bar, type Amazon OpenSearch Service and select Amazon OpenSearch Service. In the left menu, under Serverless, select Collections. Click on the Collection name newly created by Bedrock (usually named like bedrock-knowledge-data...). On the Collection details page, click the Open Dashboard button (located at the top right of the screen).\nNote: If asked to log in, use your current AWS credentials. In the OpenSearch Dashboard interface: Click the Menu (3 horizontal lines) icon in the top left corner. Select Dev Tools (usually located at the bottom of the menu list). In the Console pane (on the left), enter the following command to check data: GET _search { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} } } Click the Play (Run) button (small triangle next to the command line).\nResult: Observe the right pane, hits -\u0026gt; total -\u0026gt; value is 0.\nStep 2: Sync Data\nNow we will trigger Bedrock to read files from S3 and load them into OpenSearch.\nReturn to the Amazon Bedrock tab on the browser. Select Knowledge bases in the left menu and click on the KB name you just created. Scroll down to the Data source section, check the box (tick) next to the data source name (s3-datasource). Click the Sync button (Orange). Wait: This process will take 5 - 10 minutes depending on the sample document size. Wait until the Sync status column changes from Syncing to Available. Step 3: Re-check Vector Store (Populated)\nAfter Bedrock reports Sync completion, we return to the repository to verify the data has been successfully ingested.\nSwitch to the OpenSearch Dashboard tab (still open from Step 1). In Dev Tools, click the Play (Run) button again with the old command: GET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } Result: The hits -\u0026gt; total -\u0026gt; value section will be greater than 0 (e.g., 10, 20\u0026hellip; depending on the number of text chunks). You will see details of the vectors (number arrays) and text content stored in the _source field. Congratulations! You have completed building the \u0026ldquo;brain\u0026rdquo; for the AI. The data has been encoded and sits safely in the Vector Database, ready for retrieval.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.2-prerequiste/5.2.2-prepare-data/","title":"Prepare source data","tags":[],"description":"","content":"Overview Initialize an object storage (S3 Bucket) to hold original documents (PDF, Word, Text). This acts as the \u0026ldquo;Source of Truth\u0026rdquo; that the Knowledge Base will access to read, analyze, and synchronize knowledge for the AI. You can store knowledge related to your field to use in creating your own personal assistant or Chatbot.\nData Preparation We will create an S3 Bucket to store original documents, acting as the knowledge source for the Chatbot.\nStep 1. Create S3 Bucket\nAccess the S3 service from the search bar. AWS Region: Select United States (N. Virginia us-east-1). Click Create bucket. Configure Bucket information: Bucket Type: Click General purpose Bucket name: Enter rag-workshop-demo Object Ownership: Keep default ACLs disabled. Block Public Access settings: Keep default (Selected Block all public access). Scroll to the bottom of the page, Click Create bucket. Check that the S3 Bucket has been created successfully. Step 2. Upload sample documents\nThis is a sample document, relevant to overview of AWS cloud computing knowledge. You can use it to run demos or upload your data. PDF format file\nIn the Buckets list, Click on the bucket name you just created. Click Upload. In the Upload interface:\nClick Add files. Select the sample document file attached above or a file from your computer (PDF or Word file with a lot of text is recommended). Scroll to the bottom of the page, Click Upload.\nWhen you see the green \u0026ldquo;Upload succeeded\u0026rdquo; notification, Click Close. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"1. Objectives Before building the application, we need to establish a solid foundation. Similar to preparing ingredients before cooking, this section ensures your AWS account is ready with the necessary permissions and data.\nIn this section, we will complete 3 key initialization objectives:\nSelect Region: Set up the working environment in the United States N. Virginia (us-east-1) region to optimize connection speed and ensure service availability. Enable Model (Model Access): Check and ensure the account has permission to invoke the Anthropic Claude 3 model – the main linguistic \u0026ldquo;brain\u0026rdquo; of the system. Prepare Data (Data Setup): Initialize storage (S3 Bucket) and upload source documents to serve the knowledge ingestion process later. 2. Key Components In this preparation section, we will interact with the following components:\nAWS Management Console (Region Selector): The general management interface to switch the working Region to United States N. Virginia. Amazon Bedrock (Model Access \u0026amp; Playground): The place to manage access to Foundation Models and the chat tool to quickly test AI response capabilities. Amazon S3 (Simple Storage Service): Object storage service where we will create a Bucket to hold original document files (PDF, Word, Text). 3. Implementation Steps Check Model Access Prepare Source Data "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Personal Finance Management App (Vicobi) You can read the full proposal here: Vicobi Proposal 1. Executive Summary The Vicobi (Personal Finance Management App) project aims to provide an intelligent, modern, and highly automated personal financial management platform. Vicobi simplifies financial management through 4 main pillars:\nSmart Recording: Voice input and invoice scanning, eliminating manual data entry barriers. Goal-based Budgeting: Automated creation and flexible management of money jars. Analysis \u0026amp; Control: Provides visual reports and intelligent alert systems. AI Financial Assistant (Chatbot): Integrates AI Chatbot acting as an advisor, supporting inquiries and enhancing financial knowledge. From a technical perspective, Vicobi is built on a Microservices architecture using .NET Aspire and FastAPI, deployed on AWS Cloud, ensuring flexibility and data security. The development process follows the Agile/Scrum model (2 weeks/sprint during the main development phase), with the goal of completing MVP within 2 months of execution.\n2. Problem Statement Current Problem In today\u0026rsquo;s dynamic market, users face difficulties in controlling finances due to \u0026ldquo;behavioral inertia\u0026rdquo; — reluctance to manually record each transaction. Existing applications (like Money Lover, Misa Money Keeper) still rely heavily on manual input, causing \u0026ldquo;input fatigue\u0026rdquo; and high abandonment rates.\nSolution Vicobi solves the problem through high automation of the data entry process using AWS Cloud and Microservices:\nCore Technology: Integrates AI for Vietnamese voice processing (Voice-to-Text) and detailed invoice recognition (OCR). Optimized Architecture: Uses AWS ECS Fargate running Multi-container Task model (combining .NET Backend and AI Service) to reduce infrastructure costs while ensuring seamless communication. Modern Frontend: Uses Next.js hosted on Amazon S3 and distributed globally via Amazon CloudFront. Benefits and Return on Investment (ROI) The solution provides clear competitive advantages:\nUser Value: Reduces over 70% manual operations. Voice recognition accuracy reaches 90% and invoice extraction reaches 80%. Economic Efficiency: Maximizes AWS Free Tier usage (S3, CloudFront, Cognito). Lean operating budget around ~$60/month for infrastructure and ~$15/month for AI compute. ROI: Expected to achieve ROI within 6–12 months thanks to time savings and increased efficiency. Scalability: Microservices architecture ready for Mobile App integration or Open Banking. 3. Solution Architecture The system is designed with a distributed Microservices model, using API Gateway as the single entry point.\nTech Stack Details: Component Technology Details Frontend Next.js 16 App Router, TypeScript, Tailwind CSS, Zustand, React Query. Backend Core .NET Aspire Orchestrates Microservices (User, Wallet, Transaction, Report, Notification). AI Service FastAPI (Python) Handles Voice (PhoWhisper), OCR (Bedrock), Chatbot (RAG). Database Polyglot PostgreSQL, MongoDB, Elasticsearch, Qdrant (Vector DB). Messaging RabbitMQ Asynchronous communication between services. AWS Workflow: Access: Users access via Route 53, protected by AWS WAF and accelerated by CloudFront. Authentication: Amazon Cognito manages identity and issues JWT Tokens. API Processing: Requests go through API Gateway, connecting securely via AWS PrivateLink to Application Load Balancer (ALB). Compute: ALB distributes load to containers in ECS Fargate (located in Private Subnet). DevOps: CI/CD process fully automated by GitLab, builds images pushed to Amazon ECR and updates tasks on ECS. 4. Technical Deployment Implementation Phases The project spans 4 months (including internship):\nMonth 0 (Pre-internship): Ideation and overall planning. Month 1 (Foundation): Learn AWS, upgrade .NET/Next.js/AI skills. Set up VPC, IAM. Month 2 (Design): Design High-level \u0026amp; Detailed architecture on AWS. Month 3-4 (Realization): Coding, Integration Testing, Deploy to AWS Production, set up Monitoring. After Month 5: Research and develop Mobile App. Detailed Technical Requirements: Frontend: Deploy Next.js 16 on S3 + CloudFront. Use Origin Access Control (OAC) to secure bucket. Backend: Use .NET Aspire to manage Cloud-native configuration. Database-per-service: PostgreSQL \u0026amp; MongoDB. Elasticsearch for complex transaction search. Background Jobs: Use Hangfire. AI Service Pipelines: Voice: Preprocessing with Pydub, PhoWhisper-small Model (VinAI) for Vietnamese. OCR: Amazon Bedrock (Claude 3.5 Sonnet Multimodal) to accurately extract invoice information. Chatbot (RAG): Knowledge Base stored in Qdrant, generates responses via Amazon Bedrock (Claude 3.5 Sonnet). Security: Data encryption in transit (HTTPS/TLS 1.2+) and at rest (AES-256). Secrets management not deeply integrated (currently at MVP level), will upgrade to AWS Secrets Manager in the future. 5. Timeline \u0026amp; Milestones (Sprints) The main execution phase is divided into 4 Sprints:\nSprint 1: Core Foundation Authentication (Cognito), Wallet Management, Spending Jars. Sprint 2: Core Features Transactions (CRUD), AI Voice Processing. Sprint 3: Analytics Reports/Charts, Notification System (SES), Message Broker. Sprint 4: Stabilization Integration Testing, UI Refinement, Deploy to AWS ECS \u0026amp; CloudFront. Testing \u0026amp; Go-live: Domain Configuration, SSL, Monitoring Dashboard, UAT and project defense. 6. Budget Estimation Based on detailed cost estimates for the MVP phase.\nYou can review the detailed cost estimation by downloading the following files: 📊 CSV Pricing File 💾 JSON Pricing File\nAWS Service Component / Usage Cost (USD/month) Elastic Load Balancing Application Load Balancer $18.98 Amazon ECS Fargate (vCPU \u0026amp; Memory) $17.30 Amazon VPC VPC Endpoints \u0026amp; NAT $10.49 AWS WAF Web ACL \u0026amp; Requests $7.20 Amazon API Gateway API Calls \u0026amp; Data Transfer $2.50 Amazon CloudFront Data Transfer Out $2.00 Amazon ECR Storage $1.00 Amazon Route 53 Hosted Zones $0.54 Amazon S3 Standard Storage $0.34 TOTAL AWS COST ~$60.35 Other Costs:\nCategory Details Cost (USD/month) AI Compute / Tooling Gemini API, Amazon Bedrock ~$15.00 PROJECT TOTAL ~$75.35 / month (Based on On-Demand pricing in Singapore region - ap-southeast-1)\n7. Risk Assessment Main Risks: User information leakage (Impact: High), AWS Region connection loss (Impact: High), AI misrecognition (Impact: Medium). Mitigation Strategies: Security: AES-256 encryption, HTTPS, IAM Least Privilege, AWS WAF. High Availability: Multi-AZ deployment for ECS and ALB. AI: Continuously improve model with real data. Resilience: Use internal RabbitMQ for asynchronous processing and retry. Disaster Recovery Plan: Use IaC (Infrastructure as Code) for rapid infrastructure restoration. 8. Expected Results \u0026amp; Team Expected Results of the Project Automated financial data entry: The application helps users avoid manual entry, just take a photo of the invoice or record a voice for the system to automatically classify spending. Intuitive financial management: Users can view spending charts, monthly reports, and receive savings suggestions based on consumer behavior. Minimal user experience: Friendly web interface, modern design, optimized for mobile devices and suitable for people new to financial management. Stable, scalable system: Microservices architecture makes it easy to add new features such as spending reminders, AI predictive analysis, or expand to a mobile app. Improving development team skills: Project members have practical access to DevOps processes, CI/CD implementation, and cloud-based application optimization. Project Limitations Vietnamese AI model is still limited: The ability to recognize regional voices or handwritten invoices has not yet achieved high accuracy. No separate mobile application: The MVP version only supports the web platform, there is no native mobile app. Implementation Team: Name Role Email Le Vu Phuong Hoa Backend Developer (Leader) hoalvpse181951@fpt.edu.vn Nguyen Van Anh Duy AI Developer (Member) duynvase181823@fpt.edu.vn Uong Tuan Vu Frontend Developer (Member) vuutse180241@fpt.edu.vn Tran Nguyen Bao Minh AI Developer (Member) baominhbrthcs@gmail.com Mentor Support:\nNguyen Gia Hung - Head of Solution Architects Van Hoang Kha - Cloud Security Engineer "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.1-workshop-overview/5.1.2-services/","title":"Services","tags":[],"description":"","content":"The solution architecture is built upon the coordination of the following 4 key service components:\nKnowledge Bases for Amazon Bedrock This is a fully managed capability that helps connect Foundation Models to the enterprise\u0026rsquo;s internal data sources.\nRAG workflow automation: Manages the entire end-to-end workflow, including ingestion, chunking, embedding, and retrieval. Contextual connection: Enables AI applications to answer questions based on private data rather than relying solely on generic training data. No infrastructure management: Eliminates the need to build and maintain complex data pipelines. Amazon Simple Storage Service (Amazon S3) An object storage service with scalability, 99.999999999% (11 nines) data durability, and top-tier security.\nData Source Role: Acts as the \u0026ldquo;source of truth\u0026rdquo;. Document storage: Contains unstructured files such as PDF, Word, or Text that the business wants the AI to learn. Synchronization: The Knowledge Base will periodically scan this S3 bucket to synchronize and update the latest knowledge. Amazon OpenSearch Serverless A serverless deployment option for Amazon OpenSearch Service that helps run search and analytics workloads without managing clusters.\nVector Store Role: Stores vector embeddings generated from original documents. Semantic Search: Performs similarity search algorithms (k-NN) to identify text segments with meanings closest to the user\u0026rsquo;s question. Auto-scaling: Automatically adjusts compute and storage resources based on actual demand. Amazon Bedrock Foundation Models (FMs) Provides access to leading AI models via a unified API. In this architecture, we use two types of models with distinct roles:\nEmbedding Model (Amazon Titan Embeddings v2): Converts text (documents from S3 and user questions) into numerical vectors. Enables computers to compare semantic similarity between text segments. Text Generation Model (Anthropic Claude 3): Acts as the reasoning \u0026ldquo;brain\u0026rdquo;. Receives the question along with contextual information retrieved from the Vector Store. Synthesizes information and generates natural, accurate answers with source citations. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: WORKSHOP \u0026ldquo;DATA SCIENCE ON AWS PLATFORM\u0026rdquo; Event Objectives Introduce an overview of the ecosystem of artificial intelligence (AI) services available on AWS. Guide practical processes for building and training AI models using Amazon SageMaker. Demonstrate how to deploy AI models from laboratory to production and integrate them into applications through APIs. Speaker List Mr. Van Hoang Kha - Cloud Solutions Architect, Leader of AWS User Group (Sharing solution architecture perspective). Mr. Bach Doan Vuong - Cloud DevOps Engineer, AWS Community Builder (Sharing operations and deployment perspective). In-Depth Content The Importance of Cloud Computing in Data Science Analysis of the core role of Cloud Computing: Not just storage, Cloud provides unlimited computing power to process Big Data and train complex AI models that personal computers struggle to handle.\nEfficiency comparison between Cloud and On-premise (Physical servers):\nCloud: Strengths lie in Elasticity — can scale resources up/down instantly, extremely fast deployment, convert CAPEX (Capital Expenditure) to OPEX (Operating Expenditure), and easily integrate new technologies. On-premise: Faces major barriers in initial hardware costs, difficulty expanding infrastructure when data surges, and expensive maintenance personnel. AWS plays the role of backbone for the entire Data Science pipeline: Provides a closed-loop process from raw data collection, cleaning, training to when the model is put into practical use.\nAI Architecture Layers on AWS AWS categorizes AI services into 3 separate layers, designed to suit different user audiences with varying technical levels and control needs:\n1. AI Services (Fully Managed Service Layer)\n\u0026ldquo;Instant noodle\u0026rdquo; solution for Developers who want to integrate intelligence into applications without deep ML algorithm knowledge.\nThese are pre-trained models by AWS with billions of data points.\nUsers only need to send data via API and receive analysis results.\nNotable services:\nAmazon Comprehend: Understand and analyze text meaning, user sentiment (NLP). Amazon Translate: Remove language barriers with automatic translation capabilities. Amazon Textract: \u0026ldquo;Read\u0026rdquo; documents, extract text and tables from scanned files/images. Amazon Rekognition: Computer vision, recognize objects, faces in images/videos. Amazon Polly: Natural artificial voice reading (Text-to-Speech). Amazon Bedrock: Gateway to today\u0026rsquo;s leading Large Language Models (LLMs). Benefits: Extremely fast time-to-market, no effort building models from scratch.\n2. ML Services (Semi-Managed Layer)\nPowerful tool for Data Scientists \u0026amp; ML Engineers needing professional environments to build their own models.\nAmazon SageMaker is the heart of this layer, providing a unified platform to manage ML model lifecycle (ML Ops).\nKey modules:\nData Wrangler: Minimize time for data preparation and cleaning (typically 80% of project time). Feature Store: Repository for storing data features for reuse, avoiding waste of computing resources. AutoML (SageMaker Autopilot): Automatically test multiple algorithms to find the best model without manual intervention. Model Registry \u0026amp; Monitoring: Manage model versions and monitor model accuracy in real-time (avoid model drift phenomenon). Benefits: Perfect balance between convenience and deep customization capability in training processes.\n3. AI Infrastructure (Self-Managed Infrastructure Layer)\nFor researchers needing deep hardware intervention and lowest-level optimization.\nProvides \u0026ldquo;building blocks\u0026rdquo; to build custom AI systems:\nAmazon EC2 P5/G6/Inferentia: Virtual machines with dedicated chips (GPU/ASIC) for extremely high computing performance. Amazon EKS / ECS: Container management for large-scale ML applications. AWS Lambda: Run inference code serverless, cost-effective for small tasks. Amazon S3 / EFS: Massive \u0026ldquo;Data Lake\u0026rdquo; storage system. Benefits: Not limited by templates, thoroughly optimize costs for super-large systems, but requires high DevOps skills.\nPopular AI Services Supporting Students \u0026amp; Researchers 1. Amazon SageMaker\nAn IDE (Integrated Development Environment) dedicated to ML on cloud:\nIntegrates everything from raw data processing to hyperparameter tuning. Provides CI/CD capability for Machine Learning, automating train and deploy processes. Supports Notebooks (Jupyter) familiar to students. 2. Amazon Comprehend\nBrings \u0026ldquo;reading comprehension\u0026rdquo; capability to applications:\nSentiment analysis: Know if customers are happy, angry, or satisfied through emails/comments. Entity recognition: Automatically detect names of people, places, organizations in text. Security: Automatically find and mask personal information (PII) in data. 3. Amazon Translate\nHigh-quality translation based on Deep Learning (Neural Network). Ability to customize specialized vocabulary (e.g., correctly translate medical or technical terms). Helps expand application user base globally. 4. Amazon Textract\nFar surpasses traditional OCR technology thanks to document structure understanding. Preserves table and form formatting when extracting, helps digitize administrative documents quickly and accurately. Standard Data Science Process on AWS Ingest \u0026amp; Store: Collect data from multiple sources into Amazon S3 \u0026ldquo;warehouse\u0026rdquo;. Prepare: Clean and standardize data using AWS Glue or Lambda. Train \u0026amp; Tune: Use SageMaker to train and optimize algorithms. Deploy: Package model into API Endpoint or integrate into application. Monitor: Use CloudWatch to monitor system health and model prediction quality. Demo 1: Optimizing Workflow with Low-Code/No-Code Objective: Prove that technical barriers in AI are gradually being removed thanks to visual tools.\nTool: Amazon SageMaker Canvas (Drag-and-drop interface).\nImplementation process:\nUpload raw dataset to S3. Use graphical interface to define processing flow: Input -\u0026gt; Handle missing data -\u0026gt; Select target column -\u0026gt; Train. System automatically runs experiments and returns evaluation metrics (Accuracy, F1-score\u0026hellip;) in easy-to-understand charts. Significance: Helps students and Business Analysts create value from data immediately without writing thousands of lines of Python code.\nDemo 2: From Model to Real Application (Deployment) Objective: Illustrate the \u0026ldquo;bridge\u0026rdquo; connecting mathematical models and end users.\nTools: SageMaker Endpoint combined with API Gateway and Lambda.\nImplementation process:\nConvert trained model into an HTTP Endpoint (network address). Set up API Gateway to receive external requests (e.g., from mobile app). Process intermediate logic using AWS Lambda (serverless) to call model and return results to users. Significance: Shows the full picture of making AI products: Model is just one part, integration and serving the model creates complete products.\nComparison Table: Cloud vs. On-premise (Performance \u0026amp; Economics Perspective) Criteria Cloud (AWS) On-premise (Private servers) Scalability Unlimited \u0026amp; Instant: Auto-scale according to actual traffic. Rigid: Limited by current hardware quantity. Cost Model OPEX (Operating Expenditure): Pay as you go, no waste. CAPEX (Capital Expenditure): Must invest large sum buying equipment, depreciation risk. Deployment Speed Minutes: Just a few clicks to have servers. Weeks/Months: Must order, install, setup OS. System Maintenance AWS handles: Focus entirely on application development. DIY: Spend IT personnel on electricity, cooling, hardware operations. Student-friendly High: Utilize Free Tier for free learning. Low: Requires expensive high-spec machines. General Conclusion AWS provides a comprehensive and seamless ecosystem, blurring the gap between academic learning and practical applications. Whether beginners or large enterprises, AWS has appropriate tool sets (Layers) to solve data problems. Personal Reflections After the Event The \u0026ldquo;AI Services on AWS for Data Science\u0026rdquo; workshop not only provided theoretical knowledge but also opened up new thinking about technology approaches, helping me better define the path from data research to product development.\nExpanded thinking thanks to experts Clearly understand why the world is shifting to Cloud: It\u0026rsquo;s liberation from infrastructure burden to focus on core value which is data. Grasped the overall picture of 3 AI layers, thereby knowing how to choose appropriate services for each stage of personal projects. Visualizing knowledge through Demos Demo 1 (Canvas): Impressed with AI \u0026ldquo;democratization\u0026rdquo; capability. Training models now is as visual as assembling lego, helping validate ideas extremely fast. Demo 2 (Deployment): This is the piece I often miss. Understanding how to create an API for others to use my model is a turning point from \u0026ldquo;doing homework\u0026rdquo; to \u0026ldquo;making products\u0026rdquo;. Accessing advanced technology Services like Comprehend or Textract show the power of Pre-trained models. They help solve difficult problems (like reading invoices, sentiment analysis) in an instant without building complex models yourself. Connection value Opportunity to discuss Cost optimization issues is very practical, helping students like me know how to use Cloud without \u0026ldquo;burning money\u0026rdquo;. Networking with industry professionals gives me more motivation and clearer career direction. Core lessons Cloud First: Thinking of putting everything on cloud is an inevitable trend in modern Data Science. Practicality: An AI model only has value when deployed and solving problems for end users. AWS Ecosystem: A massive tool repository that I need to continue exploring to enhance my capabilities. Some images captured at the event Image of knowledge transfer from 2 AWS representatives\nImage of FPTers team check-in after the event\nThis is the check-in moment of all interns at AWS after the workshop ended.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand comprehensive knowledge of virtual server (Compute VM) services on AWS. Focus on the core service Amazon EC2, including configuration selection (Instance Types), storage types (EBS, Instance Store), and automation (User data, Auto Scaling). Learn about related services such as Amazon Lightsail (low-cost service), shared file storage solutions (EFS for Linux and FSx for Windows/Linux), and the AWS MGN application migration service to migrate servers to AWS or build Disaster Recovery. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Amazon Elastic Compute Cloud (EC2)\n- Learn about the architecture of Amazon EC2, a flexible virtual server service with fast boot capabilities and powerful resource scaling.\n- Understand the technique of selecting server configurations through EC2 Instance Types, which determine factors like CPU, Memory, Network, and Storage.\n- Learn how to use AMI (Amazon Machine Image) to provision one or more EC2 Instances and use Key Pairs (public and private key) to encrypt login information.\n- Understand the EBS (Elastic Block Store) block storage technique, which operates independently, replicates data (3x) to ensure high availability, and connects to EC2 over the network.\n- Differentiate EBS from Instance Store, which is an extremely high-speed NVME disk area but whose data is completely erased when the EC2 instance is stopped.\n- Learn about the User Data automation technique, a script (bash shell or powershell) that runs once when an EC2 instance is launched.\n- Understand Meta Data, information related to the EC2 instance (like IP, Hostname) that can be accessed from the server itself, often used for automation.\n- Understand the EC2 Auto Scaling technique to automatically increase (scale-out) or decrease (scale-in) the number of EC2 Instances based on conditions (scaling policy).\n- Learn about EC2 Pricing Options (On-demand, Reserved Instance, Saving Plans, Spot Instance) to optimize costs. 22/09/2025 22/09/2025 Module 03 3 Amazon Lightsail\n- Learn about the Amazon Lightsail service, a low-cost virtual server solution (starting at $3.5/month) suitable for light workloads, test/dev environments.\n- Understand the technique for connecting Lightsail (located in a special VPC) with a regular VPC via VPC Peering (with just 1 click).\nAmazon EFS/FSx\n- Learn about EFS (Elastic File System), a network file storage service (NFSv4) that allows multiple EC2 Instances (Linux only) to mount simultaneously, charging based on storage used.\n- Learn about Amazon FSx, a service that allows creating NTFS volumes (SMB protocol) to attach to multiple EC2 Instances (supports Windows and Linux).\n- Understand the deduplication technique of FSx, which helps reduce data duplication and lower storage costs.\nAWS Application Migration Service (MGN)\n- Learn about the AWS MGN service used to migrate and replicate servers (physical or virtual) from on-premise to the AWS environment.\n- Understand the replication technique of MGN used for building a Disaster Recovery Site at a low cost (using low-configuration staging machines). 23/09/2025 23/09/2025 Module 03 4 Lab: 000004 - Basic EC2 Operations.\n- Create an EC2 Instance\n- Take an EC2 Instance snapshot\n- Install an application on EC2\nLab: 000027 - Resource Management with Tags and Resource Groups\n- Use Tags\n- Use Resource Groups 24/09/2025 24/09/2025 Module 03 5 Lab: 000008 - Resource Management with Amazon CloudWatch\n- CloudWatch Agent\n- Create CloudWatch Dashboard\nLab: 000006 - Deploying an Autoscaling Group\n- Create Launch Template\n- Create Target Group\n- Create Load Balancer\n- Create Auto Scaling Group\n- Verify the results. 25/09/2025 25/09/2025 Module 03 6 Lab: 000045 - Getting Started with Amazon Lightsail\n- Preparation\n- Test the application on Lightsail\n- Use Lightsail Load Balancer\n- Use RDS\n- Migrate to EC2.\n[Supplemental Research] - Microsoft Workloads on AWS\n- A series of supplemental labs for running Microsoft servers and applications on AWS\n- Supplement knowledge about operating systems\n- Supplement knowledge about the Linux operating system such as LBI1, LBI2\n- Supplement knowledge about the Windows operating system, learn more about the Bundo management system. Refer to the lab series 26/09/2025 26/09/2025 Module 03 Research Link Week 3 Achievements: EC2 Service: Clearly understand that EC2 is the core virtual server service of AWS. EC2 Configuration Techniques: Know how to select Instance Type (CPU, RAM, Network configuration) and use AMI to provision the operating system for the server. EC2 Security Techniques: Understand how to use Key Pair (public/private key) to encrypt login information, instead of using passwords. Storage Service (Storage): Clearly differentiate the 2 main disk storage types for EC2: EBS (Elastic Block Store): Is a network drive, operates independently, data is replicated 3x within 1 AZ (99.999% availability), can be backed up using snapshots. Instance Store: Is a physical drive (NVME) with extremely high speed, but the data is temporary (will be erased when EC2 is stopped), often used for cache/buffer or swap. Automation Techniques (Automation): Know how to use User Data to run a script once when the server boots up (e.g., install a web server). Understand what Meta Data is and how to use it to retrieve information (IP, hostname) about the server from within itself, used for automation scripts. Scaling Techniques (Scaling): Master the concept of EC2 Auto Scaling to automatically increase (scale-out) or decrease (scale-in) the number of servers based on load (e.g., when ActiveConnectionCount is high or low). Cost Optimization Techniques (Pricing): Recognize the 4 EC2 pricing models: On-demand (per hour/second, most expensive), Reserved Instance (1-3 year commitment), Saving Plans (1-3 year commitment, more flexible), and Spot Instance (low price, utilizes surplus resources but can be reclaimed). Lightsail Service: Understand Amazon Lightsail is a low-cost, simplified VM service, suitable for light workloads, and know how to peer it with a VPC. File Storage Service (File Storage): Differentiate between the 2 shared file storage services for multiple servers: EFS (Elastic File System): Used for Linux (NFSv4 protocol), charges based on storage used. FSx: Used for Windows/Linux (SMB protocol), supports deduplication feature to reduce costs. Migration Service (Migration): Understand AWS MGN is a service to migrate servers from on-premise to AWS or to build a low-cost Disaster Recovery (DR) system via a staging area. Hands-on: Understand the basic hands-on steps with EC2 (create, snapshot), deploy a complete Auto Scaling Group (with Load Balancer), and get familiar with Lightsail. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.3-knowledge-base/","title":"Create and Configure Knowledge Base","tags":[],"description":"","content":"Objectives After completing the environment and data preparation, the next step is to set up the core component of the RAG architecture. In this section, we will initialize the Knowledge Base, acting as an intelligent intermediary mechanism that connects unstructured data sources with the reasoning capabilities of foundation models.\nWe will accomplish 3 key technical objectives:\nEstablish an Automated Pipeline: Configure the Knowledge Base to automate the entire RAG data processing workflow (including extraction, text chunking, and vector creation) to eliminate manual processing tasks. Initialize Vector Store: Deploy a collection on Amazon OpenSearch Serverless to store semantic vectors, serving accurate and efficient information retrieval. Data Synchronization (Data Ingestion): Perform the initial data ingestion process, converting static documents from S3 into searchable vectors within the system. Key Components During this configuration process, we will interact with and connect the following services:\nKnowledge Bases for Amazon Bedrock: A managed service acting as the orchestrator of data flow, connecting information sources, and executing semantic queries. Amazon Titan Embeddings G1 - Text v2: A specialized model for converting text data into numerical vectors (Embeddings) with high accuracy and multi-language support. Amazon OpenSearch Serverless: A fully managed vector database responsible for storage and executing similarity search algorithms (k-NN). Implementation Steps Initialize Knowledge Base Check Vector Store and Sync Data "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Redirecting Internet-bound traffic through a transparent forward proxy This blog explains how to implement a transparent forward proxy solution for centralized internet egress traffic inspection on AWS. You will learn why transparent proxies are important for organizations that need to inspect all outbound traffic without configuring explicit proxy settings on each client device, how to use Web Cache Communication Protocol (WCCP) to redirect traffic seamlessly through proxy servers, and how to integrate this solution with AWS Transit Gateway, AWS Network Firewall, and third-party virtual appliances. The article walks you through the architecture design, routing configuration, packet flow analysis, and provides real-world use cases for secure internet access, data loss prevention, and regulatory compliance requirements.\nBlog 2 - Building resilient multi-Region Serverless applications on AWS This blog explains how to build resilient multi-Region serverless applications on AWS to ensure high availability for mission-critical services. You will learn why multi-Region architectures are essential for applications that cannot tolerate regional disruptions (such as authentication systems, payment processors, and real-time gaming features), how to choose between Active-Passive and Active-Active deployment patterns based on your recovery time objectives and budget constraints, and how AWS serverless services with pay-for-value pricing make multi-Region deployments more cost-effective than traditional approaches. The article walks you through implementing a multi-Region serverless authorizer using Amazon API Gateway, AWS Lambda, Amazon Route 53, and DynamoDB Global Tables, while covering best practices for failover strategies, health monitoring, observability, and chaos engineering testing.\nBlog 3 - AWS Weekly Roundup: AWS Transform, Amazon Neptune, and more (September 8, 2025) This blog provides a weekly roundup of the latest AWS announcements and upcoming events for the week of September 8, 2025. You will learn about new AWS service launches including AWS Transform\u0026rsquo;s expanded assessment capabilities for detached storage migration, Amazon Bedrock\u0026rsquo;s global cross-region inference support for Anthropic Claude Sonnet 4, Amazon Neptune\u0026rsquo;s new public endpoints feature for simplified database access, ECS Exec\u0026rsquo;s integration into the AWS Management Console for easier container troubleshooting, and AWS User Notifications\u0026rsquo; organizational notification configurations for centralized alert management. The article also highlights upcoming AWS events including AWS Summits in various cities, AWS re:Invent 2025 in Las Vegas, and community-led AWS Community Days around the world.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #1: GENERATIVE AI, RAG \u0026amp; AWS AGENTIC AI\u0026rdquo; Event Purpose Master Prompt Engineering techniques to optimize input, helping models understand and execute user intent accurately. Leverage the power of Pretrained AI Services on AWS to quickly integrate intelligent features without building models from scratch. Deep understanding of RAG (Retrieval-Augmented Generation) architecture to solve hallucination problems and update private data for AI. Grasp the next technology wave: Agentic AI and how to use Amazon Bedrock AgentCore to transform AI Agents from proof of concept (POC) to production environment. Access Pipecat Framework to build virtual assistants with low-latency voice communication (Real-time). Speaker List Mr. Lam Tuan Kiet - Sr DevOps Engineer (FPT Software) - Expert in system operations and deployment. Mr. Danh Hoang Hieu Nghi - AI Engineer (Renova Cloud) - Expert in artificial intelligence solutions. Mr. Dinh Le Hoang Anh - Cloud Engineer Trainee (First Cloud AI Journey) - Sharing perspectives from someone starting their Cloud AI journey. Highlighted Content 1. Prompt Engineering \u0026amp; Foundation Models (Core Foundation) Before diving into complex systems, the event emphasized that \u0026ldquo;input quality determines output quality\u0026rdquo;. Effective communication with Foundation Models on Amazon Bedrock is the first step:\nZero-shot / Few-shot Prompting: Techniques to guide models by giving direct commands or providing a few sample examples (context) for AI to learn from that pattern and return desired results. Chain of Thought (CoT): An advanced technique helping AI handle complex reasoning tasks by requiring it to \u0026ldquo;explain step by step\u0026rdquo;. This reduces logical errors in answers. 2. Pretrained AI Services (AWS AI Services) This is the application layer helping developers not specialized in Machine Learning to still integrate AI into products through APIs:\nComputer Vision: Amazon Rekognition helps analyze images/videos, recognize objects and moderate content. Natural Language Processing: The trio of Amazon Translate (translation), Comprehend (sentiment/semantic analysis), and Textract (intelligent OCR - text extraction from documents). Audio Processing: Amazon Polly (convert text to natural speech) and Transcribe (convert speech to text). 3. RAG - Retrieval Augmented Generation RAG is a bridging solution helping AI \u0026ldquo;learn\u0026rdquo; enterprise-specific data without retraining (fine-tuning), solving the problem of outdated models or \u0026ldquo;hallucination\u0026rdquo;:\nEmbeddings (Vectorization): Use models like Amazon Titan Text Embeddings V2 to convert text into mathematical vectors, helping systems understand and search based on semantics (semantic search) rather than just keyword matching. Knowledge Bases for Amazon Bedrock: End-to-end managed service, automating complex steps: Document chunking -\u0026gt; Store in Vector Store -\u0026gt; Retrieve relevant data (Retrieval) -\u0026gt; Synthesize answers (Generation). 4. Evolution to Agentic AI (The Era of Task-oriented AI) GenAI is shifting from just \u0026ldquo;answering questions\u0026rdquo; to \u0026ldquo;taking action\u0026rdquo;. The event clearly categorized the development roadmap:\nGenAI Assistants: Virtual assistants performing single, repetitive tasks based on predefined rules. GenAI Agents: Goal-oriented AI with reasoning ability to choose appropriate tools to complete a sequence of work. Agentic AI Systems: Multi-agent ecosystem, highly autonomous, capable of coordinating with each other under minimal human supervision. The Challenge of \u0026ldquo;The Prototype to Production Chasm\u0026rdquo;: Why do many great Agent demos fail when deployed to production?\nPerformance \u0026amp; Scalability: Agents work slowly when processing long reasoning chains. Safety \u0026amp; Governance: Risks when Agents autonomously perform wrong actions (e.g., accidentally deleting database) or accessing sensitive data. Complexity: Difficulty maintaining context memory (Memory) across long working sessions. 5. Amazon Bedrock AgentCore: Solution for Bringing Agents to Market AgentCore was introduced as a complete infrastructure platform to solve the above problems, helping enterprises confidently deploy Agents:\nCore Components: Runtime \u0026amp; Memory: Provides stable execution environment and long-term \u0026ldquo;memory\u0026rdquo; capability of past interactions. Identity \u0026amp; Gateway: Strict identity management, ensuring Agents only execute within granted permissions. Code Interpreter: A safe \u0026ldquo;sandbox\u0026rdquo; allowing Agents to autonomously write and run Python code to process numerical calculations or draw accurate charts (instead of guessing numbers). Observability: Monitoring tools helping humans track each reasoning step (Trace) of Agents for debugging and optimization. Benefits: Helps Developers focus on business logic, reducing the burden of building supporting infrastructure. 6. Pipecat: Framework for Real-time AI Voice Introduction to an open-source Framework helping build natural human-machine communication applications (Multimodal):\nFeatures: Optimizes extremely low latency, a vital factor in voice communication. Pipeline Mechanism: WebRTC Input: Receives audio stream directly from browser/app. STT (Speech-to-Text): Translates speech to text instantly. LLM Processing: AI thinks and generates text responses. TTS (Text-to-Speech): Converts responses to speech. Output: Plays back to users. The key point is these steps occur almost in parallel (streaming) to create a seamless conversation experience. Detailed Experience in the Event The workshop helped me systematize knowledge and see a bigger picture of AI\u0026rsquo;s future.\n1. The Shift from \u0026ldquo;Question-Answer\u0026rdquo; to \u0026ldquo;Action\u0026rdquo; (Agentic AI) What impressed me most was the thinking about Agentic AI. Previously, I only viewed AI as an intelligent lookup tool. But with AgentCore, AI becomes a \u0026ldquo;digital employee\u0026rdquo; capable of autonomously planning and using tools (API, Code, Search) to solve problems completely. This is a quantum leap in usage value.\n2. Solving the \u0026ldquo;Production\u0026rdquo; Problem The sharing about \u0026ldquo;The Prototype to Production Chasm\u0026rdquo; was very practical. It explains why many AI projects die young. AWS providing security layers (Identity) and monitoring (Observability) in Bedrock Agent is the key for enterprises to dare trust giving AI authority to impact real systems.\n3. Potential of Voice AI with Pipecat The Pipecat demo showed the future of contactless communication. Combining WebRTC and LLM to create real-time conversations opens countless applications: from automatic customer service hotlines, virtual IELTS practice, to interview assistants.\nConclusion The \u0026ldquo;Generative AI \u0026amp; Agentic AI on AWS\u0026rdquo; workshop clearly outlined the AI capability development roadmap:\nPresent: We use RAG and Prompt Engineering to effectively exploit data. Near Future: We shift to Agentic AI, where autonomous systems (Autonomous Agents) will replace humans in operating complex processes. Tools: With AWS\u0026rsquo;s comprehensive ecosystem (Bedrock, AgentCore) and Open Source community (Pipecat, LangChain), technical barriers have been significantly reduced, making way for creativity in business solutions. Some images from participating in the event Image of over 400 participants attending the AWS Cloud Mastery Series #1 Event\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn comprehensive knowledge about the diverse storage services on AWS. Focus deeply on the core service Amazon S3 (Simple Storage Service), an object storage service, including its characteristics (like 11 nines durability, replication across 3 AZs), and Storage Classes. Learn about important features like Lifecycle Management, Versioning, and Static Website Hosting. Large-scale data migration solutions (the Snow Family), hybrid storage solutions connecting on-premise with the cloud (Storage Gateway), the centralized backup management service (AWS Backup), and the basic concepts and strategies for Disaster Recovery (DR). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Amazon Simple Storage Service - S3\n- Learn about the architecture of Amazon S3, an object storage service, suitable for Write-Once-Read-Many (WORM) data.\n- Understand the technique of automatic data replication across 3 AZs within 1 Region to ensure high availability.\n- Learn about the durability of S3, designed for up to 99.999999999% (11 nines).\n- Understand the technique of uploading (HTTP PUT) and accessing (HTTP GET) S3 data via REST API.\n- Learn about the architecture of Storage Classes, including S3 Standard, S3 Standard-IA, S3 Intelligent-Tiering, S3 One Zone-IA, and Amazon Glacier/Deep Archive.\n- Understand the Object Life Cycle Management technique to automatically move objects between storage classes over time.\n- Understand the technique for hosting a Static Website (suitable for Single Page Applications) and configuring CORS (Cross-origin resource sharing).\n- Understand access control techniques using S3 Access Control List (ACL) (attached to buckets/objects) and S3 Bucket Policy (easier to manage).\n- Learn about the architecture of S3 Endpoints, which allow access to S3 buckets over the AWS private network without needing the Internet.\n- Understand the Versioning technique to recover objects after accidental deletion or overwrite, and to support protection against ransomware.\n- Understand the S3 performance optimization technique by using random prefixes for object keys, helping S3 store objects across multiple partitions.\n- Learn about the architecture of S3 Glacier, a low-cost, long-term archival service that requires data to be retrieved (Expedited, Standard, Bulk) to an S3 Bucket before use. 29/09/2025 29/09/2025 Module 04 3 Snow Family\n- Learn about the Snow Family services (Snowball, Snowball Edge, Snowmobile) used to migrate PetaByte (PB) to Exabyte (EB) scale data from on-premise to AWS (S3 or Glacier).\n- Understand the technique of Snowball Edge, which is a special device with available compute resources to process data locally.\nAmazon Storage Gateway\n- Learn about the architecture of AWS Storage Gateway, a Hybrid storage solution that combines storage capacity on AWS with on-premise.\n- Understand the techniques of the three types of gateways:\n+ File Gateway: Allows storing files on S3 via NFS and SMB protocols.\n+ Volume Gateway: Provides block storage via iSCSI, with data stored on S3.\n+ Tape Gateway: Provides a virtual tape library (VTL) iSCSI, storing virtual tape data in S3 or Glacier. 30/09/2025 30/09/2025 Module 04 4 Disaster Recovery on AWS\n- Understand the technique\u0026hellip; for designing Disaster Recovery (DR) based on two key metrics:\n+ RTO (Recovery Time Objective): The time required to restore service.\n+ RPO (Recovery Point Objective): The maximum period of time during which data might be lost.\n- Learn about the 4 DR strategies on AWS: Backup and Restore, Pilot Light, Low Capacity Active-Active, and Full Capacity Active-Active.\nAWS Backup\n- Learn about the AWS Backup service, a centralized management service that allows configuring and scheduling, and setting retention policies for backing up multiple AWS resources (EBS, EC2, RDS, EFS, Storage Gateway\u0026hellip;). 01/10/2025 01/10/2025 Module 04 5 Lab: 000057 - Getting Started with Amazon S3\n- Create S3 Bucket\n- Upload data to S3\n- Host static website on S3\nLab: 000013 - AWS Backup\n- Prepare infrastructure\n- Create Backup Plan\n- Set up Notification\n- Verify operation\nLab: 000014 - AWS Import/Export\n- Prepare virtual machine\n- Import virtual machine to AWS\n- Export virtual machine from AWS 02/10/2025 02/10/2025 Module 04 6 Lab: 000024 - Storage Gateway\n- Create Storage Gateway\n- Create File Sharing\n- Connect the File Share to the machine\nLab: 000025 - FSx\n- AWS Managed MS AD\n- Deploy Instance\n- Set up and use FSx\n[Supplemental Research] - AWS Skill Builder\n- A series of in-depth theory lessons for storage specialists on AWS.\n- Storage Learning Plan: Block Storage\n- Storage Learning Plan: Object Storage 03/10/2025 03/10/2025 Module 04 Research Link Week 4 Achievements: S3 Service (Basics): Clearly understand that Amazon S3 is an object storage service, not block storage, operating on a WORM (Write Once, Read Many) model. Lesson on Durability: Know that S3 is designed for 11 nines (99.999999999%) of durability by automatically replicating data across 3 Availability Zones (AZs). S3 Cost Optimization Techniques: Differentiate between Storage Classes such as S3 Standard (frequent access), S3 Standard-IA (infrequent access), and S3 Glacier (long-term, low-cost archival, requires retrieval). S3 Automation Techniques: Know how to use Object Life Cycle Management to automatically transition data to cheaper tiers (e.g., from Standard to Glacier) over time. Understand Trigger Events (e.g., triggering a serverless function upon file upload). S3 Security Techniques: Differentiate between two access control mechanisms: S3 ACL (legacy mechanism) and S3 Bucket Policy (easier to define access permissions). Lesson on Data Protection (S3): Clearly understand the Versioning feature, which allows restoring previous versions of a file, helping to protect against accidental deletion or ransomware attacks. S3 Networking Techniques: Know how to use an S3 Endpoint to access S3 from within a VPC over the AWS private network without needing the Internet. Know how to host a Static Website on S3 and configure CORS. Data Migration Service (Migration): Recognize the Snow Family (Snowball, Snowmobile) as the physical solution for large-scale (Petabyte, Exabyte) data migration from on-premise. Hybrid Storage Service: Understand Storage Gateway as a hybrid storage solution, allowing on-premise applications to use protocols (NFS, SMB, iSCSI) to store data on S3/Glacier. Lesson on Disaster Recovery (DR): Understand the 2 basic concepts for designing DR: RTO (recovery time) and RPO (acceptable data loss). Backup Service (Backup): Know that AWS Backup is a centralized management service that helps automate backups (schedule, retention) for multiple AWS resources (EBS, RDS, EFS\u0026hellip;). Hands-on: Understand the practical steps to create an S3 bucket, host a static website, and configure AWS Backup. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 - Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data)\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: DATA SCIENCE WORKSHOP ON AWS\nTime: 09:30, October 16, 2025\nLocation: FPT University, D1 Street, High-Tech Park, Tang Nhon Phu Ward, Ho Chi Minh City.\nRole in the event: Attendees\nEvent 4 Event Name: AWS Cloud Mastery Series #1\nTime: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\nEvent 5 Event Name: AWS Cloud Mastery Series #2\nTime: 09:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\nEvent 6 Event Name: AWS Cloud Mastery Series #3\nTime: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.4-test-chatbox/","title":"Testing Chatbot (RAG)","tags":[],"description":"","content":"Target After successfully ingesting data into the Vector Store, it is time to verify the results. In this section, you will act as an end-user, asking the Chatbot questions directly within the AWS Console interface to observe how the RAG system operates.\nWe will focus on 2 factors:\nAccuracy: Does the AI answer correctly based on the documents? Transparency: Can the AI cite the source (Citation) of the information? Implementation Steps Step 1: Configure the test window\nTo start chatting, we need to select a Foundation Model that will act as the \u0026ldquo;responder\u0026rdquo;.\nIn your Knowledge Base details interface, look at the right panel titled Test knowledge base. Click the Select model button.\nIn the selection panel that appears: Category: Select Anthropic. Model: Select Claude 3 Sonnet (or Claude 3.5 Sonnet / Haiku depending on the model you enabled). Throughput: Keep On-demand. Click Apply. Step 2: Conduct conversation (Chat)\nNow, try asking a question related to the document content you uploaded.\nIn the input box (Message input), type your question. Example: If you uploaded the \u0026ldquo;AWS Overview\u0026rdquo; document, ask: \u0026ldquo;Can you explain to me what EC2 is?\u0026rdquo;. Click Run. Observe the result: The AI will think for a few seconds (querying the Vector Store). Then, it will answer in natural language, summarizing the found information. Step 3: Verify data source\nThis is the most important feature of RAG that distinguishes it from standard ChatGPT: the ability to prove the source of information.\nIn the AI\u0026rsquo;s response, pay attention to the small numbers (footnotes) or the text Show source details. Click on those numbers or the details button. A Source details window will appear, displaying: Source chunk: The exact original text segment that the AI found in the document. Score: Similarity score (relevance). S3 Location: Path to the original file. Seeing this original text segment proves that the AI is not \u0026ldquo;hallucinating\u0026rdquo; but is actually reading your documents.\nStep 4: Test with irrelevant questions (Optional)\nTo see how the system reacts when information is not found.\nAsk a question completely unrelated to the documents. Example: \u0026ldquo;Can you explain some knowledge about personal finance?\u0026rdquo; (While your documents are about Cloud Computing). Expected Result: The AI might answer based on its general knowledge (if not restricted). OR the AI will answer \u0026ldquo;Sorry, I am unable to answer your question based on the retrieved data\u0026rdquo; - This is the ideal behavior for an enterprise RAG application. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2: From DevOps, IaC to Container \u0026amp; Observability\u0026rdquo; Event Purpose Mindset Transformation: Deep understanding of the Value Cycle and how DevOps culture helps enterprises balance development speed and system stability. Modern Infrastructure Revolution: Transform from risky manual management (ClickOps) to Infrastructure as Code (IaC) with three tools: CloudFormation, Terraform, and CDK. Container Strategy: Analyze architecture and make decisions to choose the most suitable orchestration platform: From simple (App Runner), deep integration (ECS) to flexible scaling (EKS). Deep Observability: Establish proactive monitoring systems, not just for error reporting but also to understand system behavior and optimize user experience with CloudWatch and X-Ray. Speaker List AWS \u0026amp; Cloud Engineers Team: Bringing overall architectural perspective, Platform Engineering strategy, and practical demos. Mr. Tran Vi: FCJer 2024 - Sharing practical experience from the community. Mr. Long Quy Nghiem: FCJer 2024 - Sharing perspectives from someone newly approaching and developing Cloud skills. Detailed Content 1. DevOps Mindset \u0026amp; CI/CD Pipeline (Foundation of Thinking) The event emphasized that DevOps is not a job title or tool, but a philosophy focused on optimizing value flow from idea to end user.\nThe Value Cycle:\nA closed cycle consisting of 5 stages: Insights \u0026amp; Analysis -\u0026gt; Portfolio \u0026amp; Backlog (Planning) -\u0026gt; Continuous Integration -\u0026gt; Continuous Testing -\u0026gt; Continuous Delivery. Core Objective: Solve the classic \u0026ldquo;Speed vs. Stability\u0026rdquo; problem. DevOps proves we can increase the speed of launching new features (Speed) without sacrificing system safety (Stability) through automation. Redefining CI/CD concepts:\nContinuous Integration (CI): A culture of frequent code commits (daily). Every code change triggers automatic Build and Test processes to detect errors immediately (Fail fast), avoiding technical debt accumulation. Continuous Delivery: After passing CI, code is automatically deployed to Staging environment. However, deploying to Production is a business decision requiring human confirmation (Manual Approval). Continuous Deployment: The highest level of automation. If code passes all tests, it goes straight to Production without any human intervention. Effective Pipeline Strategy:\nCentralized CI: Platform team builds standard pipelines, ensuring security and compliance, but empowers Developers to self-service to avoid bottlenecks. Artifact Management: Follow the immutable principle \u0026ldquo;Build Once, Deploy Anywhere\u0026rdquo;. Source code is packaged only once into Binary/Docker Image (Artifact). Test, Staging, and Prod environments all use this same Artifact to ensure what was tested is what will run. Fail Fast Mechanism: Pipeline must be designed to stop immediately when there\u0026rsquo;s a problem (Compilation error, Bad code, Security vulnerability, Slow test). Better to stop the process early than let errors slip through to more expensive later stages. Effectiveness Metrics:\nUse Heatmap to visualize entire organization\u0026rsquo;s performance. Focus on 4 golden metrics (DORA Metrics): Deployment frequency, Lead time for changes, Change Failure Rate, and Mean Time To Recovery (MTTR). 2. Infrastructure as Code (IaC) - From ClickOps to Code This section analyzed the mandatory shift from manual management to infrastructure automation.\nThe Problem of \u0026ldquo;ClickOps\u0026rdquo;: Clicking on AWS Console is intuitive but poses major risks: Human errors (forgetting configuration), inability to recreate identical environments (Inconsistent), and extremely difficult when scaling. IaC Solution: Turn infrastructure into Code to enjoy software development benefits: Version Control (Git), Code Review, Testing, and Reusability. Detailed analysis of 3 leading IaC tools:\n1. AWS CloudFormation (Native Tool):\nAWS\u0026rsquo;s \u0026ldquo;official\u0026rdquo; tool, using YAML/JSON to declare desired state (Declarative). Template Anatomy: Structure includes Parameters (Flexible inputs), Mappings (Value mapping by region/environment), and Resources (Specific AWS resources). Stack Management: Manage resources in groups (Stack). When deleting Stack, all related resources are cleaned up, avoiding resource waste. 2. Terraform (Multi-Cloud Powerhouse):\nOpen-source tool using HCL language. The #1 choice for Multi-cloud strategy. Safe Process: Write -\u0026gt; Plan -\u0026gt; Apply. The Plan step allows previewing how changes will impact actual infrastructure before applying, helping avoid catastrophic mistakes. State File: Terraform\u0026rsquo;s \u0026ldquo;memory\u0026rdquo;, storing actual infrastructure state for comparison and synchronization. 3. AWS CDK (Cloud Development Kit):\nApproach infrastructure with modern programming languages (Python, TS, Java\u0026hellip;), leveraging loops, conditions, and object-oriented programming. Power of Abstraction (Constructs): L1: Raw configuration (CloudFormation equivalent). L2: Pre-built Classes with safe default configurations (Best practices). L3: Design Patterns building complex systems (VPC + Cluster + LB) with just a few lines of code. Drift Detection: Feature that detects \u0026ldquo;configuration drift\u0026rdquo; - the difference between Code (IaC) and reality (manual edits). This is an important tool for maintaining operational discipline.\n3. Containerization - Application Running Strategy Deep dive into orchestration models to choose optimal solutions:\nKubernetes (K8s):\nThe world\u0026rsquo;s Container standard. Complex architecture including Control Plane (brain) and Worker Nodes (muscle). Suitable for extremely large systems needing deep customization, but requires highly skilled operations team. Comparing Amazon ECS vs. Amazon EKS:\nAmazon ECS: \u0026ldquo;Simplified\u0026rdquo;. Designed by AWS to integrate seamlessly with other services (ALB, IAM). Suitable for teams wanting to focus on applications, less worry about cluster operations. Amazon EKS: \u0026ldquo;Open standard\u0026rdquo;. AWS\u0026rsquo;s Managed Kubernetes version. Suitable for enterprises needing K8s tool ecosystem or running Hybrid-cloud. Compute Options:\nEC2 Launch Type: You manage virtual machines (Servers). Maximum control but must handle OS patching, agent updates. AWS Fargate (Serverless): You only manage Containers. AWS handles all underlying server infrastructure. Eliminates OS maintenance burden. AWS App Runner:\n\u0026ldquo;Zero-ops\u0026rdquo; solution. For Developers wanting to deploy Web Apps/APIs as quickly as possible. Automates everything from Source Code -\u0026gt; Build -\u0026gt; Deploy -\u0026gt; Load Balancer -\u0026gt; HTTPS URL. 4. Observability - Monitoring \u0026amp; Optimization Shift from \u0026ldquo;Monitoring\u0026rdquo; (Is the system alive?) to \u0026ldquo;Observability\u0026rdquo; (Why is the system slow?).\nAmazon CloudWatch (Monitoring Center):\nMetrics: Quantitative measurements (High CPU, Full RAM). Logs: Detailed activity logs. Logs Insights helps query and analyze millions of log lines in seconds. Alarms: Automatic reaction mechanism. When metrics exceed threshold -\u0026gt; Send alert or Auto-scale system. AWS X-Ray (Distributed Tracing):\nSolves the \u0026ldquo;black box\u0026rdquo; problem in Microservices. Distributed Tracing: Maps the journey of a request through dozens of different services. Helps precisely identify which service is causing slowness (Latency) or errors to fix at the root. AWS Observability Best Practices:\nReference AWS Observability Recipes to apply standard monitoring patterns. Clearly distinguish roles: Logs provide event details, Traces provide context and flow of those events. Detailed Experience in the Event The specialized session completely changed how I view software system operations:\n1. The Shift from \u0026ldquo;Ops\u0026rdquo; to \u0026ldquo;Platform Engineering\u0026rdquo; I realized the role of modern DevOps is not \u0026ldquo;server attendant\u0026rdquo; or \u0026ldquo;hired deployer\u0026rdquo;. DevOps are Platform Builders. The mission is to create a safe and automated \u0026ldquo;highway\u0026rdquo; (Pipeline \u0026amp; Infrastructure), helping Developers bring code to market themselves (Self-service) without waiting, while still ensuring safety rules.\n2. Operational Discipline The concept of Immutability in Artifact management and Drift Detection in IaC is truly valuable. In enterprise environments, \u0026ldquo;it works\u0026rdquo; is not enough, it must be \u0026ldquo;stable and consistent\u0026rdquo;. Prohibiting manual edits (ClickOps) and following the \u0026ldquo;Code -\u0026gt; Build -\u0026gt; Deploy\u0026rdquo; process is vital to avoid silly human errors.\n3. Smart Tool Selection Strategy The biggest lesson is there\u0026rsquo;s no \u0026ldquo;best\u0026rdquo; tool, only \u0026ldquo;most suitable for context\u0026rdquo; tool:\nNeed absolute stability and latest AWS feature support? Choose CloudFormation. Enterprise using multiple Clouds (Multicloud)? Choose Terraform. Team strong in programming, want to write less code but get large infrastructure? Choose AWS CDK. Want to run simple Web App without managing K8s? App Runner is perfect. Conclusion The \u0026ldquo;DevOps \u0026amp; IaC Mastery\u0026rdquo; session painted a technology maturity roadmap:\nOn Mindset: Shift from intuitive, manual work to systematic thinking, automation, and data-driven measurement. On Infrastructure: Control infrastructure with Code (IaC) to achieve flexibility and unlimited scalability. On Operations: Combine Container power with system understanding capability (Observability) to ensure services are always available and optimized. This is the solid knowledge foundation for me to confidently step into building large-scale systems on AWS.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn foundational knowledge and core security services on AWS, centered around the \u0026ldquo;Security is job zero\u0026rdquo; philosophy. Start with the most basic concept: the Shared Responsibility Model. Focus deeply on managing identity and access (Identity and Access Management - IAM), including components: User, Group, Policy, and Role. Expand learning to identity management services at a larger scale, such as AWS Organizations (managing multiple accounts), AWS Identity Center (SSO) (single sign-on), and Amazon Cognito (user management for web/mobile apps). Gain solid knowledge of data protection through encryption with AWS KMS and monitoring/compliance checks with AWS Security Hub. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Shared Responsibility Model\n- Learn about the Shared Responsibility Model, in which AWS is responsible for security of the cloud (physical infrastructure, underlying software) and the customer is responsible for security in the cloud (configuration, data, applications).\n- Understand how security responsibilities change depending on the service type (infrastructure, combined management, or fully managed).\nAWS Identity and Access Management (IAM)\n- Learn about the Root Account, the account with absolute full permissions, and best practices to protect it (create an IAM Admin User for regular use, lock away root credentials).\n- Learn about IAM User, a principal used to interact with AWS, which has no permissions by default when created.\n- Understand the technique for efficient user management by grouping multiple IAM Users into an IAM Group.\n- Learn about IAM Policy, a JSON document that defines permissions, including 2 types:\n+ Identity-based Policy: Attached directly to an IAM Principal (User, Group, Role).\n+ Resource-based Policy: Attached directly to a resource (e.g., S3 Bucket Policy).\n- Understand the IAM permission evaluation technique, where an explicit deny always takes precedence, regardless of any other Allow policy.\n- Learn about the architecture of IAM Role, a set of permissions (policy) without permanent credentials (password/access key).\n- Understand the Assume Role technique: An IAM User (or Service) uses the AWS STS (Security Token Service) to temporarily \u0026ldquo;assume\u0026rdquo; the IAM Role\u0026rsquo;s permissions and receive temporary credentials.\n- Understand the practical application of IAM Role, e.g., granting an EC2 service permission to access S3 without storing access keys on the server. 06/10/2025 06/10/2025 Module 05 3 Amazon Cognito\n- Learn about Amazon Cognito, a service for managing authentication (login, sign-up) and authorization for end-users of web and mobile applications (different from IAM Users, who are AWS administrators).\n- Learn about the two main components of Cognito:\n+ User Pool: A user directory that manages users, supporting direct login or login via third-party providers (Facebook, Google).\n+ Identity Pool: Grants application users access (usually temporary) to other AWS services.\nAWS Organizations\n- Learn about AWS Organizations, a service that helps centrally manage and govern multiple AWS accounts.\n- Understand the Consolidated Billing technique for all accounts.\n- Understand the technique of grouping accounts into OUs (Organization Units) and applying Service Control Policies (SCP) to limit the maximum permissions that IAM Users/Roles in that account can perform (including deny-based).\nAWS Identity Center (SSO)\n- Learn about AWS Identity Center (SSO), a service that helps centrally manage access (single sign-on) to all AWS accounts in an Organization and to external applications.\n- Understand the technique of using Permission Sets (a set of permissions stored in Identity Center) to assign to Users/Groups. When a user accesses an account, the Permission Set is granted as an IAM Role within that account. 07/10/2025 07/10/2025 Module 05 4 AWS Key Management Service (KMS)\n- Learn about AWS KMS, a service to create and manage encryption keys to protect data at rest (Encryption at rest).\n- Learn about\u0026hellip; CMK (Customer Managed Key) (the master key within KMS) and Data Key (the key used to encrypt/decrypt actual data, generated by the CMK).\nAWS Security Hub\n- Learn about AWS Security Hub, a service for continuous security checks, based on AWS best practices and industry standards (like PCIDSS).\n- Understand how Security Hub provides results as a score and identifies resources that need attention.\nLab: 000002 - Getting Started with IAM and IAM Role\n- IAM Group and IAM User\n- Create IAM Role\n- Assume Role\nLab: 000044 - IAM Role and Condition\n- Introduction to IAM\n- Create EC2 Admin User\n- Create RDS Admin User\n- Create Admin Group-Configure IAM Role Condition\n- Create IAM Role with Admin rights 5.2 Create IAM User 5.3 Configure Switch role 5.4 Restrict IP 5.5 Restrict by time. 08/10/2025 08/10/2025 Module 05 5 Lab: 000048 - IAM Role and Application\n- Use access key\n- IAM Role on EC2\nLab: 000030 - IAM Permission Boundary\n- Introduction to IAM Permission Boundary\n- Create limiting Policy\n- Create IAM User with limited permissions\n- Test the limited User\nLab: 000027 - Tags and Resource Groups\n- Use tags\n- Use tags via Console\n- Display tags\n- Add or remove tags\n- Tag a virtual machine\n- Filter resources by tag\n- Use tags via CLI\n- Resource Group\nLab: 000028 - Manage EC2 via Resource Tag\n- Create IAM Policy\n- Create IAM Role\n- Test IAM Role 09/10/2025 09/10/2025 Module 05 6 Lab: 000018 - Using AWS Security Hub\n- Security standards\n- Activate Security Hub\n- Score for each standard set\nLab: 000012 - Using AWS SSO\n- Preparation steps\n- Create AWS Account in AWS Organizations\n- Set up Organization Unit\n- Set up AWS SSO\n- Verify\nLab: 000033 - KMS Workshop\n- Set up environment\n- Getting started with AWS KMS\n- Encryption with AWS KMS\n- Key Policy and best practices\n- Monitoring AWS KMS usage.\n[Supplemental Research] - AWS Certified Security Specialty All-in-One-Exam Guide (Exam SCS-C01)\n- Study material for the Security Specialty certification exam 10/10/2025 10/10/2025 Module 05\nResearch Link Week 5 Achievements: Foundational Lesson: Master the Shared Responsibility Model, clearly understanding AWS\u0026rsquo;s responsibilities versus the customer\u0026rsquo;s. IAM Service (Core): Clearly distinguish between the Root Account (full permissions, needs to be locked away) and IAM User (used daily, no permissions by default). Master the 3 main components for granting permissions: IAM User (the entity), IAM Policy (the permission - written in JSON), and IAM Group (a group of entities). Clearly understand IAM Role: a mechanism to grant temporary permissions (no permanent credentials) to both Users and Services (like EC2). IAM Techniques (Important): Know how a User/Service \u0026ldquo;receives\u0026rdquo; a Role\u0026rsquo;s permissions through the Assume Role technique (using the STS service). Understand the permission evaluation rule: An Explicit Deny always overrides any Allow permissions. Identity Management Services (Identity Services): Clearly differentiate between IAM (manages AWS administrators) and Amazon Cognito (manages end-users of web/mobile apps). Know that Cognito User Pool is the user directory (can log in with Facebook, Google) and Identity Pool is what grants those users access to AWS resources. Multi-Account Management Service (Multi-Account): Understand AWS Organizations is used for centrally managing multiple accounts, enabling Consolidated Billing. Know how to use Service Control Policies (SCP) within an Organization to limit the maximum permissions of member accounts. Understand AWS Identity Center (SSO) as the single sign-on solution, using Permission Sets to grant access to accounts within the Organization. Encryption Service (Encryption): Know AWS KMS is the service for creating and managing encryption keys. Understand the Encryption at Rest mechanism and differentiate between CMK (the master key in KMS) and Data Key (the key used to encrypt the actual data). Security Monitoring Service (Monitoring): Know AWS Security Hub is the service that scans and provides security scores, helping to check compliance against standards (like PCIDSS). Hands-on: Practice creating and managing Users, Groups, Policies, and Roles. Practice implementing SSO and KMS. Practice using advanced IAM features like Conditions and Permission Boundaries. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.5-client-integration/","title":"Client Application Integration (Optional)","tags":[],"description":"","content":"Target You will turn Python code into a professional Web Chatbot GUI (Graphical User Interface) that is user-friendly (similar to the ChatGPT interface) in just a few minutes.\nWe use:\nBackend: Python. Frontend: Streamlit. AI Model: Claude 3.5 Sonnet. Implementation Steps Part I: Configure AWS Credentials\nStep 1: Install AWS CLI\nOpen Terminal on your computer.\n# macOS brew install awscli # Linux curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Step 2: Configure credentials\naws configure Enter the information when prompted:\nAWS Access Key ID: YOUR_ACCESS_KEY AWS Secret Access Key: YOUR_SECRET_KEY Default region name: us-east-1 Default output format: json Step 3: Verify configuration\n# Check credentials aws sts get-caller-identity # Check Bedrock connection aws bedrock-agent-runtime list-knowledge-bases --region ap-southeast-1 Security notes:\nDO NOT commit credentials to Git DO NOT share credentials with others Use IAM roles when possible Rotate credentials periodically Required permissions:\nIAM User needs the following permissions:\nbedrock:InvokeModel bedrock:RetrieveAndGenerate bedrock:Retrieve s3:GetObject (for Knowledge Base) Troubleshooting:\nError \u0026ldquo;Unable to locate credentials\u0026rdquo;:\nCheck if ~/.aws/credentials file exists Check file format is correct Try running aws configure again Error \u0026ldquo;AccessDeniedException\u0026rdquo;:\nCheck IAM permissions Ensure region is correct (ap-southeast-1) Check Knowledge Base ID is correct Error \u0026ldquo;ExpiredToken\u0026rdquo;:\nCredentials have expired Need to create new credentials from AWS Console Part II: Clone Project from pre-made GitHub\nStep 1: Access the following GitHub link\nPlease download and open the folder above using Visual Studio Code:\nhttps://github.com/DazielNguyen/chatbot_with_bedrock.git\nStep 2: Install libraries and Python environment\nInstall environment:\nMacOS: python3 -m venv .venv Win: python -m venv .venv Activate environment:\nMacOS: source .venv/bin/activate Win: .venv\\Scripts\\activate Install libraries:\nMacOS/ Win: pip install -r requirements.txt Step 3: Get the ID of the created Knowledge Base\nAccess Amazon Bedrock -\u0026gt; Knowledge Base -\u0026gt; knowledge-base-demo Update \u0026ldquo;KB_ID=\u0026ldquo;YOUR_KNOWLEDGE_BASE_ID\u0026rdquo;\u0026rdquo; Step 4: Run Streamlit - Chatbot UI and Experience\nRun Terminal: streamlit run start.py When the command finishes running, the following page will appear: Try asking some questions you uploaded to the Knowledge Base earlier. The Chatbot has returned results based on the data file you provided, with citations of your data sources. Conclusion Congratulations on successfully building a Web Chatbot built with Amazon Bedrock\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a RAG Application using Knowledge Bases for Amazon Bedrock Overview Knowledge Bases for Amazon Bedrock is a fully managed feature that helps you implement RAG (Retrieval-Augmented Generation) techniques by connecting Foundation Models to your internal data sources to deliver accurate, cited, and contextually relevant responses.\nRAG is a technique to optimize Large Language Model (LLM) output by retrieving information from a trusted external database (Retrieval) and adding it to the context (Augmentation) before generating the answer (Generation). This method helps overcome limitations regarding outdated training data and ensures the AI answers based on the actual provided information.\nIn this lab, we will learn how to build an AI assistant capable of \u0026ldquo;reading and understanding\u0026rdquo; proprietary enterprise documents. You will perform the process from data ingestion and creating vector indexes to configuring the model to answer questions based on those documents without managing any servers.\nWe will use three main components to set up a complete RAG processing workflow:\nData Source (Amazon S3) - Acts as the repository of \u0026ldquo;truth\u0026rdquo;. You will upload documents (PDF, Word, Text) to an S3 bucket. The Knowledge Base will use this source to synchronize data. Vector Store (OpenSearch Serverless) - The place to store vector embeddings (numerically encoded data). When a user asks a question, the system will perform a semantic search here to extract the most relevant text segments instead of standard keyword searching. Foundation Model (Claude 3) - The Large Language Model acting as the processing brain. It receives the user\u0026rsquo;s question along with information found from the Vector Store, then synthesizes and generates a natural, accurate answer accompanied by source citations. Outcomes By the end of the workshop, you will have a practical, functioning Chatbot system with the following features:\nQ\u0026amp;A chat regarding proprietary document content. Accurate answers, no hallucinations. Source citations (knowing exactly which page the answer comes from). Rapid deployment without writing complex data processing code. Contents Workshop Overview Environment Preparation Create and Configure Knowledge Base Test Chatbot (RAG) Client Application Integration (Optional) Update Data Clean Up Resources "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":""},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Review foundational database (DB) concepts, including RDBMS (primary key, foreign key), optimization techniques (Index, Partition), and operational concepts (Database Log, Buffer). Clearly differentiate between the two main DB system types: OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing or Data Warehouse). Understand the managed relational database service Amazon RDS, including core features like Multi-AZ (for high availability) and Read Replicas (for read performance). Learn about Amazon Aurora, AWS\u0026rsquo;s cloud-native DB service, with its unique shared storage architecture, high performance, and superior features like Zero Replication Lag. Learn about Amazon Redshift, the petabyte-scale Data Warehouse service designed for OLAP, and understand its MPP architecture and Columnar Storage technique. Understand the role of Amazon ElastiCache (Redis, Memcached) as a high-speed caching layer to reduce load on the primary database. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Database Concepts\n- Lesson: Review fundamental database concepts like Database (a structured information system) and Session (a working session).\n- Learn about architecture: Relational Databases (RDBMS), including Primary Key (to uniquely identify a row) and Foreign Key (to create links between tables).\n- Understand the technique: Normalization, a technique of dividing data into multiple tables (using keys) to prevent data duplication.\n- Understand the technique: Performance Optimization:\n+ Index: A data structure that speeds up retrieval (read) operations, but increases the cost of writes.\n+ Partition: Dividing a large table into many smaller parts to speed up queries.\n+ Execution Plan: The set of steps the database decides to use to access data (e.g., whether to use an Index or not).\n- Understand the technique: Ensuring integrity and speed:\n+ Database Log: Records all changes, important for recovery and replication.\n+ Buffer: A temporary storage area in RAM, helping to speed up reads because reading from RAM is faster than reading from disk.\n- Lesson: Database Classification:\n+ RDBMS (ACID): Fixed structure (Schema), storage optimized (Normalization), scales vertically (Vertical Scaling).\n+ NoSQL (BASE): Flexible structure (Dynamic Schema), performance optimized (Denormalization), scales horizontally (Horizontal Scaling).\n- Lesson: System Classification:\n+ OLTP (Online Transaction Processing): Transaction processing systems (banking, ordering), need to quickly handle read/write/update operations and ensure integrity (roll back).\n+ OLAP (Online Analytical Processing): Data Warehouse systems, store historical data for complex analysis (reporting, finding trends). 13/10/2025 13/10/2025 Module 06 3 Amazon RDS\n- Learn about the service: Amazon RDS (Relational Database Service), a fully managed relational database service, supporting popular engines (MySQL, PostgreSQL, Oracle, etc.).\n- Lesson: The goal of RDS is to automate administrative tasks (updates, backups) so users can focus on the application.\n- Understand the technique: Automated Backups of the database and transaction logs, allowing for Point-in-Time Recovery within a 35-day window.\n- Learn about architecture: Multi-AZ (High Availability)\n+ Automatically creates a standby replica in another AZ.\n+ Uses Synchronous Replication.\n+ Supports Automatic Failover if the primary database fails.\n- Learn about architecture: Read Replicas (Read Performance Optimization)\n+ Creates read-only copies to offload the primary database (e.g., for reporting tasks).\n+ Uses Asynchronous Replication, which can cause \u0026ldquo;replication lag\u0026rdquo;.\n- Lesson: RDS is often used for OLTP applications and is protected by Security Groups. 14/10/2025 14/10/2025 Module 06 4 Amazon Aurora\n- Learn about the service: Amazon Aurora, a database developed by AWS, compatible with MySQL/PostgreSQL, part of the RDS service but with higher performance (3-5x faster).\n- Learn about architecture: The biggest difference for Aurora is the redesigned storage layer.\n- Learn about architecture: An Aurora \u0026ldquo;Cluster\u0026rdquo; consists of 1 Writer (write instance) and up to 15 Readers (read instances), all sharing a single (Cluster Volume) storage partition.\n- Understand the technique: Data on the Cluster Volume is replicated 6 times across 3 AZs to ensure durability.\n- Lesson: Aurora\u0026rsquo;s outstanding advantage is Zero Replication Lag because the Readers read from the same volume as the Writer.\n- Understand the technique: Enterprise features like Backtrack (rewind the database without restoring) and Global Database (create read-only replicas in different Regions). 15/10/2025 15/10/2025 Module 06 5 Amazon Redshift\n- Learn about the service: Amazon Redshift, a petabyte-scale Data Warehouse service, optimized for OLAP.\n- Learn about architecture: Massively Parallel Processing (MPP).\n+ Leader Node: Receives, parses, and coordinates queries.\n+ Compute Nodes: Store and execute parts of the work in parallel.\n- Understand the technique: Columnar Storage.\n+ Unlike OLTP (stores by row), Redshift stores data from the same column together.\n+ This technique is extremely efficient for analytical (OLAP) queries (e.g., Calculate average age only needs to read the Age column).\n- Understand the technique: Redshift Spectrum, allows running SQL queries directly on data in Amazon S3 without needing to load it.\nAmazon ElastiCache\n- Learn about the service: Amazon ElastiCache, a high-speed in-memory caching service.\n- Objective: Speed up applications and reduce the load on the primary database (like RDS).\n- Learn about supported engines: Redis (supports many data types, often preferred) and Memcached.\n- Lesson: It is the user\u0026rsquo;s responsibility to write and manage the Caching Logic (logic that decides what and when to cache) within their application. 16/10/2025 16/10/2025 Module 06 6 Lab: 000005 - Getting Started with Amazon RDS\n1. Create a database on Amazon RDS\n2. Connect the application to the DB\n3. Backup and Restore\nLab: 000043 - Migrating Databases with DMS and SCT\n1. Preparation steps\n2. Oracle to Amazon Aurora (PostgreSQL)\n2.1 Convert Schema\n2.2 Migrate database.\n[Supplemental Research] - Database Internals\n- Document to learn how databases work internally.\nDatabase Internals Deep Distributed Systems [Supplemental Research] - The Data Warehouse Toolkit\n- Document to learn how to design and the techniques used in building a Data-warehouse\nData Warehouse Toolkit Definitive Dimensional 17/10/2025 17/10/2025 Module 06 Week 6 Achievements: Lesson (Foundational): Clearly differentiate between the two system models: OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing, data warehouse). Master basic DB optimization techniques: Index (speeds up reads) and Partition (divides tables). Understand the role of Database Log (for recovery/replication) and Buffer (uses RAM for speed). Service (RDS): Know Amazon RDS is a managed relational database (OLTP) service. Clearly distinguish RDS\u0026rsquo;s 2 main features: Multi-AZ (used for High Availability - HA) and Read Replicas (used to increase read performance). Technique (Replication): Differentiate Synchronous Replication (used for RDS Multi-AZ) and Asynchronous Replication (used for RDS Read Replicas, can have lag). Service (Aurora): Know Amazon Aurora is a high-performance, cloud-native database. Understand Aurora\u0026rsquo;s shared storage (Cluster Volume) architecture and its superior benefit of Zero Replication Lag. Be aware of advanced features like Backtrack and Global Database. Service (Redshift): Know Amazon Redshift is a data warehouse (OLAP) service. Understand the MPP (Massively Parallel Processing) architecture (includes Leader Node and Compute Nodes). Master the core technique of OLAP: Columnar Storage, which speeds up analytical queries. Service (ElastiCache): Know Amazon ElastiCache (Redis/Memcached) is an in-RAM caching service. Understand the role of caching is to reduce load on the primary DB. Be aware of the responsibility to write the Caching Logic in the application. Hands-on: Know how to create and operate (backup/restore) an RDS database. Know how to use DMS and SCT services to migrate a DB from Oracle to Aurora. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AWS/First Cloud AI Journey from 08/08 to 12/12, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in learning and researching the platform provided by AWS, as well as applying the knowledge learned to a group project with the FinTech theme, through which I improved my teamwork skills, task allocation for team members, and especially acquired desired skills such as Docker, AI Engineering, and applied AWS support services to programming work and the final project.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Enhance learning ability, need to improve knowledge about Solution Architecture from senior colleagues Improve problem-solving thinking, need to limit getting myself into struggle situations, ask friends and colleagues to improve difficult parts Need to explore and improve knowledge about new technologies to fully supplement knowledge to become an AI Engineer Communicate more with mentor colleagues, thereby enhancing knowledge about Cloud and applying it to future work "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.6-update-data/","title":"Update data","tags":[],"description":"","content":"Target One of the biggest advantages of RAG compared to Fine-tuning (retraining) a model is the ability to update data quickly. When a business has new regulations, you simply need to ingest them into the Knowledge Base, and the AI will \u0026ldquo;learn\u0026rdquo; them immediately.\nIn this section, we will simulate the following scenario:\nAsk the AI for a piece of non-existent information (The AI will answer that it doesn\u0026rsquo;t know). Provide that information to the system by uploading a new file. Ask the same question again to witness the AI answer correctly immediately. Implementation Steps Step 1: Verify the initial \u0026ldquo;lack of knowledge\u0026rdquo;\nWe need to confirm that the current AI knows nothing about the confidential information we are about to create.\nReturn to the Streamlit Chatbot interface (created in Part 5) or use the Test Knowledge Base window on the Console. Ask a question about hypothetical fake information. Example: \u0026ldquo;What is the activation code for Project Omega?\u0026rdquo; Observe the result: The AI will answer that it cannot find the information in the provided documents or will attempt a generic answer (if not restricted). Step 2: Create new data\nWe will create a text file containing this \u0026ldquo;secret\u0026rdquo; to ingest into the system.\nOn your computer, open Notepad (Windows) or TextEdit (Mac). Copy and paste the following content into the file: CONFIDENTIAL NOTICE: The secret Project Omega officially launches on 01/12/2025. The activation code is: \u0026#34;AWS-ROCKS-2025-SINGAPORE\u0026#34;. The Project Manager is Mr. Robot. Please keep this information strictly confidential. Save the file as: secret-project.txt. You can download the file here: TXT format file\nStep 3: Upload and Sync\nNow, we will feed this new knowledge into the AI\u0026rsquo;s \u0026ldquo;brain\u0026rdquo;.\nAccess the S3 Console, navigate to your old bucket (rag-workshop-demo).\nClick Upload -\u0026gt; Add files -\u0026gt; Select the secret-project.txt file -\u0026gt; Upload.\nSwitch to the Amazon Bedrock Console -\u0026gt; Select Knowledge bases from the left menu. Click on your Knowledge Base name. Scroll down to the Data source section, select the data source (s3-datasource). Click the Sync button (Orange). Wait: Wait for about 30 seconds to 1 minute until the Status column changes from Syncing to Available. Step 4: Verify again (The \u0026ldquo;Wow\u0026rdquo; Moment)\nThe system now possesses the new knowledge. Let\u0026rsquo;s challenge the AI once again.\nReturn to the Streamlit Chatbot interface (No need to reload the page or restart the server). Ask the exact same question as before: \u0026ldquo;What is the activation code for Project Omega?\u0026rdquo; Expected Result: The AI answers correctly: \u0026ldquo;The activation code is AWS-ROCKS-2025-SINGAPORE\u0026rdquo;. The AI cites the source as the secret-project.txt file. Conclusion You have just witnessed the true power of RAG!\nNo code editing required. No model retraining required. Simply Sync the data. Your Chatbot has become smarter and updated with the latest information in just a few simple steps. This is exactly why businesses choose this solution to build internal virtual assistants.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn and research more about Cloud knowledge Complete MOOCs in Skill Builder to review for Cloud Practitioner Certificate Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material and Learning Notes 2 Learn about Domain 1: Cloud Concepts and Identify the benefits of AWS Cloud\n- Review 5 characteristics of cloud computing\n- Infrastructure\n- Distinguish Availability vs Durability vs Resilience\n- Distinguish Scaling and Elasticity\n- Identify AWS Cloud design principles\n- AWS Well-Architected Framework\n- General Design Principles\n- 6 main pillars 20/10/2025 20/10/2025 Domain 1 3 Learn about Domain 1: Cloud Concepts and Identify the benefits of AWS Cloud\n- Understand benefits and migration strategies to AWS Cloud\n- AWS Cloud Adoption Framework\n- Cloud Adoption Stages\n- 7 Migration Strategies (The 7 R\u0026rsquo;s)\n- Migration Services \u0026amp; Scenarios\n- Understand Billing, Pricing and Support (Cloud Economics, TCO, Reduce Cost, ..) 21/10/2025 21/10/2025 Domain 1 4 Learn about Domain 2: Security and Compliance\n- 2.1 Understand AWS Shared Responsibility Model (AWS is responsible for, Customer is responsible for, Responsibility CHANGES Based on Service)\n- 2.2 Understand AWS Cloud security, governance and compliance concepts (AWS Security, Governance and Compliance)\n- 2.3 Identify AWS access management capabilities (root, IAM, Cognito, IAM Policies)\n- 2.4 Identify components and resources for security (Security Groups, NACLs, WAF) 22/10/2025 22/10/2025 Domain 2 5 Learn about Domain 3: Cloud Technology and Services\n- 3.1 Define methods of deploying and operating in AWS Cloud\n+ Deploy and Operating methods\n+ Types of Cloud today?\n+ Compare Public and Private services\n+ Connectivity options\n- 3.2 Identify AWS global infrastructure\n+ Learn about AWS infrastructure\n+ Regional edge utilities\n+ Edge Services: CloudFront vs Global Accelerator\n+ Key concepts about AZ and DR\n- 3.3 Identify AWS compute resources\n+ Amazon EC2\n+ EC2 storage\n+ EC2 Instance types\n+ EC2 configuration\n+ Containers\n+ Serverless computing\n+ Compare High Availability vs. Scalability\n- 3.4 Identify AWS database resources\n+ Learn about Amazon RDS\n+ Learn about Amazon Aurora\n+ Learn about Amazon DynamoDB\n+ In-memory databases (Amazon ElastiCache, Amazon DynamoDB Accelerator - DAX)\n+ Learn about Amazon RedShift\n+ Learn about Database Migration (AWS Snow Family, AWS DMS, AWS DataSync) 23/10/2025 23/10/2025 Domain 3 6 Learn about Domain 3: Cloud Technology and Services\n- 3.5 Identify AWS network resources\n+ Learn about Amazon VPC\n+ VPC types\n+ VPC core components\n+ VPC security\n+ VPC Gateways\n+ VPC \u0026amp; Hybrid Connectivity\n+ DNS \u0026amp; Routing\n- 3.6 Identify AWS storage resources\n+ Cloud storage types\n+ Learn about object storage: Amazon S3\n+ Learn about file storage: EFS vs. FSx\n+ Block storage: EBS vs Instance Store\n+ EBS volume types\n+ Hybrid storage: AWS Storage Gateway\n+ Learn about Backup\n- 3.7 Identify AWS AI and ML services as well as analytics services\n+ Learn AI/ML basics\n+ 3 levels of AWS ML (AI Services, ML Services, ML Frameworks \u0026amp; Infrastructure)\n+ Data analytics services (Amazon Athena, Amazon Macie, Amazon Redshift, Amazon Kinesis, Amazon Glue, Amazon QuickSight, Amazon EMR)\n- 3.8 Identify services from other AWS service categories in scope\n+ Monitoring \u0026amp; Observability (CloudWatch, X-ray, EventBridge)\n+ Learn about application integration services (Amazon SQS, Amazon SNS)\n+ Difference between Business and Customer services (Amazon Connect, Amazon SES, AWS Activate, AWS IQ,\u0026hellip;)\n+ Developer and DevOps tools (AWS CodeCommit, AWS CodeBuild, AWS CodeDeploy,\u0026hellip;)\n+ End-User Computing (Amazon AppStream 2.0, Amazon WorkSpaces, Amazon WorkSpaces Web)\n+ Learn about Frontend Web and Mobile services (AWS Amplify, AWS AppSync)\n+ IoT support services (AWS IoT Core, AWS IoT Greengrass) 24/10/2025 24/10/2025 Domain 3\nQuestion \u0026amp; Practice Week 7 Achievements: 1. Domain 1: Cloud Concepts - Benefits of AWS Cloud Cloud Computing Basics:\nMastered 5 characteristics of cloud computing (On-demand self-service, Broad network access, Resource pooling, Rapid elasticity, Measured service) Understood AWS cloud infrastructure clearly Distinguished concepts clearly: Availability: System is always operational and accessible Durability: Data is protected from loss Resilience: System can recover quickly after incidents Understood the difference between Scaling and Elasticity AWS Well-Architected Framework:\nMastered General Design Principles Understood clearly the 6 main pillars: Operational Excellence Security Reliability Performance Efficiency Cost Optimization Sustainability Migration to AWS Cloud:\nUnderstood AWS Cloud Adoption Framework (CAF) with 6 perspectives Mastered Cloud Adoption Stages Learned 7 migration strategies (7 R\u0026rsquo;s): Rehost (Lift and Shift) Replatform (Lift, Tinker, and Shift) Repurchase (Drop and Shop) Refactor/Re-architect Retire Retain Relocate Understood migration services and scenarios Cloud Economics:\nUnderstood Total Cost of Ownership (TCO) Cost reduction methods on AWS AWS billing and pricing models 2. Domain 2: Security and Compliance AWS Shared Responsibility Model:\nAWS responsibility (Security OF the Cloud) Customer responsibility (Security IN the Cloud) Responsibility changes based on service type (IaaS, PaaS, SaaS) Security, Governance \u0026amp; Compliance:\nAWS basic security concepts Governance and compliance on AWS Cloud Security services and tools Access Management:\nAWS Root User and best practices AWS IAM (Identity and Access Management) Amazon Cognito IAM Policies and how they work Least Privilege Principle Security Components:\nSecurity Groups: Instance-level firewall Network ACLs (NACLs): Subnet-level firewall AWS WAF (Web Application Firewall): Web application protection 3. Domain 3: Cloud Technology and Services (Part 1) 3.1 Deployment and Operating Methods:\nDeployment and operation methods Cloud types: Public, Private, Hybrid Compare Public vs Private services Connectivity options 3.2 AWS Global Infrastructure:\nAWS global infrastructure structure (Regions, Availability Zones) Regional expansion utilities (Local Zones, Wavelength Zones) Edge Services: CloudFront vs Global Accelerator Concepts of Availability Zones and Disaster Recovery 3.3 AWS Compute Resources:\nAmazon EC2: Instance types, AMI, configuration EC2 Storage: EBS, Instance Store Containers: ECS, EKS, Fargate Serverless Computing: AWS Lambda Compare High Availability vs Scalability 3.4 AWS Database Resources:\nAmazon RDS: Managed relational database Amazon Aurora: MySQL/PostgreSQL compatible Amazon DynamoDB: NoSQL database In-Memory Databases: ElastiCache, DynamoDB Accelerator (DAX) Amazon Redshift: Data warehouse Migration Services: AWS Snow Family, AWS DMS, AWS DataSync 4. Domain 3: Cloud Technology and Services (Part 2) 3.5 AWS Network Resources:\nAmazon VPC and core components: Subnets (Public/Private) Route Tables Internet Gateway NAT Gateway VPC Security: Security Groups, NACLs VPC Gateways: Internet Gateway, NAT Gateway, Virtual Private Gateway VPC \u0026amp; Hybrid Connectivity: VPN, Direct Connect DNS \u0026amp; Routing: Route 53 3.6 AWS Storage Resources:\nCloud storage types: Object, Block, File Amazon S3: Object storage, storage classes File Storage: EFS vs FSx Block Storage: EBS vs Instance Store EBS Volume Types: gp2, gp3, io1, io2, st1, sc1 AWS Storage Gateway: Hybrid storage Backup Solutions: AWS Backup 3.7 AI/ML and Analytics Services:\nAI/ML basics 3 levels of AWS ML: AI Services (Rekognition, Comprehend, etc.) ML Services (SageMaker) ML Frameworks \u0026amp; Infrastructure Analytics Services: Amazon Athena: Query S3 data Amazon Macie: Data security Amazon Redshift: Data warehouse Amazon Kinesis: Real-time data streaming Amazon Glue: ETL service Amazon QuickSight: BI tool Amazon EMR: Big data processing 3.8 Other AWS Services:\nMonitoring \u0026amp; Observability: CloudWatch, X-Ray, EventBridge Application Integration: SQS, SNS Business \u0026amp; Customer Services: Amazon Connect, SES, AWS Activate, AWS IQ Developer \u0026amp; DevOps Tools: CodeCommit, CodeBuild, CodeDeploy, CodePipeline End-User Computing: AppStream 2.0, WorkSpaces, WorkSpaces Web Frontend \u0026amp; Mobile: AWS Amplify, AWS AppSync IoT Services: AWS IoT Core, AWS IoT Greengrass Summary: Week 7 completed comprehensive learning of 3 main domains for AWS Cloud Practitioner certificate. Mastered basic cloud computing concepts, understood security and compliance on AWS clearly, and became familiar with most important AWS services. Built a solid knowledge foundation to prepare for the certification exam.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.7-cleanup/","title":"Clean Resources","tags":[],"description":"","content":"Target To avoid incurring unwanted costs after finishing the practice lab, we need to delete the created resources.\n⚠️ WARNING: Deleting the Knowledge Base DOES NOT automatically delete the Vector Store (OpenSearch Serverless). You must manually delete the OpenSearch Serverless Collection as this is the costliest service in this Lab.\nImplementation Steps Step 1: Delete Knowledge Base\nAccess the Amazon Bedrock Console -\u0026gt; Knowledge bases.\nSelect the radio button next to your Knowledge Base name.\nClick the Delete button.\nA dialog box appears, enter the Knowledge Base name to confirm (or type delete).\nClick Delete. This process takes 10-15 minutes to complete successfully, so please be patient.\nStep 2: Delete Vector Store\nAccess the Amazon OpenSearch Service. In the left menu, under Serverless, select Collections. You will see a Collection named like bedrock-knowledge-base-.... Select the radio button next to that Collection name. Click the Delete button. Type confirm or the collection name to confirm deletion. Click Delete. Step 3: Delete Data on S3\nAccess the Amazon S3 service. Select the bucket rag-workshop-demo. Click the Empty button first. Type permanently delete to confirm deleting all files inside. After the bucket is empty, return to the Buckets list. Select that bucket again and click the Delete button. Enter the bucket name to confirm. Completion Congratulations on fully completing the Workshop \u0026ldquo;Building a RAG Application with Amazon Bedrock\u0026rdquo;. Your system has been cleaned up and is safe!\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is professional, with FCJ community members being very friendly and open. Especially, senior FCJ members as well as peers in my cohort are always ready to support and answer questions when my team and I face difficulties in designing and using AWS services.\nHowever, I think next time, if possible, there should be a fixed schedule for team members to come to the office more easily instead of competing as currently, so that everyone in the team can more easily access and communicate directly, as some members didn\u0026rsquo;t have time to get acquainted outside initially and could only get to know each other online.\n2. Support from Mentor / Team Admin\nThe mentors are extremely dedicated and caring, along with a very large number of supporting FCJers. Initially, I was a bit hesitant about whether I could improve the knowledge I learned at school and apply it to work, and was somewhat confused when starting to learn and work. But I saw the opportunity in it from friends and seniors who are also developing new opportunities and new challenges. I am very grateful for the enthusiastic support from Nghi Danh who answered some of my questions about applying AI to Cloud, and I also thank mentors like Brother Kha and Master Hung, as well as previous season FCJers who enthusiastically supported and helped me.\n3. Relevance of Work to Academic Major\nThe work I perform and learn will help me solve bigger problems, not just solving problems for a small school project, and instead I learned how to handle large projects, from attending events, not only expanding knowledge, but with my curiosity, I also explored many new technologies and researched how to apply them in practice. With my major being an AI Developer, I also need to develop further and learn more deeply about AWS ML/AI services.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills not only about my major but also other soft skills. I worked with new people, with people not in the same major, thereby gaining different perspectives from each field as well as skills through sharing among team members. I attended many meaningful events from experienced seniors at large corporations, learned how they apply AI to work as well as other advanced programming skills that helped me better orient my career. It also opened up the opportunity for me to become a Cloud Engineer in the future alongside my AI Developer work.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive. Everyone in the company provides dedicated support with a spirit focused on learning and sharing knowledge. During work, everyone works extremely focused but also has very enjoyable moments. When deadlines approach, everyone works together and supports each other regardless of position - whoever finishes their work supports those who haven\u0026rsquo;t finished, those who know help those who don\u0026rsquo;t, as long as there\u0026rsquo;s a problem, everyone will support to solve it quickly and effectively. This helped me easily integrate with everyone even though I\u0026rsquo;m just an intern.\nAdditional Questions What did you find most satisfying during your internship?\nI think what I was most satisfied with during the internship was attending Workshops and Events. Initially, I was quite confused when participating in large-scale events for developers for the first time. Participating was not enough, but I also got hands-on experience and learned a tremendous amount of knowledge about programming skills and applying AI to programming. This helped me significantly reduce the time spent when starting any new project, instead helping me save time and learn more knowledge. What do you think the company should improve for future interns?\nI think you should improve the Rules for replying to messages for new members, and how they can grasp information when seniors assign a new task. I propose a solution that when sending messages, you should require members to reply to that message by responding with something like \u0026lsquo;copied\u0026rsquo;, so that important messages are not lost and members who read messages later can catch up. Everything else I see is extremely good and meticulous. If recommending to a friend, would you suggest they intern here? Why or why not?\nOf course, I extremely recommend friends to intern here, because this is the closest gateway for them to access real work, about what businesses are using and applying today, helping them find more opportunities right before their eyes. Here, you not only access new knowledge but also see how large enterprises solve their company\u0026rsquo;s problems, and they can reflect on themselves and develop more during the internship. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nI have no suggestions to further improve the internship experience, because the program has already provided fully and too many new things for an intern experiencing at an enterprise. Would you like to continue this program in the future?\nDefinitely, I will pursue and continue this program in the future. I need to learn more and accomplish much more. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Complete courses on NLP (Natural Language Processing) Learn more about FastAPI to prepare for the upcoming project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material and Learning Notes 2 Complete Module 01 - Natural Language Processing with Classification and Vector Spaces\n- Week 01: Sentiment Analysis with Logistic Regression\n- Week 02: Sentiment Analysis with Naive Bayes\n- Week 03: Vector Space Models\n- Week 04: Machine Translation and Document Search 27/10/2025 27/10/2025 Module 01 - Week 01\nModule 01 - Week 02\nModule 01 - Week 03\nModule 01 - Week 04 3 Complete Module 02 - Natural Language Processing with Probabilistic Models\n- Week 01: Auto-correction and Minimum Edit Distance\n- Week 02: Part-of-Speech Tagging and Hidden Markov Models\n- Week 03: Autocomplete and Language Models\n- Week 04: Word Embeddings with Neural Networks 28/10/2025 28/10/2025 Module 02 - Week 01\nModule 02 - Week 02\nModule 02 - Week 03\nModule 02 - Week 04 4 Complete Module 03 - Natural Language Processing with Sequence Models\n- Week 01: Recurrent Neural Networks for Language Modeling\n- Week 02: LSTMs and Named Entity Recognition\n- Week 03: Siamese Networks 29/10/2025 29/10/2025 Module 03 - Week 01\nModule 03 - Week 02\nModule 03 - Week 03 5 Complete Module 04 - Natural Language Processing with Attention Models\n- Week 01: Neural Machine Translation\n- Week 02: Text Summarization\n- Week 03: Question Answering 30/10/2025 30/10/2025 Module 04 - Week 01\nModule 04 - Week 02\nModule 04 - Week 03 6 - Learn how to deploy ML models as APIs\n- Try building a CRUD application entirely with FastAPI 31/10/2025 31/10/2025 Machine_Learning_Model_AS_API\nFastAPI_Built_Application_CRUD Week 8 Achievements: 1. Module 01 - Natural Language Processing with Classification and Vector Spaces Week 01: Sentiment Analysis with Logistic Regression\nUnderstood binary classification for sentiment analysis Built logistic regression model from scratch Feature extraction with Bag of Words Preprocessing: tokenization, stemming, stop words removal Model evaluation with accuracy, precision, recall Week 02: Sentiment Analysis with Naive Bayes\nUnderstood Bayes\u0026rsquo; Theorem and conditional probability Built Naive Bayes classifier Compared performance with Logistic Regression Understood independence assumption Laplacian smoothing to handle zero probability Week 03: Vector Space Models\nVector space models and word representations Cosine similarity to measure similarity PCA (Principal Component Analysis) for dimensionality reduction Visualized word embeddings Euclidean distance vs Cosine similarity Week 04: Machine Translation and Document Search\nWord alignment for machine translation Hash tables and locality sensitive hashing Document search and information retrieval K-nearest neighbors in NLP Transformation matrices for word translation 2. Module 02 - Natural Language Processing with Probabilistic Models Week 01: Auto-correction and Minimum Edit Distance\nEdit distance (Levenshtein distance) Dynamic programming for minimum edit distance Spelling correction algorithms Probability-based error correction N-gram models for spell checking Week 02: Part-of-Speech Tagging and Hidden Markov Models\nPart-of-Speech (POS) tagging Hidden Markov Models (HMMs) Viterbi algorithm for sequence labeling Transition and emission probabilities Training HMMs with tagged corpus Week 03: Autocomplete and Language Models\nN-gram language models (unigram, bigram, trigram) Perplexity for evaluating language models Smoothing techniques (Laplace, Add-k) Backoff and interpolation Building autocomplete systems Week 04: Word Embeddings with Neural Networks\nContinuous Bag of Words (CBOW) Skip-gram model Word2Vec architecture Training word embeddings Negative sampling Evaluating word embeddings 3. Module 03 - Natural Language Processing with Sequence Models Week 01: Recurrent Neural Networks for Language Modeling\nRNN architecture and forward propagation Backpropagation through time (BPTT) Vanishing and exploding gradient problems Language modeling with RNNs Text generation with RNNs GRU (Gated Recurrent Units) Week 02: LSTMs and Named Entity Recognition\nLong Short-Term Memory (LSTM) architecture Cell state, forget gate, input gate, output gate Named Entity Recognition (NER) task Bidirectional LSTMs Training LSTMs for sequence labeling Evaluating NER systems Week 03: Siamese Networks\nSiamese network architecture Triplet loss function One-shot learning Similarity learning Applications: question duplicate detection, semantic similarity Cosine similarity in neural networks 4. Module 04 - Natural Language Processing with Attention Models Week 01: Neural Machine Translation\nSequence-to-sequence (Seq2Seq) models Encoder-decoder architecture Attention mechanism Teacher forcing BLEU score for machine translation Beam search decoding Week 02: Text Summarization\nExtractive vs Abstractive summarization Seq2Seq with attention for summarization ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) Coverage mechanism Pointer-generator networks Handling long documents Week 03: Question Answering\nQuestion answering systems Context-based QA Attention mechanisms for QA SQuAD dataset Extractive QA models End-to-end trainable QA systems 5. FastAPI and Machine Learning Model Deployment Machine Learning Model as API:\nSetup FastAPI project structure Load and serve ML models Request/Response schemas with Pydantic Input validation and error handling Preprocessing pipelines in API Testing API endpoints Dockerize ML API FastAPI CRUD Application:\nRESTful API design principles CRUD operations (Create, Read, Update, Delete) Database integration (SQLAlchemy) Async/await operations Authentication and authorization API documentation with Swagger/OpenAPI Dependency injection in FastAPI File structure best practices: app/main.py: Entry point app/routers/: API routes app/models/: Database models app/schemas/: Pydantic schemas app/crud/: Database operations app/db/: Database configuration FastAPI Key Features Mastered:\nPath parameters and query parameters Request body validation Response models Background tasks Middleware CORS configuration Environment variables Testing with pytest Summary: Week 8 completed the entire NLP curriculum from basic to advanced, including 4 modules covering topics from classification, probabilistic models, sequence models to attention mechanisms. Mastered techniques from traditional methods (Naive Bayes, HMM) to modern deep learning approaches (RNN, LSTM, Attention). Also mastered FastAPI framework to deploy ML models as production-ready APIs with full CRUD operations, validation, and best practices. Ready to apply NLP and FastAPI knowledge to real-world projects.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: FastAPI project with MongoDB Vietnamese STT library selected and integrated Vietnamese OCR library selected and tested NLP extraction: amount, category, date, jar detection (REQ-027) Detect multiple transactions (REQ-027) Publish events to RabbitMQ Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material and Learning Notes 2 Project Setup\n- Create FastAPI project structure (/app, /models, /services, /utils, /routers, /schemas, /ai-models)\n- Setup virtual environment (Python 3.11+)\n- Install FastAPI, Uvicorn, Pydantic\n- Install local MongoDB (Docker)\n- Create database: ai_service_db (configured in docker-compose.yml)\n- Collections: Bills, Voices (using MongoEngine models)\n- Setup MongoDB connection with MongoEngine\n- Test connection (lifecycle management in database.py)\nTechnology Research\n- For Bill: Try various OCR models that best solve Vietnamese extraction\n- For Voice: Search for Speech-to-Text models in Vietnamese language 03/11/2025 03/11/2025 Sprint 01 - Day 01 3 Core API Structure and Endpoints\n- Shared API infrastructure\n- Setup Voice and Bill APIs\n- Create basic API endpoint structure\n- Setup Pydantic models for request/response\n- Add CORS middleware\n- Add error handling middleware\n- Create health check endpoint (GET /health)\n- Setup logging (structlog) 04/11/2025 04/11/2025 Sprint 01 - Day 02 4 Voice Processing Pipeline\n- Install Vietnamese STT Library\n- Audio preprocessing\n- Voice-to-Text integration\n- Implement STT functionality\n- Test Voice Model\nOCR Preparation for Bill Extraction\n- Install OCR library selected from Day 1\n- Research image preprocessing techniques\n- Create sample preprocessing pipeline\n- Test with bill images 05/11/2025 05/11/2025 Sprint 01 - Day 03 5 Text Processing for Vietnamese Language\n- Test model accuracy and select appropriate model, team uses PhoWhisper from VinAI\n- Check model detection, whether it receives voice and returns text\n- Adjust endpoint to return correct categories defined by team\n- Test processing multiple transactions simultaneously, check if model can handle\n- Setup detection of transaction time from speech\n- Create transaction object\nOCR Preprocessing\n- Process images when user inputs and process that bill. Enhance image to best quality before feeding to OCR model\n- Collect Vietnamese bills (electricity bills, shopping mall bills, restaurant bills, convenience store bills, coffee shop bills, \u0026hellip;) 06/11/2025 06/11/2025 Sprint 01 - Day 04 6 Integration and Testing\n- Handle background tasks for Voice and Bill\n- Setup event publishing\nSpeech to Text\n- End-to-end testing of Voice\n- Pipelines running as Recording -\u0026gt; Processing -\u0026gt; Return Endpoint, check and improve Voice processing accuracy\nBill Detection\n- Deploy and test OCR models: Tesseract, EasyOCR,\u0026hellip; 07/11/2025 07/11/2025 Sprint 01 - Day 05 Week 9 Achievements: 1. Project Setup and Infrastructure Completed FastAPI project structure with standard directories (/app, /models, /services, /utils, /routers, /schemas, /ai-models) Setup Python 3.11+ environment with virtual environment Installed and configured local MongoDB using Docker Created ai_service_db database and collections for Bills and Voices Setup MongoDB connection with MongoEngine and lifecycle management 2. API Structure and Middleware Setup API endpoints for Voice and Bill processing Created Pydantic models for request/response validation Configured CORS middleware and error handling middleware Implemented health check endpoint (GET /health) Integrated structlog for logging system 3. Voice Processing (Speech-to-Text) Researched and selected PhoWhisper from VinAI for Vietnamese STT Implemented audio preprocessing and Voice-to-Text integration Tested model accuracy and voice detection capability Configured endpoint to return correct defined categories Tested processing multiple transactions simultaneously Setup time detection for transactions from speech Created transaction objects from voice data 4. Bill Processing (OCR) Researched and selected appropriate OCR libraries (Tesseract, EasyOCR) Implemented image preprocessing to optimize quality before OCR Collected diverse Vietnamese bill dataset (electricity, supermarket, restaurants, convenience stores, coffee shops) Tested OCR models with real bill images 5. NLP Extraction and Data Processing Implemented information extraction: amount, category, date (REQ-027) Jar detection Multiple transaction detection in a single request 6. RabbitMQ Integration Setup event publishing to RabbitMQ Handled background tasks for Voice and Bill processing 7. Testing and Quality Assurance End-to-end testing for Voice processing pipeline (Recording → Processing → Return Endpoint) Evaluated and improved Voice processing accuracy Tested OCR models with real dataset Summary: Week 9 successfully completed all objectives, including FastAPI-MongoDB infrastructure setup, PhoWhisper model integration for Vietnamese STT, OCR implementation for bill processing, and event publishing system with RabbitMQ. NLP functions for transaction information extraction were implemented and successfully tested.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Improve and optimize OCR model for various types of invoices Enhance Voice processing quality with Vietnamese numbers and compound phrases Deploy Confidence Scoring system for both Voice and Bill Smart category detection and context recognition Comprehensive error handling and performance optimization Multi-dimensional testing with edge cases Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Participate in Cloud Mastery Series #2\nOCR Enhancement\n- Detect and extract line items in proper JSON format\n- Extract multiple items in invoice\n- Count components in invoice\n- Test with various bill types\nVoice Model Quality Improvement\n- Handle Vietnamese numbers (e.g., \u0026ldquo;hai mươi hai\u0026rdquo; to 22)\n- Recognize compound phrases\n- Fix spelling errors and handle noise\n- Improve JSON response format\n- Testing 10/11/2025 10/11/2025 Sprint 02 - Day 06 3 Check OCR model confidence score\n- Calculate Field-level Confidence\n- Calculate confidence score for each extracted field:\n- Amount/Total confidence\n- Date confidence\n- Text clarity\n- Clear line separation\n- Price alignment\n- Quantity detection\n- Overall confidence algorithm\n- Calculate Low Confidence Threshold\n- Test with over 30 bills\nCheck Voice Confidence Scoring\n- Test multi-layer confidence scoring\n- Test overall confidence algorithm\n- Test Voice Low Confidence Thresholds\n- Performance check (over 50 samples, \u0026lt;4 seconds)\nComprehensive testing of Voice and Bill systems 11/11/2025 11/11/2025 Sprint 02 - Day 07 4 Smart category detection and context recognition in Voice\n- Advanced category detection such as merchant name analysis, keyword extraction based on categories\n- Determine context\n- Testing\nExtract new invoice types\n- Test with supermarket invoices\n- Test with restaurant invoices\n- Test with coffee shop and beverage store invoices\n- Extraction rules for each specific type\n- Improve Bill endpoint JSON response\nBackend integration and testing\n- Integrate transaction functions\n- Error handling\n- Return correct JSON values 12/11/2025 12/11/2025 Sprint 02 - Day 08 5 Error Handling, Optimization \u0026amp; Monitoring\nVoice processing optimization\n- Optimize Voice model performance\nError Handling \u0026amp; Resilience\n- Handle Corrupted Audio:\n- Validate audio format (WAV, MP3, M4A)\n- Check audio duration (minimum 0.5 seconds, maximum 30 seconds)\n- Detect corrupted/incomplete files\n- Return clear error: \u0026ldquo;Invalid audio format or corrupted file\u0026rdquo;\n- Handle STT Timeout:\n- Set timeout for STT processing (maximum 30 seconds)\n- If timeout, return partial result with warning\n- Log timeout events for monitoring\nRetry Logic:\n- Retry STT on temporary errors (maximum 3 retries)\n- Exponential backoff: 1 second, 2 seconds, 4 seconds\n- Return error after maximum retries\nException Cases:\n- Empty/silent audio → Error: \u0026ldquo;No speech detected\u0026rdquo;\n- Non-Vietnamese speech → Low confidence warning\n- Multiple speakers → Warning + best effort extraction\n- Very long audio (\u0026gt;30 seconds) → Error: \u0026ldquo;Audio too long\u0026rdquo;\nGraceful Degradation:\n- If category detection fails → Return \u0026ldquo;Uncategorized\u0026rdquo;\n- If quantity extraction fails → Return null value with warning\n- If date not detected → Use current date with warning\n- Always return partial results when possible 13/11/2025 13/11/2025 Sprint 02 - Day 09 6 Comprehensive testing and documentation\n- Prepare data for testing both Voice and Bill models\n- Test Voice API\n- Test Bill OCR API\nVoice test cases\n- Test with background noise\n- Test with fast or slow speech\n- Test with invalid input, such as nonsense speech\n- Test multiple transactions in one recording\n- Test ambiguous input\nBill processing test cases\n- Test images rotated in various directions\n- Test image quality\n- Test inputs that are not invoice images\n- Test with highly complex invoices\n- Test with invoices containing many items 14/11/2025 14/11/2025 Sprint 02 - Day 10 Week 10 Achievements: 1. OCR Model Improvements Detect and extract line items in proper JSON format Extract multiple items in a single invoice Automatically calculate quantity of components in invoice Successfully tested with various invoice types: supermarket, restaurant, coffee shop, beverage stores Built extraction rules for specific invoice types Improved JSON response format for Bill endpoint 2. Voice Processing Quality Enhancement Handle Vietnamese numbers (convert \u0026ldquo;hai mươi hai\u0026rdquo; → \u0026ldquo;22\u0026rdquo;) Recognize Vietnamese compound phrases Fix spelling errors and handle audio noise Improved JSON response format Optimized Voice processing performance 3. Confidence Scoring System OCR Confidence:\nCalculate field-level confidence Confidence for Amount/Total Confidence for Date Evaluate text clarity and line separation Detect price alignment and quantity Build overall confidence algorithm Set Low Confidence Threshold Tested on over 30 real invoices Voice Confidence:\nDeploy multi-layer confidence scoring Overall confidence algorithm for Voice Set Low Confidence Thresholds Performance check on over 50 samples (processing time \u0026lt;4 seconds) 4. Smart Category Detection Merchant/seller name analysis Keyword extraction based on categories Transaction context determination Advanced Category Detection 5. Error Handling \u0026amp; Resilience Audio Error Handling:\nValidate audio format (WAV, MP3, M4A) Check duration (min: 0.5s, max: 30s) Detect corrupted/incomplete files Handle STT timeout (max: 30s) Retry logic with exponential backoff (3 times, 1s-2s-4s) Exception Handling:\nEmpty/silent audio → Error: \u0026ldquo;No speech detected\u0026rdquo; Non-Vietnamese speech → Low confidence warning Multiple speakers → Warning + best effort extraction Audio too long (\u0026gt;30s) → Error: \u0026ldquo;Audio too long\u0026rdquo; Graceful Degradation:\nCategory not detected → Return \u0026ldquo;Uncategorized\u0026rdquo; Quantity not extracted → Return null + warning Date not detected → Use current date + warning Always return partial results when possible 6. Comprehensive Testing Voice Testing:\nTest with background noise Test speech speed (fast/slow) Test invalid input (nonsense speech) Test multiple transactions in one recording Test ambiguous input Bill OCR Testing:\nTest images rotated in various directions Test diverse image quality Test input not in invoice format Test highly complex invoices Test invoices with many items 7. Backend Integration Integrate transaction functions with Backend Comprehensive error handling Return standardized JSON values Summary: Week 10 focused on enhancing quality and reliability of both Voice and OCR models. Successfully deployed confidence scoring system, comprehensive error handling, and multi-dimensional testing with various edge cases. The system is ready for Backend integration and handling complex real-world scenarios.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Detect and handle multiple complex transactions Integrate PDF file support for Bill OCR Implement feedback system and model fine-tuning Enhance image and audio processing quality Integrate Backend and MongoDB storage Automated testing and Docker deployment Get acquainted with AWS and management tools Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material and Learning Notes 2 Multiple transaction detection and PDF support\n- Complex multi-transaction parsing\n- Refine multi-transaction detection algorithm:\n- Detect jar allocation in phrases\n- Handle mixed transaction types:\nTest complex phrases\n- Test 1 transaction with 1 jar\n- Test multiple transactions\n- Test switching between jars\n- Test mixed contexts\nImprove PDF file support for Bill section\n- Install libraries\n- Test transactions with PDF files 17/11/2025 17/11/2025 Sprint 03 - Day 11 3 User feedback and learning system\n- Handle user feedback system for Voice section\n- Handle user feedback system for Bill section\n- Fine-tune models based on feedback\n- Improve syntax handling for incorrect inputs\nTest improvements of Voice and Bill models 18/11/2025 18/11/2025 Sprint 03 - Day 12 4 Advanced image/audio processing\n- Detect and enhance Bill quality\n- Detect image quality\n- Auto-rotate and deskew images\n- Detect ROI (Region of Interest)\n- Integrate with OCR Pipeline\n- Testing\nVoice background noise handling\n- Implement noise reduction\n- Voice activity detection\n- Silence trimming\n- Reduce processing time\n- Integrate with Voice Pipeline\n- Testing 19/11/2025 19/11/2025 Sprint 03 - Day 13 5 Backend Integration \u0026amp; File Storage\nBackend service transaction integration\n- Review Backend API\n- Test AI and Backend flow\n- Verify event consumption\n- Check automatic transaction creation\n- Troubleshoot integration\nMongoDB storage integration\n- Setup Database for Voice and Bill\n- Prepare for Amazon S3 integration 20/11/2025 20/11/2025 Sprint 03 - Day 14 6 Create automated tests\n- Create tests for Voice and Bill\n- Acceptance and verification of all tests\n- Bug fixes\n- Full regression testing\nSetup and Deploy to Docker 21/11/2025 21/11/2025 Sprint 03 - Day 15 Week 11 Achievements: 1. Multiple Transaction Detection and PDF Support Complex multi-transaction parsing Refined multi-transaction detection algorithm Detected jar allocation in phrases Handled mixed transaction types Complex Phrase Testing:\nTested 1 transaction with 1 jar Tested multiple transactions Tested switching between jars Tested mixed contexts PDF Support:\nInstalled PDF processing libraries Tested transactions with PDF files Integrated PDF into OCR Pipeline 2. Feedback and Learning System Handled user feedback system for Voice section Handled user feedback system for Bill section Fine-tuned models based on feedback Improved handling of incorrect input syntax Tested improvements of Voice and Bill models 3. Advanced Image/Audio Processing Bill Quality Enhancement:\nDetected and assessed image quality Auto-rotated and deskewed images Detected ROI (Region of Interest) Integrated with OCR Pipeline Tested with various image conditions Voice Background Noise Handling:\nImplemented noise reduction Voice Activity Detection Silence trimming Reduced processing time Integrated with Voice Pipeline Tested with various audio environments 4. Backend \u0026amp; Storage Integration Backend Integration:\nReviewed Backend API endpoints Tested AI and Backend workflow Verified event consumption Checked automatic transaction creation Troubleshot integration issues MongoDB Integration:\nSetup Database for Voice and Bill Prepared schema for Amazon S3 integration Implemented storage strategy 5. Automated Testing and Deployment Created automated tests for Voice and Bill Acceptance and verification of all test cases Fixed bugs discovered from testing Performed full regression testing Setup Docker environment Deployed to Docker container 6. AWS Learning Understood AWS and basic service groups (Compute, Storage, Networking, Database) Created and configured AWS Free Tier account Became familiar with AWS Management Console Installed and configured AWS CLI (Access Key, Secret Key, Region) Performed basic operations with AWS CLI Connected and became familiar with First Cloud Journey community Summary: Week 11 completed the enhancement of multi-transaction processing capabilities, integrated PDF support, implemented feedback system and model fine-tuning. Significantly improved image/audio processing quality with advanced techniques (ROI detection, noise reduction, VAD), successfully integrated with Backend and MongoDB, while completing automated testing and Docker deployment. Additionally, became familiar with AWS ecosystem and basic management tools, preparing for cloud infrastructure deployment.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Load testing and system performance optimization Improve accuracy of Voice and OCR models Enhance security and comprehensive error handling Implement advanced logging and metrics collection Prepare for deployment and final testing Improve code quality and documentation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material and Learning Notes 2 Setup load testing\n- Install load testing tools\n- Create load testing scenarios\n- Setup resource monitoring\n- Prepare test data\nRun load tests\n- Voice load (10 files)\n- Bill load (10 files)\n- Concurrent load for both\nOptimization\n- Database optimization\n- Implement caching\n- Memory optimization 24/11/2025 24/11/2025 Sprint 04 - Day 16 3 Improve voice accuracy\n- Analyze failure cases\n- Improve NLP rules\n- Retest and iterate\nImprove OCR accuracy\n- Analyze failure cases\n- Format-specific improvements\n- Character recognition improvements\nAmount parsing edge cases\n- Handle ambiguous cases\n- Validation logic\n- Total amount extraction\n- Total validation logic 25/11/2025 25/11/2025 Sprint 04 - Day 17 4 Security enhancements\n- File upload validation\n- Rate limiting\n- Input sanitization\n- JWT validation\n- MongoDB security\n- Security checklist\nComprehensive error handling\n- Try-Catch all functions\n- Appropriate HTTP status codes\n- Helpful error messages\n- Logging with context\nVoice robustness testing\n- Test corrupted/invalid files\nOCR robustness testing\n- Test corrupted/invalid images 26/11/2025 26/11/2025 Sprint 04 - Day 18 5 Logging improvements\n- Structured JSON logging\n- Request logging\n- Processing step logs with timing\n- Error logging with stack traces\n- Correlation IDs for tracking\nMetrics collection\n- Track processing time\n- Accuracy and error rates\n- Store metrics in MongoDB\n- Create metrics API\nVoice deployment prep\nOCR deployment prep 27/11/2025 27/11/2025 Sprint 04 - Day 19 6 Final comprehensive testing\n- Full regression testing\n- Test all error scenarios\n- UI integration testing\n- Backend integration testing\nCode quality\n- Add docstrings\n- Add type hints\n- Run linter \u0026amp; fix issues\n- Add unit tests for critical functions 28/11/2025 28/11/2025 Sprint 04 - Day 20 Week 12 Achievements: 1. Load Testing and Optimization Load Testing Setup:\nInstalled load testing tools (JMeter/Locust) Created load testing scenarios (Voice, Bill, concurrent) Setup resource monitoring (CPU, RAM, Disk I/O) Prepared test data Running Load Tests:\nTested Voice load (10 files concurrently) Tested Bill OCR load (10 files concurrently) Tested concurrent load for both Voice and Bill Analyzed bottlenecks and chokepoints Optimization:\nOptimized Database queries and indexing Implemented caching for results Optimized memory and garbage collection Improved API response time 2. Accuracy Improvements Voice Accuracy:\nAnalyzed failure cases Improved NLP rules for Vietnamese Tested and iterated Enhanced accuracy for number and category recognition OCR Accuracy:\nAnalyzed OCR failure cases Format-specific improvements for bills Improved special character recognition Handled difficult font cases Amount Parsing:\nHandled ambiguous cases Validation logic for amounts Total amount extraction Total validation logic 3. Security Enhancements File Security:\nFile upload validation (file type, size validation) Rate limiting for APIs Input sanitization JWT token validation MongoDB security (authentication, authorization) Completed security checklist Error Handling:\nComprehensive Try-Catch for all functions Appropriate HTTP status codes Clear and helpful error messages Logging with full context Robustness Testing:\nTested Voice with corrupted/invalid files Tested OCR with corrupted/invalid images Handled graceful degradation 4. Logging and Metrics Enhanced Logging:\nStructured JSON logging Logging for each HTTP request Processing step logs with timestamps Error logging with stack traces Correlation IDs for tracking request flow Metrics Collection:\nTracked processing time Accuracy and error rates Stored metrics in MongoDB Created API endpoints for metrics Dashboard for monitoring 5. Deployment Preparation Prepared Voice service deployment Prepared OCR service deployment Docker configuration and optimization Environment variables and secrets management Health check endpoints 6. Comprehensive Testing and Code Quality Comprehensive Testing:\nFull regression testing Tested all error scenarios UI integration testing (frontend integration) Backend integration testing End-to-end testing Code Quality:\nAdded docstrings for all functions/classes Added type hints (Python typing) Ran Linter (Pylint/Flake8) and fixed issues Added unit tests for critical functions Code review and refactoring Summary: Week 12 focused on finalizing and making the AI system production-ready. Successfully performed load testing and performance optimization, significantly improved accuracy of both Voice and OCR models. Implemented comprehensive security with file validation, rate limiting, JWT authentication, and MongoDB security. Deployed structured logging and metrics collection for monitoring. Enhanced code quality with docstrings, type hints, linting, and unit tests. The system is now ready for production deployment with robust error handling and comprehensive testing.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/tags/","title":"Tags","tags":[],"description":"","content":""}]