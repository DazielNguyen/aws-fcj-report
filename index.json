[{"uri":"https://dazielnguyen.github.io/aws-fcj-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders (Track 2: Migration \u0026amp; Modernization)” Event Objectives Complete large-scale migration and modernization with AWS Modernize applications using generative AI–powered tools Group discussion: Application modernization—Accelerate business transformation Transform VMware with AI-driven cloud-modernization technologies AWS security at scale: From development to production Speakers Nguyen Van Hai - Director of Software Engineering, Techcombank Nguyen The Vinh - Co-Founder \u0026amp; CTO, Ninety Eight Nguyen Minh Nganh - AI Specialist, OCB Nguyen Manh Tuyen - Head of Data Application, LPBank Securities Key Highlights 1. Learn large-scale migration and modernization strategies with AWS through real-world case studies from Techcombank The Modernization Journey of Techcombank\u0026rsquo;s\nAssess: Inventory the environment and identify gaps. Mobilize: Establish CCoE, define guardrails, and build cloud fluency. Migrate \u0026amp; Modernize: Prioritize high-impact workloads. Reinvent: AI, automation, data products, new business models. Generative AI in Modernization.\nCode Transformation: Java 8 -\u0026gt; 21, .NET -\u0026gt; .NET 8 Dependency Mapping: Automatied analysis of system relationships. Environment Assessment: Amazon: thousands of services modernized with AI Solutions and strategies Techcombank applied when using AWS services. Modernization with AWS native technologies:\nAmazon EKS Amazon Aurora MySQL Amazon MSK Amazon ElastiCache for Redis OSS. Overview of the Modernization Strategy Blueprint.\nAlign: Executive sponsorship and business drivers | Assess: Understand people, process, and technology | Mobilize: CoE, governance, training | Modernize: Replatform, refactor, rebuild | Reinvent: Data, AI, and modern apps for innovation 2. Gain knowledge about modernizing applications using Generative Al-powered tools, with practical insights from VPBank Modernization is the process of progressively transforming applications to achieve the availability, scalability, business agility, and cost optimization benefits of running on the cloud. Top 4 Use cases – App Modernization with GenAI.\nUse case 1: Streamline VMware Migration with AWS Transform for VMware\nAccelerate infrastructure migration and modernization with intelligent discovery and automated execution\nSlash VMware migration timelines with AWS Transform\u0026rsquo;s intelligent automation.\nConvert complex network configurations in hours instead of weeks using Al-powered discovery, dependency mapping, and automated wave planning.\nScale your migration practice with automated security group creation, intelligent EC2 instance selection, and flexible deployment options including hub-and-spoke or isolated VPC configurations.\nGain up to 90% improverent in execution times while reducing manual effort by 80%.\nUse case 2: GenAI Development with AWS Serverless and Container Solutions\nBuild Enterprise-Ready GenAI Applications Across AWS Serverless and Containers Platforms\nAWS offers two powerful paths for GenAI application development and deployment:\nServerless with AWS Bedrock: Rapidly develop and deploy GenAI applications using AWS Lambda, ECS with Fargate, Step Functions, and EventBridge. Ideal for chatbots, document generation, and intelligent content processing. Leverage the latest AWS Bedrock updates and reference architectures.\nContainer-Based with Amazon EKS: Build, train, and run GenAl apps on Kubernetes, benefiting from its powerful orchestration capabilities. Utilize open-source tools and cloud-native services for scalable GenAl workloads. Flexible deployment across cloud and on-premises environments with constant innovation from the OSS community.\nChoose one approach or combine both to best fit your specific GenAI application requirements and accelerate your AI journey.\nUse case 3: Revolutionize .NET Modernization with AWS Transform for .NET\nTransform legacy Windows applications to cloud-native with AI-powered automation.\nModernize Windows-based applications up to 4× faster with AWS Transform for .NET. Leverage AI automation to analyze dependencies, refactor code, and optimize for Linux deployment while cutting licensing costs by up to 40%. Transform hundreds of applications in parallel with automated testing and validation câpbilities—from legacy MVC applications to WCF services. Advanced features include automatic UI modernization, private package handling, and intelligent wave planning, delivering comprehensive modernization with exceptional speed. Use case 4: Elevate Platform Engineering with GenAI \u0026amp; IDP\nHarness the power of intelligent assistants like AWS Transform Developer with Internal Development Platforms.\nScaling modernization at an enterprise level requires time and investment to develop Internal Development Platforms (IDP). Gartner predicts that by 2026, 80% of software engineering organizations will establish platform teams as internal providers of reusable services, components, and tools for application delivery.\nHarness the power of intelligent assistants like AWS Transform Developer with IDPs to:\nCreate workflows and automate repetitive tasks.\nLearn IDP best practices leading organizations, such as Adobe, Expedia, JPMC, and Goldman Sachs.\nUnderstand AWS container blueprints and reference architectures to deliver accelerated velocity and scaling for an enterprise scale modernization initiative.\nCommon modernization drivers\nReduce costs\nReduce/eliminate Windows \u0026amp; SQL Server licensing costs Create architectures that match cost to actual load Leverage ARM64 architectures for better price-performance Increase pace of innovation\nDecompose monoliths into smaller services / microservices Take advantage of new technologies and C# language features Automate manual processes Improve ability to scale\nScale individual components / services Granular scaling with containers / serverless Attract and retain talent\n3. Gain insights from top industry experts through panel discussions on application modernization .NET Framework vs cross-platform .NET\n.NET Framework:\nWindows-only OS Version 1.0 released in 2002 Final version is 4.8*, released in 2019 Monolithic installation—A large number of libraries are installed at once. EC2, Elastic Beanstalk, ECS, and EKS. .NET (formerly .NET Core)\nCross-platform (Windows, Linux, macOS) Version 1.0 released in 2016 Current GA version is 8.0, released in 2023 Supports side-by-side installations Most libraries are distributed individually EC2, Elastic Beanstalk, ECS, EKS, Lambda Fargate AWS Transform: Orchestrated intelligence\nUnified web experience -\u0026gt; End-to-end automation -\u0026gt; Dedicated agency -\u0026gt; Goal-driven -\u0026gt; Human-in-the-loop -\u0026gt; Simplified collaboration AWS Transform for .NET\nCustomer benefits\nReduce operating costs by up to 40% Eliminate operating system license commercial costs Access a larger developer pool Cloud scale and performance. Technical benefits\nVulnerability remediation support Cross-platform support: Windows, macOS, Linux (x86-64, arm64) Compatibility with x86-64 and arm64 LightweightContainer Lambda serverless architecture Complete language upgrades in minutes via Amazon Q\nAccelerate application modernization Automatic Language Upgrade (Java, .NET) Reduce Technical Debt Save Costs and Improve Operational Efficiency Enhance Competitive Advantage Kiro application: Specification-driven development solution\nKiro helps developers and engineering teams ship high-quality software with AI agents. Kiro turns your prompts into clear requirements, system designs, and discrete tasks. Iterate with Kiro on your specifications and architecture. Kiro agents implement the specification while keeping you in control. Agent hook\nDelegate tasks to AI agents triggered by events such as ‘file save’ Agents auto-execute in the background based on your predefined prompts Agent hooks help you scale work by generating documentation, unit tests, or code performance optimizations Advanced context management\nConnect to documents, databases, APIs, and more with native MCP integrations Configure how you want Kiro agents to interact with each project via directive files Drop in an image of your UI design or a photo from your architecture discussion and Kiro can use it to guide implementation Power, flexibility, and security\nCompatible with VS Code\nKiro supports VS Code Open VSX plugins, themes, and settings in a streamlined AI-ready environment Advanced Claude models\nChoose between Claude Sonnet 3.7 or Sonnet 4, with more options coming soon Enterprise-grade security\nKiro is built and operated by AWS Use cases\nBuild new applications\nQuickly go from prototype to production code and deployment, with best practices baked in, including structured design, runbooks, or test coverage scope Build on existing applications\nWith smart specification and context management, Kiro makes it easy to integrate and extend existing applications while maintaining consistency Refactor and modernize\nKiro understands your codebase and can precisely guide refactoring across codebases exceeding one million LOC 4. Learn about AI-driven cloud modernization tailored for VMware environments The future state of your VMware workloads\nRELOCATE: Amazon EVS | REHOST: Amazon EC2 | REPLATFORM TO CONTAINERS: Amazon ECS or Amazon EKS | REPLATFORM TO MANAGED SERVICES: Amazon RDS, Amazon FSx, Amazon WorkSpaces, and more | REFACTOR: Modern Application =\u0026gt; Rapid adoption, foundational cloud benefits and quick ROI....................----\u0026gt;....................All native cloud benefits and high ROI AWS Transform for VMware\nModernize VMware workloads onto Amazon EC2 with purpose-built AI agents Automate and simplify transformation tasks Reduce costs and licensing fees with Amazon EC2 Enhance security, scalability, and resilience Drive innovation with over 200 AWS native services Mapping VMware native technologies to AWS\nAn agentic AI-based approach to VMware modernization\n1. Connect to your VMware environment | 2. Analyze workloads, dependencies, and readiness | 3. Transform VMware network configurations into AWS native constructs | 4. Generate intelligent wave plans based on application dependencies | 5. Validate with your team, then execute =\u0026gt; Step-by-step transformation with human-in-the-loop validation Why AWS Transform for migrating from VMware?\nLower costs\nEliminate VMware licensing fees Optimize infrastructure costs with AI-driven instance right-sizing Faster migration\nAccelerate network transformation up to 80× Minimize disruption, preserve application integrity, and speed up the transition Improved security\nStrengthen security with a cloud-native foundation Migrate safely with a human-in-the-loop validation process Innovation at scale\nReduce technical debt and build modern, scalable applications Seamlessly integrate with over 200 AWS native services like data lakes, advanced analytics, and AI/ML 5. Connect and learn directly from AWS Solutions Architects and industry experts In this part, experts presented the challenges faced in the early stages of full-system modernization from on-premises to AWS.\nThey proposed specific plans and strategies for each area, executing the most critical parts first. They also adhered to current regulations and laws in management and did not collect user information. Once on AWS, the most important factor is the ability to scale rapidly, which leads to substantial gains when moving to the AWS environment. Applying AI is proving highly effective in their businesses—for example, anh Vinh has applied AI to detect potentially fraudulent transactions and defend against hackers in Blockchain. 6. Understand AWS security best practices from development to production environments What I Learned 5-step framework: Align → Assess → Mobilize → Modernize → Reinvent. GenAI-assisted modernization: code transformation (Java 8→21, .NET→8), dependency mapping, environment assessment. Prioritize high-impact workloads, human-in-the-loop, measure ROI. Design Thinking Problem→Pilot→Scale; prioritize value-first. Strangler Fig refactor by parts; event-driven mindset. Platform thinking/IDP, security-by-design, early governance. Technical Architecture Microservices, containers (EKS/ECS/Fargate), serverless (Lambda/Step Functions/EventBridge). Data: Aurora MySQL, MSK (Kafka), ElastiCache (Redis). VMware→AWS: rehost EC2 → replatform containers/managed → refactor app. Multi-arch (x86_64 + ARM64), end-to-end observability. Modernization Strategies Assess/Mobilize/MM/Reinvent (Techcombank blueprint). AWS Transform: for VMware \u0026amp; .NET (automated migration/testing/UI modernization). Cost-first: drop Windows/SQL licenses, right-size, ARM64. Scale \u0026amp; Innovate: decompose monolith, CI/CD automation, adopt GenAI. Application to Work Build a migration backlog by ROI; select a small pilot. Standardize container baseline (EKS) + Bedrock pattern for GenAI. Use Amazon Q/Transform to upgrade languages \u0026amp; refactor quickly. Design an internal IDP: service templates, golden paths, policy guardrails. Experience at the event “GenAI-powered Migration \u0026amp; Modernization provided a comprehensive view of transforming applications \u0026amp; databases at enterprise scale. Highlights: demos of automated VMware/.NET migration, serverless–container reference architectures, quantitative ROI lessons and battle-tested governance models, along with case studies that significantly shortened migration time and reduced costs.”\nLearning from highly specialized speakers Techcombank: operate with CCoE, measure business outcomes, 5-step roadmap. Ninety Eight: AI anti-fraud, strong security posture, real-time. OCB/LPBankS: data products, automation, safe cloud scale. Hands-on technical experience Clear view of dependency mapping, wave planning, auto SG/EC2 sizing, hub-and-spoke VPC. Auto code upgrade, cross-platform .NET, automatic UI modernization. Modern tool adoption AWS Transform (VMware/.NET), Amazon Q (auto language upgrade). Bedrock, Lambda, ECS/Fargate, EKS, Step Functions, EventBridge. Aurora, MSK, ElastiCache, EC2; IDP tooling; Kiro (spec→tasks/agents, MCP). Networking and exchange Finalize best practices from SAs \u0026amp; major banks; governance/security checklists. Connect for mentorship, pattern reuse, ROI comparison \u0026amp; benchmarking. The event created opportunities to interact directly with experts, peers, and business teams, helping to strengthen the ubiquitous language between business and tech. Lessons learned Applying DDD and event-driven patterns reduces coupling and increases scalability and resilience.\nA modernization strategy needs a phased approach and ROI measurement; avoid rushing a full system overhaul.\nAI tools like Amazon Q Developer can boost productivity when integrated into the current development workflow.\nModernize with strategy: measure ROI, prioritize by value.\nAutomation + GenAI shortens timelines and reduces technical debt.\nPlatform/IDP is a scale lever; security-by-default is indispensable.\nHuman-in-the-loop ensures safety for broad automation.\nSome photos from the event Over 400 passionate technology developers in Thành phố Hồ Chí Minh gathered at the AWS office (36th Floor) to watch the live plenary session from Hà Nội, sharing the excitement and knowledge about AWS Cloud Day Vietnam 2025\nOverall, the event not only provided technical knowledge but also changed my mindset about application design, system modernization, and more effective cross-team collaboration.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: DATA SCIENCE ON AWS WORKSHOP Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Van Anh Duy\nPhone Number: 0387 883 041\nEmail: duynguyenvananh@gmail.com\nUniversity: FPT University Campus Ho Chi Minh\nMajor: Artificial Intelligent\nClass: SE181823\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect with teammates and get acquainted with the Champions in the First Cloud Journey. Learn about the AWS services provided to customers. Complete the Lab as well as the knowledge in Module 1 of FJC 2025. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Read carefully the notes, rules, and regulations at AWS\n- Connect and interact with team members\n- Form groups and plan study schedules as well as project timing needed for the internship at the company.\n- Create a Google Sheet to manage study activities and track the progress of team members. 08/09/2025 08/09/2025 - Documentation: https://cloudjourney.awsstudygroup.com/\n- Youtube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\n- Notes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_01/Take_notes_module_01.md 3 - Learn about the basic concepts:\n+ Understand what cloud computing is.\n+ What are the benefits of using cloud computing?\n+ What makes AWS different.\n+ How to start a cloud journey? 09/09/2025 09/09/2025 - Documentation: https://cloudjourney.awsstudygroup.com/\n- Youtube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\n- Notes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_01/Take_notes_module_01.md\" 4 - Learn about AWS global infrastructure.\n- Learn about AWS Services Management tools.\n- How to optimize costs on AWS.\n- Practice Lab 01:\n+ Create an AWS account.\n+ Set up MFA for the account and understand what MFA is.\n+ Create Admin Group and Admin User.\n+ Experiment with AWS Support. 10/09/2025 10/09/2025 - Documentation: https://cloudjourney.awsstudygroup.com/\n- Youtube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\n- Notes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_01/Take_notes_module_01.md\" 5 - Practice Lab 07:\n+ Create Budget\n+ Create Cost Budget\n+ Create Usage Budget\n+ Create RI Budget\n+ Create Saving Plan Budget\n+ How to delete resources.\n- Learn about the services of each Budget\n- And identify which Budgets are suitable for which users. 11/09/2025 11/09/2025 - Documentation: https://cloudjourney.awsstudygroup.com/\n- Youtube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\n- Notes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_01/Take_notes_module_01.md\" 6 - Practice Lab 09:\n+ Learn about AWS Support packages\n+ Access AWS Support\n+ Initiate support requests. 12/09/2025 12/09/2025 - Documentation: https://cloudjourney.awsstudygroup.com/\n- Youtube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\n- Notes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_02/Take_notes_module_02.md Week 1 Achievements: Understand what AWS is and the concepts and services AWS provides:\nCloud computing The differences of AWS How to start a cloud journey AWS global infrastructure AWS Services management tools How to optimize costs on AWS Successfully created and configured an AWS Free Tier account.\nGet familiar with AWS Management Console and know how to find, access, and use services from the web interface.\nInstalled and configured AWS CLI on the computer including:\nAccess Key Secret Key Default Region MFA for the account Admin Group Admin User AWS Support Know how to set up Budgets:\nCost Budget Usage Budget RI Budget Saving Plan Budget Clean resources Clearly understand the support packages and how to access AWS Support:\nBasic Package Developer Package Business Package Enterprise Package "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Get familiar with AWS platforms and learn about the FCJ 2025 program\nWeek 2: Networking on AWS (AWS VPC, VPC Peering \u0026amp; Transit Gateway, VPN \u0026amp; Direct Connect, Elastic Load Balancing)\nWeek 3: Compute VM Services on AWS (Amazon EC2, Amazon Lightsail, Amazon EFS/FSx, AWS Application Migration Service (MGN))\nWeek 4: Storage Services on AWS (S3, Snow Family, Amazon Storage Gateway, Disaster Recovery on AWS, AWS Backup)\nWeek 5: Security Services on AWS - \u0026ldquo;Security is job zero\u0026rdquo; (Shared Responsibility Model, AWS Identity and Access Management, Amazon Cognito, AWS Organizations, AWS Identity Center (SSO), AWS Key Management Service - KMS, AWS Security Hub)\nWeek 6: Database Services on AWS (Database Concepts, Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon ElastiCache)\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand AWS VPC: Grasp the basic concepts of VPC (Virtual Private Cloud) as an isolated logical network environment, including key components like Subnets (Public and Private), Route Tables, and ENI. Traffic Control and Security: Learn to configure security layers (Security Groups and NACLs) and control network traffic flow to/from the Internet (Internet Gateway and NAT Gateway). Complex Network Connectivity: Differentiate and know how to use methods for connecting VPCs (VPC Peering) and the central connection model (Transit Gateway). Build a Hybrid Cloud Environment: Learn about solutions for connecting on-premises networks with AWS, including VPN (Site-to-Site) and private connections (AWS Direct Connect). Application Load Balancing: Understand the function of Elastic Load Balancing (ELB) and differentiate between various load balancer types (ALB, NLB, CLB, GLB) to ensure high availability and scalability for applications. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS Virtual Private Cloud (VPC)\n+ What is VPC?\n+ How does the VPC structure work?\n- Learn about VPC-Subnets and Subnet architecture?\n- Learn about VPC-Route Table?\n- Learn about VPC-ENI and VPC-ENI architecture?\n- Learn about VPC-Endpoint and VPC-Endpoint architecture?\n- Learn about VPC-Internet Gateway and VPC-Internet Gateway architecture?\n- Learn about VPC-NAT Gateway and VPC-NAT Gateway architecture?\n- Learn about VPC-Security Group and VPC-Security Group architecture?\n- Learn about VPC-NACL and VPC-NACL architecture?\n- Learn about VPC-Flow Logs 15/09/2025 15/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_02/Take_notes_module_02.md 3 - Learn about networking services on AWS?\n- Learn about VPC Peering and its architecture?\n- Learn about Transit Gateway and its architecture?\n- Understand the concepts of VPN \u0026amp; Direct Connect services?\n- What is Site-to-Site VPN? How to set it up?\n- Learn about Client-to-Site VPN?\n- What is AWS Direct Connect? 16/09/2025 16/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_02/Take_notes_module_02.md 4 - Learn about the concepts and overview of Elastic Load Balancing? And the current types of ELB?\n- Learn about ELB - Application Load Balancer and its architecture?\n- Learn about ELB - Network Load Balancer and understand the concept?\n- Learn about ELB - Classic Load Balancer and understand the concept?\n- Learn about ELB - Gateway Load Balancer and its architecture? 17/09/2025 17/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_02/Take_notes_module_02.md 5 - Lab 03 - VPC Initialization\n1. Configure VPC Firewall\n2. Practice Creating a VPC\n3. Configure Site to Site VPN\n- Lab 58 - System Manage - Session Manage\n1. Create Connection to EC2 Server\n2. Manage Session Logs 3. Use Port Forwarding\n- Lab 19 - Set Up VPC Peering\n1. Update Network ACL\n2. Create Peering Connection 3. Configure Route Tables\n4. Enable Cross-Peer DNS\u0026quot; 18/09/2025 18/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_02/Take_notes_module_02.md 6 - Lab 20 - Transit Gateway Setup\n1. Infrastructure Setup\n2. Create Transit Gateway -\u0026gt; Connect Multiple VPCs Together\n3. Transit Gateway Attachments\n4. Create Route Table for TGW\n5. Add Gateway to Route Tables \u0026amp; Check Results\n- Lab 10 - Hybrid DNS\n1. Hybrid DNS Setup\n2. Create Outbound Endpoint\n3. Create Route 53 Resolver Rule\n4. Create Inbound Endpoint.\n- Additional research on AWS Advanced Networking - Specialty Study Guid 19/09/2025 19/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_02/Take_notes_module_02.md Research Link: https://www.amazon.com/Certified-Advanced-Networking-Official-Study/dp/1119439833 Week 2 Achievements: Explain what VPC is, its role in AWS, and its core components (Subnet, Route Table, ENI). Clearly differentiate between a Public Subnet (with an Internet Gateway) and a Private Subnet (using a NAT Gateway for Internet access). Compare and contrast the two main firewall mechanisms: Security Group (stateful, applies to ENI) and NACL (stateless, applies to Subnet). Present how to privately connect from a VPC to AWS services (like S3) without going over the Internet using a VPC Endpoint. Evaluate the pros and cons of two VPC connection solutions: VPC Peering (1:1 connection, no transitive support) and Transit Gateway (hub-and-spoke model, simplifies management). Describe methods for establishing a hybrid cloud connection, including Site-to-Site VPN (over the Internet) and AWS Direct Connect (private physical connection). Classify and select the appropriate Elastic Load Balancer type for specific scenarios: Application Load Balancer (ALB): For HTTP/HTTPS traffic (Layer 7), supports path-based routing. Network Load Balancer (NLB): For TCP/TLS traffic (Layer 4), requires ultra-high performance and static IP. Gateway Load Balancer (GLB): Used for integrating virtual appliances. Identify the necessary labs to reinforce knowledge of VPC, Peering, Transit Gateway, and related services. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"In this section, you need to summarize the contents of the workshop that you plan to conduct.\nIoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The IoT Weather Platform is designed for the ITea Lab team in Ho Chi Minh City to enhance weather data collection and analysis. It supports up to 5 weather stations, with potential scalability to 10-15, utilizing Raspberry Pi edge devices with ESP32 sensors to transmit data via MQTT. The platform leverages AWS Serverless services to deliver real-time monitoring, predictive analytics, and cost efficiency, with access restricted to 5 lab members via Amazon Cognito.\n2. Problem Statement What’s the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, Amazon S3 for storage (including a data lake), and AWS Glue Crawlers and ETL jobs to extract, transform, and load data from the S3 data lake to another S3 bucket for analysis. AWS Amplify with Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time dashboards, trend analysis, and low operational costs.\nBenefits and Return on Investment The solution establishes a foundational resource for lab members to develop a larger IoT platform, serving as a study resource, and provides a data foundation for AI enthusiasts for model training or analysis. It reduces manual reporting for each station via a centralized platform, simplifying management and maintenance, and improves data reliability. Monthly costs are $0.66 USD per the AWS Pricing Calculator, with a 12-month total of $7.92 USD. All IoT equipment costs are covered by the existing weather station setup, eliminating additional development expenses. The break-even period of 6-12 months is achieved through significant time savings from reduced manual work.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases This project has two parts—setting up weather edge stations and building the weather platform—each following 4 phases:\nBuild Theory and Draw Architecture: Research Raspberry Pi setup with ESP32 sensors and design the AWS serverless architecture (1 month pre-internship) Calculate Price and Check Practicality: Use AWS Pricing Calculator to estimate costs and adjust if needed (Month 1). Fix Architecture for Cost or Solution Fit: Tweak the design (e.g., optimize Lambda with Next.js) to stay cost-effective and usable (Month 2). Develop, Test, and Deploy: Code the Raspberry Pi setup, AWS services with CDK/SDK, and Next.js app, then test and release to production (Months 2-3). Technical Requirements\nWeather Edge Station: Sensors (temperature, humidity, rainfall, wind speed), a microcontroller (ESP32), and a Raspberry Pi as the edge device. Raspberry Pi runs Raspbian, handles Docker for filtering, and sends 1 MB/day per station via MQTT over Wi-Fi. Weather Platform: Practical knowledge of AWS Amplify (hosting Next.js), Lambda (minimal use due to Next.js), AWS Glue (ETL), S3 (two buckets), IoT Core (gateway and rules), and Cognito (5 users). Use AWS CDK/SDK to code interactions (e.g., IoT Core rules to S3). Next.js reduces Lambda workload for the fullstack web app. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Lambda: $0.00/month (1,000 requests, 512 MB storage). S3 Standard: $0.15/month (6 GB, 2,100 requests, 1 GB scanned). Data Transfer: $0.02/month (1 GB inbound, 1 GB outbound). AWS Amplify: $0.35/month (256 MB, 500 ms requests). Amazon API Gateway: $0.01/month (2,000 requests). AWS Glue ETL Jobs: $0.02/month (2 DPUs). AWS Glue Crawlers: $0.07/month (1 crawler). MQTT (IoT Core): $0.08/month (5 devices, 45,000 messages). Total: $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand comprehensive knowledge of virtual server (Compute VM) services on AWS. Focus on the core service Amazon EC2, including configuration selection (Instance Types), storage types (EBS, Instance Store), and automation (User data, Auto Scaling). Learn about related services such as Amazon Lightsail (low-cost service), shared file storage solutions (EFS for Linux and FSx for Windows/Linux), and the AWS MGN application migration service to migrate servers to AWS or build Disaster Recovery. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Amazon Elastic Compute Cloud (EC2)\n- Learn about the architecture of Amazon EC2, a flexible virtual server service with fast boot capabilities and powerful resource scaling.\n- Understand the technique of selecting server configurations through EC2 Instance Types, which determine factors like CPU, Memory, Network, and Storage.\n- Learn how to use AMI (Amazon Machine Image) to provision one or more EC2 Instances and use Key Pairs (public and private key) to encrypt login information.\n- Understand the EBS (Elastic Block Store) block storage technique, which operates independently, replicates data (3x) to ensure high availability, and connects to EC2 over the network.\n- Differentiate EBS from Instance Store, which is an extremely high-speed NVME disk area but whose data is completely erased when the EC2 instance is stopped.\n- Learn about the User Data automation technique, a script (bash shell or powershell) that runs once when an EC2 instance is launched.\n- Understand Meta Data, information related to the EC2 instance (like IP, Hostname) that can be accessed from the server itself, often used for automation.\n- Understand the EC2 Auto Scaling technique to automatically increase (scale-out) or decrease (scale-in) the number of EC2 Instances based on conditions (scaling policy).\n- Learn about EC2 Pricing Options (On-demand, Reserved Instance, Saving Plans, Spot Instance) to optimize costs. 22/09/2025 22/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_03/Take_notes_module_03.md 3 Amazon Lightsail\n- Learn about the Amazon Lightsail service, a low-cost virtual server solution (starting at $3.5/month) suitable for light workloads, test/dev environments.\n- Understand the technique for connecting Lightsail (located in a special VPC) with a regular VPC via VPC Peering (with just 1 click).\nAmazon EFS/FSx\n- Learn about EFS (Elastic File System), a network file storage service (NFSv4) that allows multiple EC2 Instances (Linux only) to mount simultaneously, charging based on storage used.\n- Learn about Amazon FSx, a service that allows creating NTFS volumes (SMB protocol) to attach to multiple EC2 Instances (supports Windows and Linux).\n- Understand the deduplication technique of FSx, which helps reduce data duplication and lower storage costs.\nAWS Application Migration Service (MGN)\n- Learn about the AWS MGN service used to migrate and replicate servers (physical or virtual) from on-premise to the AWS environment.\n- Understand the replication technique of MGN used for building a Disaster Recovery Site at a low cost (using low-configuration staging machines). 23/09/2025 23/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_03/Take_notes_module_03.md 4 Lab: 000004 - Basic EC2 Operations.\n- Create an EC2 Instance\n- Take an EC2 Instance snapshot\n- Install an application on EC2\nLab: 000027 - Resource Management with Tags and Resource Groups\n- Use Tags\n- Use Resource Groups 24/09/2025 24/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_03/Take_notes_module_03.md 5 Lab: 000008 - Resource Management with Amazon CloudWatch\n- CloudWatch Agent\n- Create CloudWatch Dashboard\nLab: 000006 - Deploying an Autoscaling Group\n- Create Launch Template\n- Create Target Group\n- Create Load Balancer\n- Create Auto Scaling Group\n- Verify the results. 25/09/2025 25/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_03/Take_notes_module_03.md 6 Lab: 000045 - Getting Started with Amazon Lightsail\n- Preparation\n- Test the application on Lightsail\n- Use Lightsail Load Balancer\n- Use RDS\n- Migrate to EC2.\n[Supplemental Research] - Microsoft Workloads on AWS\n- A series of supplemental labs for running Microsoft servers and applications on AWS\n- Supplement knowledge about operating systems\n- Supplement knowledge about the Linux operating system such as LBI1, LBI2\n- Supplement knowledge about the Windows operating system, learn more about the Bundo management system. Refer to the lab series 26/09/2025 26/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_03/Take_notes_module_03.md\nResearch Link: [https://www.youtube.com/playlist?list=PLhr1KZpdzukdJ|IxuIUM7pMB7aJ2_FfTP] Week 3 Achievements: EC2 Service: Clearly understand that EC2 is the core virtual server service of AWS. EC2 Configuration Techniques: Know how to select Instance Type (CPU, RAM, Network configuration) and use AMI to provision the operating system for the server. EC2 Security Techniques: Understand how to use Key Pair (public/private key) to encrypt login information, instead of using passwords. Storage Service (Storage): Clearly differentiate the 2 main disk storage types for EC2: EBS (Elastic Block Store): Is a network drive, operates independently, data is replicated 3x within 1 AZ (99.999% availability), can be backed up using snapshots. Instance Store: Is a physical drive (NVME) with extremely high speed, but the data is temporary (will be erased when EC2 is stopped), often used for cache/buffer or swap. Automation Techniques (Automation): Know how to use User Data to run a script once when the server boots up (e.g., install a web server). Understand what Meta Data is and how to use it to retrieve information (IP, hostname) about the server from within itself, used for automation scripts. Scaling Techniques (Scaling): Master the concept of EC2 Auto Scaling to automatically increase (scale-out) or decrease (scale-in) the number of servers based on load (e.g., when ActiveConnectionCount is high or low). Cost Optimization Techniques (Pricing): Recognize the 4 EC2 pricing models: On-demand (per hour/second, most expensive), Reserved Instance (1-3 year commitment), Saving Plans (1-3 year commitment, more flexible), and Spot Instance (low price, utilizes surplus resources but can be reclaimed). Lightsail Service: Understand Amazon Lightsail is a low-cost, simplified VM service, suitable for light workloads, and know how to peer it with a VPC. File Storage Service (File Storage): Differentiate between the 2 shared file storage services for multiple servers: EFS (Elastic File System): Used for Linux (NFSv4 protocol), charges based on storage used. FSx: Used for Windows/Linux (SMB protocol), supports deduplication feature to reduce costs. Migration Service (Migration): Understand AWS MGN is a service to migrate servers from on-premise to AWS or to build a low-cost Disaster Recovery (DR) system via a staging area. Hands-on: Understand the basic hands-on steps with EC2 (create, snapshot), deploy a complete Auto Scaling Group (with Load Balancer), and get familiar with Lightsail. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn comprehensive knowledge about the diverse storage services on AWS. Focus deeply on the core service Amazon S3 (Simple Storage Service), an object storage service, including its characteristics (like 11 nines durability, replication across 3 AZs), and Storage Classes. Learn about important features like Lifecycle Management, Versioning, and Static Website Hosting. Large-scale data migration solutions (the Snow Family), hybrid storage solutions connecting on-premise with the cloud (Storage Gateway), the centralized backup management service (AWS Backup), and the basic concepts and strategies for Disaster Recovery (DR). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Amazon Simple Storage Service - S3\n- Learn about the architecture of Amazon S3, an object storage service, suitable for Write-Once-Read-Many (WORM) data.\n- Understand the technique of automatic data replication across 3 AZs within 1 Region to ensure high availability.\n- Learn about the durability of S3, designed for up to 99.999999999% (11 nines).\n- Understand the technique of uploading (HTTP PUT) and accessing (HTTP GET) S3 data via REST API.\n- Learn about the architecture of Storage Classes, including S3 Standard, S3 Standard-IA, S3 Intelligent-Tiering, S3 One Zone-IA, and Amazon Glacier/Deep Archive.\n- Understand the Object Life Cycle Management technique to automatically move objects between storage classes over time.\n- Understand the technique for hosting a Static Website (suitable for Single Page Applications) and configuring CORS (Cross-origin resource sharing).\n- Understand access control techniques using S3 Access Control List (ACL) (attached to buckets/objects) and S3 Bucket Policy (easier to manage).\n- Learn about the architecture of S3 Endpoints, which allow access to S3 buckets over the AWS private network without needing the Internet.\n- Understand the Versioning technique to recover objects after accidental deletion or overwrite, and to support protection against ransomware.\n- Understand the S3 performance optimization technique by using random prefixes for object keys, helping S3 store objects across multiple partitions.\n- Learn about the architecture of S3 Glacier, a low-cost, long-term archival service that requires data to be retrieved (Expedited, Standard, Bulk) to an S3 Bucket before use. 29/09/2025 29/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_04/Take_notes_module_04.md 3 Snow Family\n- Learn about the Snow Family services (Snowball, Snowball Edge, Snowmobile) used to migrate PetaByte (PB) to Exabyte (EB) scale data from on-premise to AWS (S3 or Glacier).\n- Understand the technique of Snowball Edge, which is a special device with available compute resources to process data locally.\nAmazon Storage Gateway\n- Learn about the architecture of AWS Storage Gateway, a Hybrid storage solution that combines storage capacity on AWS with on-premise.\n- Understand the techniques of the three types of gateways:\n+ File Gateway: Allows storing files on S3 via NFS and SMB protocols.\n+ Volume Gateway: Provides block storage via iSCSI, with data stored on S3.\n+ Tape Gateway: Provides a virtual tape library (VTL) iSCSI, storing virtual tape data in S3 or Glacier. 30/09/2025 30/09/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_04/Take_notes_module_04.md 4 Disaster Recovery on AWS\n- Understand the technique\u0026hellip; for designing Disaster Recovery (DR) based on two key metrics:\n+ RTO (Recovery Time Objective): The time required to restore service.\n+ RPO (Recovery Point Objective): The maximum period of time during which data might be lost.\n- Learn about the 4 DR strategies on AWS: Backup and Restore, Pilot Light, Low Capacity Active-Active, and Full Capacity Active-Active.\nAWS Backup\n- Learn about the AWS Backup service, a centralized management service that allows configuring and scheduling, and setting retention policies for backing up multiple AWS resources (EBS, EC2, RDS, EFS, Storage Gateway\u0026hellip;). 01/10/2025 01/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_04/Take_notes_module_04.md 5 Lab: 000057 - Getting Started with Amazon S3\n- Create S3 Bucket\n- Upload data to S3\n- Host static website on S3\nLab: 000013 - AWS Backup\n- Prepare infrastructure\n- Create Backup Plan\n- Set up Notification\n- Verify operation\nLab: 000014 - AWS Import/Export\n- Prepare virtual machine\n- Import virtual machine to AWS\n- Export virtual machine from AWS 02/10/2025 02/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_04/Take_notes_module_04.md 6 Lab: 000024 - Storage Gateway\n- Create Storage Gateway\n- Create File Sharing\n- Connect the File Share to the machine\nLab: 000025 - FSx\n- AWS Managed MS AD\n- Deploy Instance\n- Set up and use FSx\n[Supplemental Research] - AWS Skill Builder\n- A series of in-depth theory lessons for storage specialists on AWS.\n- Storage Learning Plan: Block Storage\n- Storage Learning Plan: Object Storage 03/10/2025 03/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_04/Take_notes_module_04.md\nResearch Link: https://explore.skillbuilder.aws/ Week 4 Achievements: S3 Service (Basics): Clearly understand that Amazon S3 is an object storage service, not block storage, operating on a WORM (Write Once, Read Many) model. Lesson on Durability: Know that S3 is designed for 11 nines (99.999999999%) of durability by automatically replicating data across 3 Availability Zones (AZs). S3 Cost Optimization Techniques: Differentiate between Storage Classes such as S3 Standard (frequent access), S3 Standard-IA (infrequent access), and S3 Glacier (long-term, low-cost archival, requires retrieval). S3 Automation Techniques: Know how to use Object Life Cycle Management to automatically transition data to cheaper tiers (e.g., from Standard to Glacier) over time. Understand Trigger Events (e.g., triggering a serverless function upon file upload). S3 Security Techniques: Differentiate between two access control mechanisms: S3 ACL (legacy mechanism) and S3 Bucket Policy (easier to define access permissions). Lesson on Data Protection (S3): Clearly understand the Versioning feature, which allows restoring previous versions of a file, helping to protect against accidental deletion or ransomware attacks. S3 Networking Techniques: Know how to use an S3 Endpoint to access S3 from within a VPC over the AWS private network without needing the Internet. Know how to host a Static Website on S3 and configure CORS. Data Migration Service (Migration): Recognize the Snow Family (Snowball, Snowmobile) as the physical solution for large-scale (Petabyte, Exabyte) data migration from on-premise. Hybrid Storage Service: Understand Storage Gateway as a hybrid storage solution, allowing on-premise applications to use protocols (NFS, SMB, iSCSI) to store data on S3/Glacier. Lesson on Disaster Recovery (DR): Understand the 2 basic concepts for designing DR: RTO (recovery time) and RPO (acceptable data loss). Backup Service (Backup): Know that AWS Backup is a centralized management service that helps automate backups (schedule, retention) for multiple AWS resources (EBS, RDS, EFS\u0026hellip;). Hands-on: Understand the practical steps to create an S3 bucket, host a static website, and configure AWS Backup. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 - Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data)\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn foundational knowledge and core security services on AWS, centered around the \u0026ldquo;Security is job zero\u0026rdquo; philosophy. Start with the most basic concept: the Shared Responsibility Model. Focus deeply on managing identity and access (Identity and Access Management - IAM), including components: User, Group, Policy, and Role. Expand learning to identity management services at a larger scale, such as AWS Organizations (managing multiple accounts), AWS Identity Center (SSO) (single sign-on), and Amazon Cognito (user management for web/mobile apps). Gain solid knowledge of data protection through encryption with AWS KMS and monitoring/compliance checks with AWS Security Hub. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Shared Responsibility Model\n- Learn about the Shared Responsibility Model, in which AWS is responsible for security of the cloud (physical infrastructure, underlying software) and the customer is responsible for security in the cloud (configuration, data, applications).\n- Understand how security responsibilities change depending on the service type (infrastructure, combined management, or fully managed).\nAWS Identity and Access Management (IAM)\n- Learn about the Root Account, the account with absolute full permissions, and best practices to protect it (create an IAM Admin User for regular use, lock away root credentials).\n- Learn about IAM User, a principal used to interact with AWS, which has no permissions by default when created.\n- Understand the technique for efficient user management by grouping multiple IAM Users into an IAM Group.\n- Learn about IAM Policy, a JSON document that defines permissions, including 2 types:\n+ Identity-based Policy: Attached directly to an IAM Principal (User, Group, Role).\n+ Resource-based Policy: Attached directly to a resource (e.g., S3 Bucket Policy).\n- Understand the IAM permission evaluation technique, where an explicit deny always takes precedence, regardless of any other Allow policy.\n- Learn about the architecture of IAM Role, a set of permissions (policy) without permanent credentials (password/access key).\n- Understand the Assume Role technique: An IAM User (or Service) uses the AWS STS (Security Token Service) to temporarily \u0026ldquo;assume\u0026rdquo; the IAM Role\u0026rsquo;s permissions and receive temporary credentials.\n- Understand the practical application of IAM Role, e.g., granting an EC2 service permission to access S3 without storing access keys on the server. 06/10/2025 06/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_05/Take_notes_module_05.md 3 Amazon Cognito\n- Learn about Amazon Cognito, a service for managing authentication (login, sign-up) and authorization for end-users of web and mobile applications (different from IAM Users, who are AWS administrators).\n- Learn about the two main components of Cognito:\n+ User Pool: A user directory that manages users, supporting direct login or login via third-party providers (Facebook, Google).\n+ Identity Pool: Grants application users access (usually temporary) to other AWS services.\nAWS Organizations\n- Learn about AWS Organizations, a service that helps centrally manage and govern multiple AWS accounts.\n- Understand the Consolidated Billing technique for all accounts.\n- Understand the technique of grouping accounts into OUs (Organization Units) and applying Service Control Policies (SCP) to limit the maximum permissions that IAM Users/Roles in that account can perform (including deny-based).\nAWS Identity Center (SSO)\n- Learn about AWS Identity Center (SSO), a service that helps centrally manage access (single sign-on) to all AWS accounts in an Organization and to external applications.\n- Understand the technique of using Permission Sets (a set of permissions stored in Identity Center) to assign to Users/Groups. When a user accesses an account, the Permission Set is granted as an IAM Role within that account. 07/10/2025 07/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_05/Take_notes_module_05.md 4 AWS Key Management Service (KMS)\n- Learn about AWS KMS, a service to create and manage encryption keys to protect data at rest (Encryption at rest).\n- Learn about\u0026hellip; CMK (Customer Managed Key) (the master key within KMS) and Data Key (the key used to encrypt/decrypt actual data, generated by the CMK).\nAWS Security Hub\n- Learn about AWS Security Hub, a service for continuous security checks, based on AWS best practices and industry standards (like PCIDSS).\n- Understand how Security Hub provides results as a score and identifies resources that need attention.\nLab: 000002 - Getting Started with IAM and IAM Role\n- IAM Group and IAM User\n- Create IAM Role\n- Assume Role\nLab: 000044 - IAM Role and Condition\n- Introduction to IAM\n- Create EC2 Admin User\n- Create RDS Admin User\n- Create Admin Group-Configure IAM Role Condition\n- Create IAM Role with Admin rights 5.2 Create IAM User 5.3 Configure Switch role 5.4 Restrict IP 5.5 Restrict by time. 08/10/2025 08/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_05/Take_notes_module_05.md 5 Lab: 000048 - IAM Role and Application\n- Use access key\n- IAM Role on EC2\nLab: 000030 - IAM Permission Boundary\n- Introduction to IAM Permission Boundary\n- Create limiting Policy\n- Create IAM User with limited permissions\n- Test the limited User\nLab: 000027 - Tags and Resource Groups\n- Use tags\n- Use tags via Console\n- Display tags\n- Add or remove tags\n- Tag a virtual machine\n- Filter resources by tag\n- Use tags via CLI\n- Resource Group\nLab: 000028 - Manage EC2 via Resource Tag\n- Create IAM Policy\n- Create IAM Role\n- Test IAM Role 09/10/2025 09/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_05/Take_notes_module_05.md 6 Lab: 000018 - Using AWS Security Hub\n- Security standards\n- Activate Security Hub\n- Score for each standard set\nLab: 000012 - Using AWS SSO\n- Preparation steps\n- Create AWS Account in AWS Organizations\n- Set up Organization Unit\n- Set up AWS SSO\n- Verify\nLab: 000033 - KMS Workshop\n- Set up environment\n- Getting started with AWS KMS\n- Encryption with AWS KMS\n- Key Policy and best practices\n- Monitoring AWS KMS usage.\n[Supplemental Research] - AWS Certified Security Specialty All-in-One-Exam Guide (Exam SCS-C01)\n- Study material for the Security Specialty certification exam 10/10/2025 10/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_05/Take_notes_module_05.md\nResearch Link: [https://www.amazon.com/Certified-Security-Specialty-Guide-SCS-C01/dp/1260461726] Week 5 Achievements: Foundational Lesson: Master the Shared Responsibility Model, clearly understanding AWS\u0026rsquo;s responsibilities versus the customer\u0026rsquo;s. IAM Service (Core): Clearly distinguish between the Root Account (full permissions, needs to be locked away) and IAM User (used daily, no permissions by default). Master the 3 main components for granting permissions: IAM User (the entity), IAM Policy (the permission - written in JSON), and IAM Group (a group of entities). Clearly understand IAM Role: a mechanism to grant temporary permissions (no permanent credentials) to both Users and Services (like EC2). IAM Techniques (Important): Know how a User/Service \u0026ldquo;receives\u0026rdquo; a Role\u0026rsquo;s permissions through the Assume Role technique (using the STS service). Understand the permission evaluation rule: An Explicit Deny always overrides any Allow permissions. Identity Management Services (Identity Services): Clearly differentiate between IAM (manages AWS administrators) and Amazon Cognito (manages end-users of web/mobile apps). Know that Cognito User Pool is the user directory (can log in with Facebook, Google) and Identity Pool is what grants those users access to AWS resources. Multi-Account Management Service (Multi-Account): Understand AWS Organizations is used for centrally managing multiple accounts, enabling Consolidated Billing. Know how to use Service Control Policies (SCP) within an Organization to limit the maximum permissions of member accounts. Understand AWS Identity Center (SSO) as the single sign-on solution, using Permission Sets to grant access to accounts within the Organization. Encryption Service (Encryption): Know AWS KMS is the service for creating and managing encryption keys. Understand the Encryption at Rest mechanism and differentiate between CMK (the master key in KMS) and Data Key (the key used to encrypt the actual data). Security Monitoring Service (Monitoring): Know AWS Security Hub is the service that scans and provides security scores, helping to check compliance against standards (like PCIDSS). Hands-on: Practice creating and managing Users, Groups, Policies, and Roles. Practice implementing SSO and KMS. Practice using advanced IAM features like Conditions and Permission Boundaries. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Review foundational database (DB) concepts, including RDBMS (primary key, foreign key), optimization techniques (Index, Partition), and operational concepts (Database Log, Buffer). Clearly differentiate between the two main DB system types: OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing or Data Warehouse). Understand the managed relational database service Amazon RDS, including core features like Multi-AZ (for high availability) and Read Replicas (for read performance). Learn about Amazon Aurora, AWS\u0026rsquo;s cloud-native DB service, with its unique shared storage architecture, high performance, and superior features like Zero Replication Lag. Learn about Amazon Redshift, the petabyte-scale Data Warehouse service designed for OLAP, and understand its MPP architecture and Columnar Storage technique. Understand the role of Amazon ElastiCache (Redis, Memcached) as a high-speed caching layer to reduce load on the primary database. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Database Concepts\n- Lesson: Review fundamental database concepts like Database (a structured information system) and Session (a working session).\n- Learn about architecture: Relational Databases (RDBMS), including Primary Key (to uniquely identify a row) and Foreign Key (to create links between tables).\n- Understand the technique: Normalization, a technique of dividing data into multiple tables (using keys) to prevent data duplication.\n- Understand the technique: Performance Optimization:\n+ Index: A data structure that speeds up retrieval (read) operations, but increases the cost of writes.\n+ Partition: Dividing a large table into many smaller parts to speed up queries.\n+ Execution Plan: The set of steps the database decides to use to access data (e.g., whether to use an Index or not).\n- Understand the technique: Ensuring integrity and speed:\n+ Database Log: Records all changes, important for recovery and replication.\n+ Buffer: A temporary storage area in RAM, helping to speed up reads because reading from RAM is faster than reading from disk.\n- Lesson: Database Classification:\n+ RDBMS (ACID): Fixed structure (Schema), storage optimized (Normalization), scales vertically (Vertical Scaling).\n+ NoSQL (BASE): Flexible structure (Dynamic Schema), performance optimized (Denormalization), scales horizontally (Horizontal Scaling).\n- Lesson: System Classification:\n+ OLTP (Online Transaction Processing): Transaction processing systems (banking, ordering), need to quickly handle read/write/update operations and ensure integrity (roll back).\n+ OLAP (Online Analytical Processing): Data Warehouse systems, store historical data for complex analysis (reporting, finding trends). 13/10/2025 13/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_06/Take_notes_module_06.md 3 Amazon RDS\n- Learn about the service: Amazon RDS (Relational Database Service), a fully managed relational database service, supporting popular engines (MySQL, PostgreSQL, Oracle, etc.).\n- Lesson: The goal of RDS is to automate administrative tasks (updates, backups) so users can focus on the application.\n- Understand the technique: Automated Backups of the database and transaction logs, allowing for Point-in-Time Recovery within a 35-day window.\n- Learn about architecture: Multi-AZ (High Availability)\n+ Automatically creates a standby replica in another AZ.\n+ Uses Synchronous Replication.\n+ Supports Automatic Failover if the primary database fails.\n- Learn about architecture: Read Replicas (Read Performance Optimization)\n+ Creates read-only copies to offload the primary database (e.g., for reporting tasks).\n+ Uses Asynchronous Replication, which can cause \u0026ldquo;replication lag\u0026rdquo;.\n- Lesson: RDS is often used for OLTP applications and is protected by Security Groups. 14/10/2025 14/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_06/Take_notes_module_06.md 4 Amazon Aurora\n- Learn about the service: Amazon Aurora, a database developed by AWS, compatible with MySQL/PostgreSQL, part of the RDS service but with higher performance (3-5x faster).\n- Learn about architecture: The biggest difference for Aurora is the redesigned storage layer.\n- Learn about architecture: An Aurora \u0026ldquo;Cluster\u0026rdquo; consists of 1 Writer (write instance) and up to 15 Readers (read instances), all sharing a single (Cluster Volume) storage partition.\n- Understand the technique: Data on the Cluster Volume is replicated 6 times across 3 AZs to ensure durability.\n- Lesson: Aurora\u0026rsquo;s outstanding advantage is Zero Replication Lag because the Readers read from the same volume as the Writer.\n- Understand the technique: Enterprise features like Backtrack (rewind the database without restoring) and Global Database (create read-only replicas in different Regions). 15/10/2025 15/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_06/Take_notes_module_06.md 5 Amazon Redshift\n- Learn about the service: Amazon Redshift, a petabyte-scale Data Warehouse service, optimized for OLAP.\n- Learn about architecture: Massively Parallel Processing (MPP).\n+ Leader Node: Receives, parses, and coordinates queries.\n+ Compute Nodes: Store and execute parts of the work in parallel.\n- Understand the technique: Columnar Storage.\n+ Unlike OLTP (stores by row), Redshift stores data from the same column together.\n+ This technique is extremely efficient for analytical (OLAP) queries (e.g., Calculate average age only needs to read the Age column).\n- Understand the technique: Redshift Spectrum, allows running SQL queries directly on data in Amazon S3 without needing to load it.\nAmazon ElastiCache\n- Learn about the service: Amazon ElastiCache, a high-speed in-memory caching service.\n- Objective: Speed up applications and reduce the load on the primary database (like RDS).\n- Learn about supported engines: Redis (supports many data types, often preferred) and Memcached.\n- Lesson: It is the user\u0026rsquo;s responsibility to write and manage the Caching Logic (logic that decides what and when to cache) within their application. 16/10/2025 16/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_06/Take_notes_module_06.md 6 Lab: 000005 - Getting Started with Amazon RDS\n1. Create a database on Amazon RDS\n2. Connect the application to the DB\n3. Backup and Restore\nLab: 000043 - Migrating Databases with DMS and SCT\n1. Preparation steps\n2. Oracle to Amazon Aurora (PostgreSQL)\n2.1 Convert Schema\n2.2 Migrate database.\n[Supplemental Research] - Database Internals\n- Document to learn how databases work internally.\nLink: [https://www.amazon.com/Database-Internals-Deep-Distributed-Systems/dp/1492040347]\n[Supplemental Research] - The Data Warehouse Toolkit\n- Document to learn how to design and the techniques used in building a Data-warehouse\nLink: [https://www.amazon.com/Data-Warehouse-Toolkit-Definitive-Dimensional/dp/1118530802] 17/10/2025 17/10/2025 Documentation: https://cloudjourney.awsstudygroup.com/\nYoutube: https://youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026si=W80Cdf_fSc6sjOV\nNotes: https://github.com/DazielNguyen/aws-fcj-report/blob/main/TAKE_NOTES_%26_LABS/Module_06/Take_notes_module_06.md Week 6 Achievements: Lesson (Foundational): Clearly differentiate between the two system models: OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing, data warehouse). Master basic DB optimization techniques: Index (speeds up reads) and Partition (divides tables). Understand the role of Database Log (for recovery/replication) and Buffer (uses RAM for speed). Service (RDS): Know Amazon RDS is a managed relational database (OLTP) service. Clearly distinguish RDS\u0026rsquo;s 2 main features: Multi-AZ (used for High Availability - HA) and Read Replicas (used to increase read performance). Technique (Replication): Differentiate Synchronous Replication (used for RDS Multi-AZ) and Asynchronous Replication (used for RDS Read Replicas, can have lag). Service (Aurora): Know Amazon Aurora is a high-performance, cloud-native database. Understand Aurora\u0026rsquo;s shared storage (Cluster Volume) architecture and its superior benefit of Zero Replication Lag. Be aware of advanced features like Backtrack and Global Database. Service (Redshift): Know Amazon Redshift is a data warehouse (OLAP) service. Understand the MPP (Massively Parallel Processing) architecture (includes Leader Node and Compute Nodes). Master the core technique of OLAP: Columnar Storage, which speeds up analytical queries. Service (ElastiCache): Know Amazon ElastiCache (Redis/Memcached) is an in-RAM caching service. Understand the role of caching is to reduce load on the primary DB. Be aware of the responsibility to write the Caching Logic in the application. Hands-on: Know how to create and operate (backup/restore) an RDS database. Know how to use DMS and SCT services to migrate a DB from Oracle to Aurora. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Connect and get acquainted with members of First Cloud Journey. fhfhg Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://dazielnguyen.github.io/aws-fcj-report/tags/","title":"Tags","tags":[],"description":"","content":""}]